[
  {    
    "id" : "http-0-0-0-0-4000-building-a-better-blog-1498413600",
    "site" : "damianflynn.com",
    "title": "Building a better blog; an experiment in UI and UX",      
    "url": "http://0.0.0.0:4000/building-a-better-blog/",                    
    "categories" : [],
    "tags" : ["Jekyll","UI","UX"],
    "authors" : ["Damian Flynn"],      
    "publishedDate" : "2017-06-25 18:00:00 +0000",
    "content" : "My initial blog had some nice features, but really didn’t do anything to place my content first. Was it really important that my logo be prominently placed? Not really, I am recoding both technical, and product related tips, tricks and news, and not focusing on how to keep a Wordpress site secure, and more importantly working reliably, So I have chosen to leverage static site technology, and ensure the layout focuses on what is truly important, the content. The article should have your sole attention The hamburger menu - gone. If you’re visiting on mobile, chances are you got here from a link, which means you’re interested in the article. As you move down the article, it’s just content. Some of the other standards have worked their way in - how long will it take to read, and how fresh is the content? If you’re visiting on a tablet or larger, you start to see another layer of content/navigation. The 10 most recent article appear as a sidebar. We still want your attention on the article, so content other than a headline is blurred until you hover. How do I move around? Once you reach the end of the article, you’re presented with related articles (if we can find any based on tags), else you’re given the previous and next articles for additional reading. You can also explore content by tags, author, or click the binoculars to bring all articles into focus. Curious about who we are? As you move back up the article, a footer with light navigation appears. We just keep it out of your way while you’re reading. What’s next? I am planning other features, including posting events; and looking for feedback? What do you think?",
    
  },
  {    
    "id" : "http-0-0-0-0-4000-adding-mermaids-back-in-the-game-1498231551",
    "site" : "damianflynn.com",
    "title": "Adding Mermaid support to the site",      
    "url": "http://0.0.0.0:4000/adding-mermaids-back-in-the-game/",                    
    "categories" : [],
    "tags" : ["Jekyll"],
    "authors" : ["Damian Flynn"],      
    "publishedDate" : "2017-06-23 15:25:51 +0000",
    "content" : "A picture is worth a thousand words, so I assume a diagram is worth a billion words. Expressing our ideas is important and our blog is an outlet to not only share with our teammates, but the broader community of readers. Mermaid is a simple markdown based diagramming language with a compelling result. Placing the following html in our posts… &lt;div class=\"mermaid\"&gt; graph TD; A--&gt;B; A--&gt;C; B--&gt;D; C--&gt;D; &lt;/div&gt; results in the wonderful diagram below. graph TD; A--&gt;B; A--&gt;C; B--&gt;D; C--&gt;D; Additionally, I can create diagrams like this. sequenceDiagram participant Alice participant Bob Alice-&gt;John: Hello John, how are you? loop Healthcheck John-&gt;John: Fight against hypochondria end Note right of John: Rational thoughts prevail... John--&gt;Alice: Great! John-&gt;Bob: How about you? Bob--&gt;John: Jolly good!",
    
  },
  {    
    "id" : "http-0-0-0-0-4000-webinar-savision-passport-to-scom-2016-1484847720",
    "site" : "damianflynn.com",
    "title": "Passport To SCOM 2016, Savision Webinar",      
    "url": "http://0.0.0.0:4000/Webinar-Savision-Passport-To-SCOM-2016/",                    
    "categories" : ["Announcements","Events","IT Pro/DevOps","Monitoring & Management","Supportability","Updates"],
    "tags" : ["MVP","Webinar","Live Event","Lumagate","Community","System Center","Operations Manager","Operations Management Suite","Virtual Machine Manager","Windows Azure Pack","Windows Azure Stack","Windows Server 2016","Windows Server 2012"],
    "authors" : ["Damian Flynn"],      
    "publishedDate" : "2017-01-19 17:42:00 +0000",
    "content" : "With the greatest honour, I am scheduled to present for the 4th year in succession at the original Experts Live! event which is once again taking place in the Cinemec, found in the center of Ede, The Netherlands. The event is bi-lingual, but with the vast majority of the presentations been delivered in English. The list of presenters is quite impressive with lots of recongnisable names from the Microsoft Datacenter, Enterprise and Cloud deciplines. Further information and tickets for the event are available at the event website, and are extremly good value at less that €100 for a full day of content, across a number of concurrent paths; ensuing there is lots of chocces for everyone. For further details please check out the event site located at (Experts Live! NL)[https://www.expertslive.nl] Presentations I am currently scheduled to present just a single session at the event, and will be joining some of the ask the experts sessions, as well as recording a show discussing Openstack and the Windows Server Fabric. Demystified: Azure Stack - Resource Provider’s Presentation Abstract The Azure Stack solution is designed to no longer leverage the capabilities of Virtual Machine Manager for managing the IaaS resources of the cloud. In this session we will take a close inspection to how the Compute, Network and Storage Resource Provider’s take direction from the Resource Manager. We will also get a glimpse on where these providers live in the current preview, and how they interact with both the fabric and other resource providers. Slide Deck",
    
  },
  {    
    "id" : "http-0-0-0-0-4000-webinar-nlwug-azure-pack-vs-stack-1484156520",
    "site" : "damianflynn.com",
    "title": "NL Windows User Group Webinar",      
    "url": "http://0.0.0.0:4000/Webinar-NLWUG-Azure-Pack-vs-Stack/",                    
    "categories" : ["Announcements","Events","IT Pro/DevOps","Monitoring & Management","Supportability","Updates"],
    "tags" : ["MVP","Webinar","Live Event","Lumagate","Community","System Center","Operations Manager","Operations Management Suite","Virtual Machine Manager","Windows Azure Pack","Windows Azure Stack","Windows Server 2016","Windows Server 2012"],
    "authors" : ["Damian Flynn"],      
    "publishedDate" : "2017-01-11 17:42:00 +0000",
    "content" : "With the greatest honour, I am scheduled to present for the 4th year in succession at the original Experts Live! event which is once again taking place in the Cinemec, found in the center of Ede, The Netherlands. The event is bi-lingual, but with the vast majority of the presentations been delivered in English. The list of presenters is quite impressive with lots of recongnisable names from the Microsoft Datacenter, Enterprise and Cloud deciplines. Further information and tickets for the event are available at the event website, and are extremly good value at less that €100 for a full day of content, across a number of concurrent paths; ensuing there is lots of chocces for everyone. For further details please check out the event site located at (Experts Live! NL)[https://www.expertslive.nl] Presentations I am currently scheduled to present just a single session at the event, and will be joining some of the ask the experts sessions, as well as recording a show discussing Openstack and the Windows Server Fabric. Demystified: Azure Stack - Resource Provider’s Presentation Abstract The Azure Stack solution is designed to no longer leverage the capabilities of Virtual Machine Manager for managing the IaaS resources of the cloud. In this session we will take a close inspection to how the Compute, Network and Storage Resource Provider’s take direction from the Resource Manager. We will also get a glimpse on where these providers live in the current preview, and how they interact with both the fabric and other resource providers. Slide Deck Recording",
    
  },
  {    
    "id" : "http-0-0-0-0-4000-conference-mms-2017-minesota-1483901100",
    "site" : "damianflynn.com",
    "title": "Minnesota Management Summit (MMS) 2017",      
    "url": "http://0.0.0.0:4000/Conference-MMS-2017-Minesota/",                    
    "categories" : ["Announcements","Events","IT Pro/DevOps","Monitoring & Management","Supportability","Updates"],
    "tags" : ["MVP","Webinar","Live Event","Lumagate","Community","System Center","Operations Manager","Operations Management Suite","Virtual Machine Manager","Windows Azure Pack","Windows Azure Stack","Windows Server 2016","Windows Server 2012"],
    "authors" : ["Damian Flynn"],      
    "publishedDate" : "2017-01-08 18:45:00 +0000",
    "content" : "With the greatest honour, I am scheduled to present for the 4th year in succession at the original Experts Live! event which is once again taking place in the Cinemec, found in the center of Ede, The Netherlands. The event is bi-lingual, but with the vast majority of the presentations been delivered in English. The list of presenters is quite impressive with lots of recongnisable names from the Microsoft Datacenter, Enterprise and Cloud deciplines. Further information and tickets for the event are available at the event website, and are extremly good value at less that €100 for a full day of content, across a number of concurrent paths; ensuing there is lots of chocces for everyone. For further details please check out the event site located at (Experts Live! NL)[https://www.expertslive.nl] Presentations I am currently scheduled to present just a single session at the event, and will be joining some of the ask the experts sessions, as well as recording a show discussing Openstack and the Windows Server Fabric. Demystified: Azure Stack - Resource Provider’s Presentation Abstract The Azure Stack solution is designed to no longer leverage the capabilities of Virtual Machine Manager for managing the IaaS resources of the cloud. In this session we will take a close inspection to how the Compute, Network and Storage Resource Provider’s take direction from the Resource Manager. We will also get a glimpse on where these providers live in the current preview, and how they interact with both the fabric and other resource providers. Slide Deck",
    
  },
  {    
    "id" : "http-0-0-0-0-4000-conference-cloud-and-datacenter-2017-duseldorf-1483901100",
    "site" : "damianflynn.com",
    "title": "Cloud and Datacenter Conference 2017",      
    "url": "http://0.0.0.0:4000/Conference-Cloud-And-Datacenter-2017-Duseldorf/",                    
    "categories" : ["Announcements","Events","IT Pro/DevOps","Monitoring & Management","Supportability","Updates"],
    "tags" : ["MVP","Webinar","Live Event","Lumagate","Community","System Center","Operations Manager","Operations Management Suite","Virtual Machine Manager","Windows Azure Pack","Windows Azure Stack","Windows Server 2016","Windows Server 2012"],
    "authors" : ["Damian Flynn"],      
    "publishedDate" : "2017-01-08 18:45:00 +0000",
    "content" : "With the greatest honour, I am scheduled to present for the 4th year in succession at the original Experts Live! event which is once again taking place in the Cinemec, found in the center of Ede, The Netherlands. The event is bi-lingual, but with the vast majority of the presentations been delivered in English. The list of presenters is quite impressive with lots of recongnisable names from the Microsoft Datacenter, Enterprise and Cloud deciplines. Further information and tickets for the event are available at the event website, and are extremly good value at less that €100 for a full day of content, across a number of concurrent paths; ensuing there is lots of chocces for everyone. For further details please check out the event site located at (Experts Live! NL)[https://www.expertslive.nl] Presentations I am currently scheduled to present just a single session at the event, and will be joining some of the ask the experts sessions, as well as recording a show discussing Openstack and the Windows Server Fabric. Demystified: Azure Stack - Resource Provider’s Presentation Abstract The Azure Stack solution is designed to no longer leverage the capabilities of Virtual Machine Manager for managing the IaaS resources of the cloud. In this session we will take a close inspection to how the Compute, Network and Storage Resource Provider’s take direction from the Resource Manager. We will also get a glimpse on where these providers live in the current preview, and how they interact with both the fabric and other resource providers. Slide Deck",
    
  },
  {    
    "id" : "http-0-0-0-0-4000-conference-nicconf-2017-oslo-1483901100",
    "site" : "damianflynn.com",
    "title": "NIC Conference 2017, OSLO",      
    "url": "http://0.0.0.0:4000/Conference-NICCONF-2017-Oslo/",                    
    "categories" : ["Announcements","Events","IT Pro/DevOps","Monitoring & Management","Supportability","Updates"],
    "tags" : ["MVP","Webinar","Live Event","Lumagate","Community","System Center","Operations Manager","Operations Management Suite","Virtual Machine Manager","Windows Azure Pack","Windows Azure Stack","Windows Server 2016","Windows Server 2012"],
    "authors" : ["Damian Flynn"],      
    "publishedDate" : "2017-01-08 18:45:00 +0000",
    "content" : "With the greatest honour, I am scheduled to present for the 4th year in succession at the original Experts Live! event which is once again taking place in the Cinemec, found in the center of Ede, The Netherlands. The event is bi-lingual, but with the vast majority of the presentations been delivered in English. The list of presenters is quite impressive with lots of recongnisable names from the Microsoft Datacenter, Enterprise and Cloud deciplines. Further information and tickets for the event are available at the event website, and are extremly good value at less that €100 for a full day of content, across a number of concurrent paths; ensuing there is lots of chocces for everyone. For further details please check out the event site located at (Experts Live! NL)[https://www.expertslive.nl] Presentations I am currently scheduled to present just a single session at the event, and will be joining some of the ask the experts sessions, as well as recording a show discussing Openstack and the Windows Server Fabric. Demystified: Azure Stack - Resource Provider’s Presentation Abstract The Azure Stack solution is designed to no longer leverage the capabilities of Virtual Machine Manager for managing the IaaS resources of the cloud. In this session we will take a close inspection to how the Compute, Network and Storage Resource Provider’s take direction from the Resource Manager. We will also get a glimpse on where these providers live in the current preview, and how they interact with both the fabric and other resource providers. Slide Deck",
    
  },
  {    
    "id" : "http-0-0-0-0-4000-oms-and-teams-alerts-1479810600",
    "site" : "damianflynn.com",
    "title": "OMS Alerts for MS Teams",      
    "url": "http://0.0.0.0:4000/OMS-and-Teams-Alerts/",                    
    "categories" : ["IT Pro/DevOps"],
    "tags" : ["Azure Automation","Service Management Automation","SMA","GIT","Presentation","Continous Deployment","DevOps","Open Source"],
    "authors" : ["Damian Flynn"],      
    "publishedDate" : "2016-11-22 10:30:00 +0000",
    "content" : "At Tech-Ed 2014, Ryan Andorfer presented a session, and followed up with a series of blog posts on the ‘Building Clouds’ blog detailing how to implement a continous deployment version control solution for SMA leveraging a TFS Repository. A lot of the work we have been implementing with Service Management Automation (SMA) has been version protected using GIT, as it offers a much lighter experience. In this post I am going to introduce you to Ryan’s latest integration work, which is an evolution of the original efforts leveraging GIT and Azure Automation. We have many steps to cover, so lets get started.",
    
  },
  {    
    "id" : "http-0-0-0-0-4000-conference-experts-live-europer-2017-berlin-1477996200",
    "site" : "damianflynn.com",
    "title": "Experts Live 2017 Europe",      
    "url": "http://0.0.0.0:4000/Conference-Experts-Live-Europer-2017-Berlin/",                    
    "categories" : ["Announcements","Events","IT Pro/DevOps","Monitoring & Management","Supportability","Updates"],
    "tags" : ["MVP","Webinar","Live Event","Lumagate","Community","System Center","Operations Manager","Operations Management Suite","Virtual Machine Manager","Windows Azure Pack","Windows Azure Stack","Windows Server 2016","Windows Server 2012"],
    "authors" : ["Damian Flynn"],      
    "publishedDate" : "2016-11-01 10:30:00 +0000",
    "content" : "With the greatest honour, I am scheduled to present for the 4th year in succession at the original Experts Live! event which is once again taking place in the Cinemec, found in the center of Ede, The Netherlands. The event is bi-lingual, but with the vast majority of the presentations been delivered in English. The list of presenters is quite impressive with lots of recongnisable names from the Microsoft Datacenter, Enterprise and Cloud deciplines. Further information and tickets for the event are available at the event website, and are extremly good value at less that €100 for a full day of content, across a number of concurrent paths; ensuing there is lots of chocces for everyone. For further details please check out the event site located at (Experts Live! NL)[https://www.expertslive.nl] Presentations I am currently scheduled to present just a single session at the event, and will be joining some of the ask the experts sessions, as well as recording a show discussing Openstack and the Windows Server Fabric. Demystified: Azure Stack - Resource Provider’s Presentation Abstract The Azure Stack solution is designed to no longer leverage the capabilities of Virtual Machine Manager for managing the IaaS resources of the cloud. In this session we will take a close inspection to how the Compute, Network and Storage Resource Provider’s take direction from the Resource Manager. We will also get a glimpse on where these providers live in the current preview, and how they interact with both the fabric and other resource providers. Slide Deck",
    
  },
  {    
    "id" : "http-0-0-0-0-4000-conference-experts-live-nl-2016-1477996200",
    "site" : "damianflynn.com",
    "title": "Experts Live NL 2016",      
    "url": "http://0.0.0.0:4000/Conference-experts-live-nl-2016/",                    
    "categories" : ["Announcements","Events","IT Pro/DevOps","Monitoring & Management","Supportability","Updates"],
    "tags" : ["MVP","Webinar","Live Event","Lumagate","Community","System Center","Operations Manager","Operations Management Suite","Virtual Machine Manager","Windows Azure Pack","Windows Azure Stack","Windows Server 2016","Windows Server 2012"],
    "authors" : ["Damian Flynn"],      
    "publishedDate" : "2016-11-01 10:30:00 +0000",
    "content" : "With the greatest honour, I am scheduled to present for the 4th year in succession at the original Experts Live! event which is once again taking place in the Cinemec, found in the center of Ede, The Netherlands. The event is bi-lingual, but with the vast majority of the presentations been delivered in English. The list of presenters is quite impressive with lots of recongnisable names from the Microsoft Datacenter, Enterprise and Cloud deciplines. Further information and tickets for the event are available at the event website, and are extremly good value at less that €100 for a full day of content, across a number of concurrent paths; ensuing there is lots of chocces for everyone. For further details please check out the event site located at (Experts Live! NL)[https://www.expertslive.nl] Presentations I am currently scheduled to present just a single session at the event, and will be joining some of the ask the experts sessions, as well as recording a show discussing Openstack and the Windows Server Fabric. Demystified: Azure Stack - Resource Provider’s Presentation Abstract The Azure Stack solution is designed to no longer leverage the capabilities of Virtual Machine Manager for managing the IaaS resources of the cloud. In this session we will take a close inspection to how the Compute, Network and Storage Resource Provider’s take direction from the Resource Manager. We will also get a glimpse on where these providers live in the current preview, and how they interact with both the fabric and other resource providers. Slide Deck",
    
  },
  {    
    "id" : "http-0-0-0-0-4000-user-group-webinar-1477945800",
    "site" : "damianflynn.com",
    "title": "WMUG Webinar",      
    "url": "http://0.0.0.0:4000/user-group-webinar/",                    
    "categories" : ["IT Pro/DevOps"],
    "tags" : ["Service Management Automation","SMA","Windows Azure Pack","Windows Azure Stack","Continous Deployment","DevOps"],
    "authors" : ["Damian Flynn"],      
    "publishedDate" : "2016-10-31 20:30:00 +0000",
    "content" : "I am delighed to be invited as a guest presenter for the Windows Mangement User Group, for thier November 2016 online Webinar. This dutch event is open to everyone to join and will be presented in English (well seriosuly, can you imagine what my Irish Dutch would sound like?!!?) As an online event, everyone is welcome to join and I am following in the foot steps of some amazing people whom have presented over the years. Further information and details of the Webinar are available at the User Groups website. As this event is totally free, and will only take an hout of your life, block off your calander and join us for an interactive session on Wednesday evening, November 16th. For further details please check out the user groups site located at (WMUG NL)[https://www.expertslive.nl] Presentations 1 full hour of interaction as we take a look at Windows Server and System Center 2016, in its private cloud mode. Head to Head: Azure Pack and Azure Stack Presentation Abstract Azure Pack is built on the proven System Center stack, and will be supported by Microsoft until 2022; Azure Stack brings the public resource manager on premise, with fabric resource providers. Learn how these products work, what the share in common, and how to differ. Each solution has a place in our data-centers, learn which is the correct solution for your implementation, and why! https://1drv.ms/f/s!AlO5kKw89zUis_UivjXvPfoJ9IVw1A Slide Deck",
    
  },
  {    
    "id" : "http-0-0-0-0-4000-system-center-universe-2016-iot-in-the-enterprise-1472812200",
    "site" : "damianflynn.com",
    "title": "System Center Universe 2016: IoT in the Enterprise",      
    "url": "http://0.0.0.0:4000/system-center-universe-2016-iot-in-the-enterprise/",                    
    "categories" : ["IT Pro/DevOps"],
    "tags" : ["Azure Automation","Service Management Automation","SMA","GIT","Presentation","Continous Deployment","DevOps","Open Source"],
    "authors" : ["Damian Flynn"],      
    "publishedDate" : "2016-09-02 10:30:00 +0000",
    "content" : "IoT in the Enterprise Speaker: Damian Flynn Abstract Surrounding all the Hype, can the Internet of Things really offer value. How do we expose data to the analytics’ systems from our generally non-connected equipment. In this session we will learn about real world scenarios where IoT has been put into practice, and gain insights to the weird and wonderful places which data has been collect from to offer new insights. We will also take a look at the process of connecting up industrial instrumentation to the Internet of Thing’s to begin the road to enablement and insight",
    
  },
  {    
    "id" : "http-0-0-0-0-4000-azure-automation-with-git-obsidian-copy-1472725800",
    "site" : "damianflynn.com",
    "title": "SMA: GIT Version Control Integration",      
    "url": "http://0.0.0.0:4000/azure-automation-with-git-OBSIDIAN-Copy/",                    
    "categories" : ["IT Pro/DevOps"],
    "tags" : ["Azure Automation","Service Management Automation","SMA","GIT","Presentation","Continous Deployment","DevOps","Open Source"],
    "authors" : ["Damian Flynn"],      
    "publishedDate" : "2016-09-01 10:30:00 +0000",
    "content" : "At Tech-Ed 2014, Ryan Andorfer presented a session, and followed up with a series of blog posts on the ‘Building Clouds’ blog detailing how to implement a continous deployment version control solution for SMA leveraging a TFS Repository. A lot of the work we have been implementing with Service Management Automation (SMA) has been version protected using GIT, as it offers a much lighter experience. In this post I am going to introduce you to Ryan’s latest integration work, which is an evolution of the original efforts leveraging GIT and Azure Automation. We have many steps to cover, so lets get started. GIT Repository The first step is to ensure you have registered an account on GitHub. We are not going to start from point zero, as that would be simply re-inventing the wheel; instead we are going to make a copy of the great work which Ryan has been working on, an use this as a starting point. We have two options to choose from in order to get started, which are Forking the Repository Cloning the Repository, to be used as a base for creating a new repository I will cover both approaches, the Forking option is preferred as it leads to more comunity focus, and allows up to share back any fixes we might add to the solution. However sometimes this is not the focus and the Clone approach may work better. Forking ScorchDev The work we are going to leverage is already published by Ryan in his public GitHub repository which is hosted as RAndorfer/ScorchDev, or you can also fork from my personal copy as DamianFlynn/ScorchDev_MMS2016 We will use one of GitHub’s features know as ‘Forking’ while will crate a copy of the repository in our account. We will take this approach as we will not have write access to Ryan’s copy of the repository, and we are going to need to customise the configurations stored in the repository for our environment. If we encounter bugs in the solution, we will also be able to resolve these, and then create a ‘Pull’ request to have Ryan check the suggested fixes and decide if these should be pulled into the main code. To Fork the repository, we will follow these steps: Navigate to GitHub.com and Sign In with your account to the service. Next, we will navigate to Ryans repository which is located at RAndorfer/ScorchDev or my copy hosted at DamianFlynn/ScorchDev_MMS2016. With the repository now presented, we will click on the button (currently in the upper left of the screen) labeled as Fork GitHub will take you back to your account, where you will now have a copy of the project, and an indicator that this is a fork from the upstream repository. Cloning ScorchDev to our Development Station Now that we have our copy of the ScorchDev repository hosted in our GitHub account, we will need to clone a copy of this locally so we can correctly setup our development workstation. Note: Before you proceed you will need to have a copy of Git for Windows installed on your workstation! Create or Navigate to a working folder which we will use as the base for where we are going to clone our repositories to, for example I will use C:\\git on my system Now, we will clone a copy of the repository and any submodules which it may contain to our workstation using the following command git clone --recursive https://github.com/damianflynn/ScorchDev. Of course you will replace my account name ‘damianflynn’ with your account right! After a few minutes, we should have a copy of the repository on our machine. Clone and Rebase The alternative approach to forking, is to create a local clone of the repository, and then from there delete the existing .git file, and initialize a new version of the repository as your new base. Note: Before you proceed you will need to have a copy of Git for Windows installed on your workstation! Cloning the repository locally Create or Navigate to a working folder which we will use as the base for where we are going to clone our repositories to, for example I will use C:\\git on my system Now, we will clone a copy of the repository and any submodules which it may contain to our workstation using the following command git clone --recursive https://github.com/damianflynn/ScorchDev_MMS2016 ScorchDev. After a few minutes, we should have a copy of the repository on our machine. Removing the GIT artifacts With the repository now local, we will proceed to remove all the Git related artifacts from this copy. This will erase all the history and the relationship which this copy has with the hosted version we just cloned from (commonly refered to as the upstream master) cd c:\\git\\ScorchDev del -Recurse -Force .\\git\\ Initialize a new Instance Next step is to reinitialize the copy, as a new Git repository. Once this is complete, we can also proceed to create a project in our github account, and push the content back up to this project. git init Using the Web Interface, login to your GitHub account and select the option to Create a new repository, this will request some simple details from you, including The Repository name, for example ‘ScorchDev’ A Description, for example ‘Continous Integation for Azure Automation’ Wheather the project should be Public or Private, which is totally up to your requirements If you wish to Initialize with a README, and add a .gitignore and/or license to the repoisitory, in this case none of these are required as they are part of the copy we are about to push up. Once this is all in place, we simply click the option to Create Repository. This will take a few moments, and you should then see some instructions on how to push your content back up. In our case, the example will look similar to the following git remote add origin https://github.com/DamianFlynn/ScorchDev.git git push -u origin master Updating our PowerShell Profile With the local copy of the repository now available, we will proceed to update our PowerShell Profile to automatically load the Local Authoring environment support modules which are included in the repoistory. These will save lots of time, as we proceed to create and debug both runbooks and thier associated assets. Looking around the content of the repoistory, you will find a folder called Profile which contains a sample of the information which we should add to our live profile. The sample is really in 2 parts, A hash table defining the workspaces (repositories) we will be working with. At minumum we will have 1 workspace which is our ScorchDev repository A Loop to process the details in the hashtable, updating our PowerShell profile with pointers to the modules hosted in each of the workspaces. The configuration hash table The mimimum hast table will look similar to the following example, which is an extract from the profile.ps1 file. Here we can see the name of the workspace (or repoistory) which we are defining, along with some details to how the workspace is organised. First we are calling the Workspace SCOrchDev. Within this Workspace we first define where we cloned the repository to under the variable Workspace. The remaining Variables, define the folder name within the repository which will be host for our Runbooks, Modules, Global Settings and Local Development resources. $Global:AutomationWorkspace = @{ 'SCOrchDev' = @{ 'Workspace' = 'C:\\GIT\\SCOrchDev' 'ModulePath' = 'PowerShellModules' 'GlobalPath' = 'Globals' 'LocalPowerShellModulePath' = 'LocalPowerShellModules' 'RunbookPath' = 'Runbooks' } } The remaining content of the sample profile.ps1 file, will process this hast table and update our powershell environment with the relevant paths to publish the contained modules Foreach($_AutomationWorkspace in $Global:AutomationWorkspace.Keys) { $PowerShellModulePath = \"$($Global:AutomationWorkspace.$_AutomationWorkspace.Workspace)\\$($Global:AutomationWorkspace.$_AutomationWorkspace.ModulePath)\" $LocalPowerShellModulePath = \"$($Global:AutomationWorkspace.$_AutomationWorkspace.Workspace)\\$($Global:AutomationWorkspace.$_AutomationWorkspace.LocalPowerShellModulePath)\" if(Test-Path -Path $PowerShellModulePath) { $env:PSModulePath = \"$PowerShellModulePath;$env:PSModulePath\" } if(Test-Path -Path $LocalPowerShellModulePath) { $env:PSModulePath = \"$LocalPowerShellModulePath;$env:PSModulePath\" } } $env:LocalAuthoring = $true $Global:AutomationDefaultWorkspace = 'SCOrchDev' # Set up debugging $VerbosePreference = 'Continue' $DebugPreference = 'Continue' Now that we understand what we are about to add to our profile, and why, we can ensure that the hash table is correctly defined for our workstation folders, and implement these changes. If you are using the recommended PowerShell ISE, then all you should need to do is type in the interactive window psedit $profile which will open a new pane with your current profile, or a new blank profile if one is not already defined. Here we will add the snippets we just reviewed and edited as appropriate. Once you are finished, save the changes to the profile. Remember that you will need to shutdown and restart the ISE to have it load up the profile on the next startup cycle. Before we move on with the work, we will run a quick check to ensure that all the modules are available to us now based on the new profile settings. Back in the interactive window we can query for the list of modules which are now offered to us, which should include a number of modules starting with the name ScorchDev. If these are not offered, then most likely you will have a mistake in your hash table paths; which you will need to resolve before we proceed. get-module -name SCOrchDev* -listavailable The results we are expecting would be similar to the following output Directory: C:\\GIT\\SCOrchDev\\LocalPowerShellModules ModuleType Version Name ExportedCommands ---------- ------- ---- ---------------- Script 1.0.1 SCOrchDev-Scaffold New-Scaffold Directory: C:\\GIT\\SCOrchDev\\PowerShellModules ModuleType Version Name ExportedCommands ---------- ------- ---- ---------------- Script 3.0.4 SCOrchDev-AzureAutomationIntegra... {Publish-AzureAutomationRunbookChange, Publish-AzureAutomationPowerShellModule, Publish-AzureAut... Script 2.2.1 SCOrchDev-Exception {Write-Exception, New-Exception, Convert-ExceptionToString, Get-ExceptionInfo...} Script 2.1.0 SCOrchDev-File {New-ZipFile, New-TempDirectory, Get-FileEncoding, ConvertTo-UTF8...} Script 2.2.5 SCOrchDev-GitIntegration {New-ChangesetTagLine, Get-GlobalFromFile, Update-RepositoryInformationCommitVersion, Get-GitRep... Script 2.1.1 SCOrchDev-PasswordVault {Get-PasswordVaultCredential, Set-PasswordVaultCredential, Remove-PasswordVaultCredential} Script 0.1.1 SCOrchDev-StoredCredential Get-StoredCredential Script 2.1.9 SCOrchDev-Utility {Format-ObjectDump, ConvertTo-Boolean, Select-FirstValid, Find-DeclaredCommand...} Preparing for Continuous Deployment With our development system now ready, we can now focus on updating the variables which the SCOrchDev Continuous Deployment solution will leverge to deliver its service. There are a number of predefined variables we need to declare, including: Azure subscription details and access Automation resource group details Git Repository information to link for the CI. Defining the continuous deployment configuration Variables The variables we are going to focus on, are all hosted in a folder of our new solution, called Globals, within which we will locate a JSON formated file, called zzGlobal.json. To begin, we will open and edit this file with our favourite code editor, for example I will launch Visual Studio Code as follows cd ScorchDev\\Globals code .\\zzGlobal.json With the file open, we can begin the process of customisation. Our initial variable, zzGlobal-RunbookWorkerAccessCredentialName is used to identify the credential object which will have local administrative privilages to the on premise resources. In the scenarion where we are dealing with domain resources this will be defined in Universal Principal Name Format (UPN), and for Local accounts we will just use the short account name. \"zzGlobal-RunbookWorkerAccessCredentialName\": { \"isEncrypted\": false, \"Description\": \"The credential name for accessing runbook workers\", \"Value\": \"automation@damianflynn.com\" }, The next variable, zzGlobal-SubscriptionAccessCredentialName identifies the credential object which has privilages to your Azure Subscription; which will be leveraged to provision your Azure Automation services. \"zzGlobal-SubscriptionAccessCredentialName\": { \"isEncrypted\": false, \"Description\": \"Name of the credential to be used for authenticating to the subscription\", \"Value\": \"azureadmin@damianflynn.com\" }, Azure Automation is part of the Microsoft Operations Management Suite (OMS), and requires your Log Analythics workspace ID to enable the solution with access to the Hybrid Workers. \"zzGlobal-WorkspaceID\": { \"isEncrypted\": false, \"Description\": \"the workspace ID of the Log Analytics account with the automation solution deployed to it\", \"Value\": \"01234567-abcd-efab-0123-0123456789ab\" }, As you may have multiple azure Subscriptions, the variable zzGlobal-SubscriptionName enables us to identify which subscription we will deploy the Azure Automation account to. \"zzGlobal-SubscriptionName\": { \"isEncrypted\": false, \"Description\": \"\", \"Value\": \"My MVP Subscription\" }, Leveraging the variable zzGlobal-AutomationAccountName we can provide a name for our new Azure Automation account \"zzGlobal-AutomationAccountName\": { \"isEncrypted\": false, \"Description\": \"name of the automation account\", \"Value\": \"SCOrchDev\" }, Similarly, the variable zzGlobal-StorageAccountName premits us to define the name of the storage account we will associate with the Automation Account \"zzGlobal-StorageAccountName\": { \"isEncrypted\": false, \"Description\": \"Name of a storage account for storing PSModules during import\", \"Value\": \"damianscorchdev\" }, Workers, connected to the automation account, hosted on premise will be allocated to the hybrid group identifed bu the variable zzGlobal-HybridWorkerGroup \"zzGlobal-HybridWorkerGroup\": { \"isEncrypted\": false, \"Description\": \"name of the hybrid worker group for starting sync job on\", \"Value\": \"Hybrid\" }, All the resources we have defined for our subscription so far, will be associatated with the resource group identified by the variable zzGlobal-ResourceGroupName \"zzGlobal-ResourceGroupName\": { \"isEncrypted\": false, \"Description\": \"resource group that the target automation account is stored in\", \"Value\": \"SCOrchDev\" }, For continous deployment to function, we will use the variable zzGlobal-GitRepository to identify the GIT reporitories, which will be syncronised to the automation account. Multiple Git repositories can be defined as key value pairs for this variable. \"zzGlobal-GitRepository\": { \"isEncrypted\": false, \"Description\": \"a key value pair of repositories and their respective branches to sync into this automation account\", \"Value\": \"{\\\"https://github.com/damianflynn/SCOrchDev_MMS2016\\\":\\\"master\\\"}\" }, The variable zzGlobal-GitRepositoryCurrentCommit is used to track the latest commit which has been syncronised to the automation account. This varaible will initially have a -1 value, which will instruct the runbooks to get the lastest version, after which the runbooks will update and manage this variable automatically. \"zzGlobal-GitRepositoryCurrentCommit\": { \"isEncrypted\": false, \"Description\": \"a key value pair of repositories and their current commit. -1 for initial\", \"Value\": \"{\\\"SCOrchDev_MMS2016\\\":\\\"-1\\\"}\" }, Finally, we will define the zzGlobal-LocalGitRepositoryRoot variable to identify the local path as to where the \"zzGlobal-LocalGitRepositoryRoot\": { \"isEncrypted\": false, \"Description\": \"the local path to map the git repositories to\", \"Value\": \"c:\\\\git\" }, Once all the varibales have been defined to match our subscription details, we will save the file edits and commit them to our copy of the repository, with a comment cd \\git\\ScorchDev git add -A :/ git commit -m \"Updated the Configuration Variables to match the subscriptions\" Credentials Our next step is to create some credential objects to host the account details which will be used within the solution. There are a total of three which we will define. We will store these in two locations, Initially in the local Windows Password Valut, and also as atrifacts in the Azure Automation account. RunbookWorkerAccessCredentialName SubscriptionAccessCredentialName OMS Subscription and Primary Key Windows Password Vault To define and store the credentials in the local password vault, we simply create a credental object, and place the details in the object; then we store that object in the local password vault with a helper function called Set-PasswordVaultCredential as follows $cred = get-credential Set-PasswordVaultCredential $cred The first two we will set, are pretty standard, providing the shortname, or UPN for both the RunbookWorkerAccessCredentialName and SubscriptionAccessCredentialName. The third and finaly one we will save is the Log Analythics Subscription ID and its private key. In this case the username will be Subscription ID, and the password should be the Private Key as we provide the credentials. Once all of the credentials have being defined, and stored, we can check the vault to ensure that everything is working as expected by using the helper command Get-PasswordVaultCredential Deploy At this point we have now completed all the required configuration work in the solution, and can proceed with deploying the framework. This will acomplish quite a lot for us, including Building out the Azure Automation account Creating a Azure Storage blob to host the PS Modules we may use And deploying a runbook which will monitor our Git Repository To start the process, we will call our deployment script as follows: Note: It is imperitave that you Change into the directory, and not launch indirectly due to path mappings cd \\Git\\ScorchDev\\ARM\\\\automation-sourcecontrol ./psDeploy.ps1 Edits and Clippings Defining the Runbook Workers Web Endpoints All of our runbooks will at some point need to communicate with the Automation engine environment which is hosting it, for example just to query or update our global variables. Therefore we need to define what the endpoints for the REST API which the runbooks should communicate with for the hosting environment. These will generally change depending on your environments, for example Staging verus Production. Note: Its also possible that you might just want to refer to the environment as “https://localhost” but only if you have the web REST APIs also deployed to each of your runbook servers. WebserviceEndpoint : “https://sma.diginerve.net” WebservicePort : “9090” Set-AutomationVariable -Prefix ContinuousDeployment -Name WebserviceEndpoint -Value \"https://sma.diginerve.net\" -Description \"Local SMA Instance URI\" Set-AutomationVariable -Prefix ContinuousDeployment -Name WebservicePort -Value \"9090\" -Description \"Local SMA Instance Port\" Get-AutomationVariable -Name ContinuousDeployment-WebserviceEndpoint Get-AutomationVariable -Name ContinuousDeployment-WebservicePort Defining the Runbook Workers Credentials CredentialName : “WIN-CHTKSP77JVN\\Administrator” Set-AutomationVariable -Prefix ContinuousDeployment -Name CredentialName -Value \"WIN-CHTKSP77JVN\\\\Administrator\" -Description \"Local SMA Instance URI\" Get-AutomationVariable -Name ContinuousDeployment-WebserviceEndpoint $credential = Get-Credential -Message \"Credentials for Local Development SMA REST Access\" -UserName \"WIN-CHTKSP77JVN\\Administrator\" Set-PasswordVaultCredential -Credential $credential -Resource 'LocalDev' Get-PasswordVaultCredential Defining the Integration Runbook Montitor Life Time The solution is built on the concept of Continous deployment, and thus we need some paramaters to define what this really means. Also based on best practices a runbook should never just run for ever, but instead kill it self, and spwan a new instance to keep everything fresh and clean; and make reviewing logs a ton load easier also! The following Variables are defined to set the life time settings related to the runbooks existance MonitorLifeSpan : 300 The period in Minutes for which this runbook will run, after which it will attempt to commit suicide and kill it self, but will start a new instance before dieing MonitorCheckpoint : 5 How often should the runbook execute a checkpoint during it life MonitorDelayCycle : 30 The period of time to wait between executions in seconds Set-AutomationVariable -Prefix ContinuousDeployment -Name MonitorLifeSpan -Value 300 -Description \"Local SMA Instance URI\" Set-AutomationVariable -Prefix ContinuousDeployment -Name MonitorCheckpoint -Value 5 -Description \"Local SMA Instance Port\" Set-AutomationVariable -Prefix ContinuousDeployment -Name MonitorDelayCycle -Value 30 -Description \"Local SMA Instance Port\" Get-AutomationVariable -Name ContinuousDeployment-MonitorLifeSpan Get-AutomationVariable -Name ContinuousDeployment-MonitorCheckpoint Get-AutomationVariable -Name ContinuousDeployment-MonitorDelayCycle Defining Deployment Repository Information The last, and probably most important variable we will set, is used to tell the Continous Integration runbook what repoisitories it will be monitoring, the branch of the repository which is applicable to this environment, and where to keep the working copies on the files on the runbook server. NOTE: The working copy of the repositories are used for GIT to check out the latest version of the code, and once complete, this code is then imported into service management automation. The architecture of the integration runbook, is designed to support multiple repositories. To simplify the configuration, we can use the very flexiable hash table functions in powershell again, to define the required paramaters which the integration workflow leverages. For each repoisitory, we need to define the following settings Path : C:\\SMAData\\ScorchDev The Location on the Runbook Worker which should be used as the staging area; The runbook will use this path as the clone location for the GIT Repoistory, and then as the source location to import the resources into SMA GlobalsFolder : Globals In the Local Development Environment as we define Global Assets, these are stored in a JSON file; this path indicates the name of the folder within the repository which hosts the JSON File. RunbookFolder : Runbooks In the Local Development Environment as we define Runbooks for the repository, we will keep these organised under one main folder; this path indicates the name of the folder within the repository which hosts the runbooks. PowerShellModuleFolder : PowerShellModules In the Local Development Environment as we define PowerShell Modules for the repository, we will keep these organised under one main folder; this path indicates the name of the folder within the repository which hosts the modules. RepositoryPath : https://github.com/damianflynn/ScorchDev.git The URI to the Repoistory which is to feed this workspace Branch : master The Branch of the repository which is to be deployed for this workspace CurrentCommit : -1 The Current Commit level of the repository which is located on the worker. Initially there is no clone, we will simply set this to ‘-1’ and the runbook will maintain this after the first pull. The following is an example of just the main continous integration workflow. NOTE: The RepositoryPath which is defined below is pointing to a public GIT repository hosted on GitHub. You can easily change this to point at any other GIT repoistory, for example one hosted as part of TFS, or an internal Enterprise GIT Server, Just ensure that the Runbook Worker has access to reach the repository. NOTE: In the event the repoistory is Private, you will need to provide credentials to access the repository. In this case we recommend you create a user account with access to the repository which will be used only for the integration pulls. Then encode the username and password as part of the URI, for example ‘https://Username:Password@/damianflynn/ScorchDev.git’ $RepositoryInformation = @{ 'ContinuousDeployment' = @{ 'Path' = 'C:\\GIT\\ContinuousDeployment' 'RepositoryPath' = 'https://github.com/damianflynn/ScorchDev.git' 'RunbookFolder' = 'Runbooks' 'PowerShellModuleFolder' = 'PowerShellModules' 'Branch' = 'master' 'CurrentCommit' = '-1' } } Now, once we have defined this table, to store the details in a Variable, we will simply convert the table to a nice clean JSON string, and save it to our local configuration environment. ConvertTo-Json -InputObject $RepositoryInformation Set-AutomationVariable -Prefix ContinuousDeployment -Name RepositoryInformation -Value (ConvertTo-Json -InputObject $RepositoryInformation) -Description \"Repository Details for Continuous Deployment\" Get-AutomationVariable -Name ContinuousDeployment-RepositoryInformation Checking in the updated configuration With the configuration complete, we can now add the changes we just completed to the active branch, for now this is just ‘master’ as we have not discussed branches yet. Then commit these changes with a comment, and finally push the changes to our hosted repoisitory. git add -A :/ git commit -m \"Updated the Configuration Settings in the Local Development Environment to match the runbook hosts\" git push Preparing our SMA Server for Continous deployment Now that we have our copy of the ScorchDev repository hosted in our GitHub account, and updated it to represent the runbook environment configuration, we will need to clone a copy of this locally to just one of the runbook workers in this farm so we can deploy the continus integration services. Note: Before you proceed you will need to have a copy of Git for Windows installed on your workstation! Create or Navigate to a working folder which we will use as the base for where we are going to clone our repositories to, for example I will use C:\\git on my system Now, we will clone a copy of the repository and any submodules which it may contain to our workstation using the following command git clone --recursive https://github.com/damianflynn/ScorchDev. Of course you will replace my account name ‘damianflynn’ with your account right! After a few minutes, we should have a copy of the repository on our machine. Installing the Modules and Runbooks on the Worker First, lets get the modules added to our PowerShell search path $ContinousModules = \"C:\\git\\ScorchDev\\PowerShellModules\" $env:PSModulePath = \"$($env:PSModulePath);$ContinousModules\" Now lets register the installer in our session so we can begin the implemention cd \\git\\ScorchDev\\Installer .\\Deploy-Integration.ps1 Defining the Runbook Workers Credentials CredentialName : “WIN-CHTKSP77JVN\\Administrator” Set-AutomationVariable -Prefix ContinuousDeployment -Name CredentialName -Value \"WIN-CHTKSP77JVN\\\\Administrator\" -Description \"Local SMA Instance URI\" Get-AutomationVariable -Name ContinuousDeployment-WebserviceEndpoint $credential = Get-Credential -Message \"Credentials for Local Development SMA REST Access\" -UserName \"WIN-CHTKSP77JVN\\Administrator\" Set-PasswordVaultCredential -Credential $credential -Resource 'LocalDev' Get-PasswordVaultCredential Add the Credential to the SMA Store also Using the UI, we can add the following credential object Credential Type : PowerShell Credential Name : WIN-CHTKSP77JVN\\Administrator Note this Name Must Match the Value in the ContinuousDeployment-CredentialName Variablevvv Description : Credentials with access to the Current Runbook Farm REST API User Name : The Windows Account which has access to the REST API Password : The associated password for the Windows Account Deploying the Integation Note: This MUST be ran from the Installer folder, as the scripts use relative paths to locate thier resources currently. C:\\git\\ScorchDev\\Installer&gt; Deploy-Integration -currentcommit -1 -repositoryname ContinuousDeployment -Credential $credental Assuming this completed without any issues, we should now see that there are a number of new variable assets in the SMA environment representing the variables we defined earlier, and also we should see the integration runbook. Starting the Integration Now, simply start the Runbook Monitor-SourceControlChange, and if you like to montior the progress, then simply watch the job progress. The workflow has verbose output but by default when deployed this logging is turned off, so you may also choose to enable this for the first few days to observe the flow processing. Next Steps Remember, that your new disk will not typically be available to the VM if it reboots unless you write that information to your fstab file. If you want, you can add several more disks and configure RAID. To learn more about Linux on Azure, see: Linux and Open-Source Computing on Azure How to use the Azure Command-Line Interface Deploy a LAMP app using the Azure CustomScript Extension for Linux The Docker Virtual Machine Extension for Linux on Azure",
    
  },
  {    
    "id" : "http-0-0-0-0-4000-starting-in-incubation-1472023800",
    "site" : "damianflynn.com",
    "title": "Starting in Incubation , Hello Lumagate ",      
    "url": "http://0.0.0.0:4000/Starting-in-Incubation/",                    
    "categories" : ["Announcements","Updates"],
    "tags" : ["Achivements","Career","Lumagate","Community"],
    "authors" : ["Damian Flynn"],      
    "publishedDate" : "2016-08-24 07:30:00 +0000",
    "content" : "This morning I am proud and delighted to announce that I am now officially part of the amazing energy of the Lumagate team. I will continue be based in Ireland, but will be working close with the Lumagate team across the Nordics and throughout Europe, exercising my favourite motto – “Making Incredible Technology, Incredibly Simple!” Learn more, about my new role and focus, by clicking below and reading the official press release. Lumagate Press Release Lumagate officially welcomes Damian Flynn, a long-time Microsoft’s Most Valued Professional and Cisco Champion, as an integral part of the Incubation team. A team solely dedicated to develop and package solutions for our customers with the latest and greatest technology from Microsoft. Of course, Now that I am no longer part of an Enterprise organisation, please feel free to reach out and lets engage! - “Beam me up Scotty!”",
    
  },
  {    
    "id" : "http-0-0-0-0-4000-unemployeed-after-18-years-1470658035",
    "site" : "damianflynn.com",
    "title": "Unemployeed after 18 Years",      
    "url": "http://0.0.0.0:4000/Unemployeed-After-18-Years/",                    
    "categories" : ["Announcements","Updates"],
    "tags" : ["Achivements","Career","Lionbridge","ITIL"],
    "authors" : [],      
    "publishedDate" : "2016-08-08 12:07:15 +0000",
    "content" : "It’s a Monday morning, and I am sitting on the ferry from Holyhead to Dublin with my wife and daughter, on the way home from a lovely vacation. What makes this trip different however is that today I am returning home in the knowledge that my last day being employed has just passed last Friday, August 5th (1 day after celebrating 19 years of marriage.) End of an Era After what could be considered a life time, 18.5 years, I have made the decision that the time is right to stop, and refocus on the things which I feel rewarding, motivating, and reignite my passion; helping me to grow personally, while also providing a platform to help others grow as well. This sadly was the primary sentiment which made my job so exciting, however over the last 4 years something broke and we have fallen into a rut, reinventing the wheel and making a decreasing amount of progress, quarter after quarter. Now don’t get me wrong, some amazing objectives have been accomplished in that time, many which I am quite proud of, but we could have achieved a lot more, and without all the cost was losing some really good talent (and friends) to new opportunities. Despite many failed attempts to reinvigorate and address the issues directly, the time finally felt right to address with a different approach. With over 2 years in deliberation, this decision was not one which could be, or was taken lightly as I have gained a lot of fantastic contacts, unmeasurable experience, and both good friends and work colleagues during my tenure. Looking back… My Career has been varied and interesting, joining as the IT manager for a new office which was yet to be even built! Once completed, just 2 years later I was relocating with my wife to sunny California, overseeing the move of the organisations testing service headquarters and assuming the role of World Wide IT Manager for the VeriTest division. Working in this role for a number of years I was lucky to have had many opportunities to travel around most of the globe and meet so many members of the business and IT teams in the organisation, building lasting relationships as we worked together to create solutions which not only benefited the respective locations, but also sharing and implementing these technologies with the cooperation and support of the teams in each of the other locations. We grew and shared as a cohesive and technologically driven group, always focused on delivering the business objectives with lean and streamlined efficiency. As time passed, and Lionbridge completed many acquisitions, I gained a vast amount of exposure to the semantics of evolving business objectives, expansion and consolidation of teams. Circa 2003/4 we begun the challenge which was to become the platform which excelled my destiny for the following decade. The path begun with the ominous challenge of normalising the IT platform of the complete organisation, consolidating 100’s of domains, multiple messaging and intranet platforms to ultimately extend the fabric of VeriTest globally; while also centralising services to our first true Data Center. The following years were hard and fast as we delivered many more centralised services for various aspects of the business, spanning operations to finance, but the biggest change was the replacement of a terrible leased line WAN service with an Internet hosted VPN mesh build in Cisco’s DMVPN solution. The Investment dwarfed the return in both financial and level of service in under a year, and amazingly is still forms the core of the backbone still running today! Holding the post of Global IT Manager, we had the opportunity to bring all 100 or so IT staff from across the globe to Boston for our first summit, focused on team building and crafting a road map for the following years. This event still holds many fond memories even despite all the challenges we experienced due to a terrible orchestra of snoring, as we quickly learned while sharing rooms. Focus changed in 2007 as I was again challenged to find even more opportunities for the business to save on capital expenditure while continuing to optimise on efficiency. This begun with the first steps in virtualisation and centralised Remote Desktop services, which paved the road to my first experiences with the Microsoft Technology Adoption Program (TAP). The enabling project we identified for this challenge I still fondly remember as a testing project for Microsoft which was been executed in multiple offices around the globe, each location normally requiring vast amounts of hardware to be leased and configured just to provision the platform upon which the works would be executed at no small expense. We of course experienced large variances with the cohesiveness and standard implementation of the platform between locations, often resulting in unnecessary rework. Leveraging a sibling technology code names Viridian (now commonly known as Hyper-V) and a yet unknown product, which was to become Virtual Machine Manager, we worked vigorously with the product groups to deliver an extremely cost effective platform and a rudimentary self-service experience with VCR type controls which was release as the VMM Self Service Portal. The learnings and experience form this project provided the foundation for our first endeavour into Private Cloud, and highlighted clearly why NIST defines 7 components to a true private cloud solution. The largest challenges I remember from that first postmortem was elasticity and automation; for what are now very obvious reasons, but we had to learn some way!! My first MVP award, as a result of the work with the product groups, and public speaking engagements related to our experiences arrived in 2011. It did not take long to realise that there was so much more to learn, and of course share. Being part of the TAP enabled the opportunity to help so many more people as the products became Generally available. Before long, My very good friend Aidan Finn then done the unthinkable and before I knew it, I was head deep in authoring content for my first ever book (I think I still have scar’s from that experience). This exercise proved to be the first of what currently stands at 3 books (one of which had its own second revision to add to the count). In the most recent years, while holding the position of Infrastructure Architect, the number of active projects grew exponentially, and I found myself building Proof Of Concept solutions, before pairing up with different team members to mentor and guide them as we implemented and deployed services ranging from ITSM, to Messaging, to Automation. Building a cohesive relationship with the development and quality assurance teams, we focused on the refactoring many of the line of business applications to deliver federated identity support and single sign on; spending much time training, mentoring and building sample applications to interact with solutions including ADFS, AzureAD and their associated protocols and API’s. With a large measure of MS Identity Manager to synchronise repositories with the applications which required much larger investment to refactor. With cloud all the rage, and stateless the way forward, architecting applications around micro services, queue solutions like RabbitMQ , and NoSQL databases, I was once more focused and training on technologies which were far more at home on the Linux platform, returning to some of my roots from 10 years earlier. Lots of fun was had with Kickstart and Samba as we made this platform just part of the ecosystem, going as far as implementing the OMI stack to deliver monitoring, management and declarative configuration trough PowerShell. With the development teams all adhering to agile practices, I spent many hours evangelising the benefits of DevOps, infrastructure as code, hybrid computing, service fabric and so many more enabling technologies and processes. Building from basic test automation, to release managed azure automation templates. What did I feel had broken? The largest hurdles to conquer turned out to be the technology group management focus and direction; this has been badly segmented, and as its stated in the fantastic book “The Phoenix Project”; everyone on the team MUST trust each other unequivocally. Additionally, the IT Ops groups whom ultimately had the most to gain were lease prepared or ready to embrace change, as its cost was proving to be too large, the cost of re-skilling. Combined with no clear road map, all the different technology teams working very hard to deliver their respective objectives, but with everyone following different routes to their destination, and most of whom blissfully unaware of processes and tooling available to assist in both streamlining and simplifying their efforts along their respective paths. “DevOps” was really just a tiny portion of “Dev”, and the “Ops” had emerged as a shadow IT. Team morale had been diminished, primarily due to some really bad mistakes, lots of secret management meetings, and an unsustainable unfocused, unplanned, workload of reinventing the wheel; driven by prioritisations which no longer made any logical sense; the time for management action was very clear; and yet sadly not happening; or at least in a productive manner. What Next… Well for the first time in 25 years, I am officially unemployed - But, just for 10 days. I begin the next chapter on the 18th of August; and I am super excited to start that story… In the interim if you are heading to “System Centre Europe” in Berlin, please do keep an eye for me, and pop over to say hello; and if you are really brave, please be sure to check out some the three sessions I will be presenting; I have a LOT of amazing content to share… Its I bright future ahead. Stay tuned and I will share it with you…",
    
  },
  {    
    "id" : "http-0-0-0-0-4000-azure-pack-adfs-token-1454578800",
    "site" : "damianflynn.com",
    "title": "Azure Pack: Get the Token?",      
    "url": "http://0.0.0.0:4000/azure-pack-adfs-token/",                    
    "categories" : ["Identity & Access Management","IT Pro/DevOps","Security"],
    "tags" : ["PowerShell","Azure","Active Directory","Azure AD","Federation","Windows Azure Pack"],
    "authors" : [],      
    "publishedDate" : "2016-02-04 09:40:00 +0000",
    "content" : "This post is primarily for personal reference. It is a clone of the code posted on the (Windows Azure Pack Security Troubleshooting)[https://technet.microsoft.com/en-us/library/dn554315.aspx#C_GetMgmtSvcToken] page. The code resolves a problem in the default function when requesting a token to work with the Azure Pack sites from PowerShell, in the scenario that the site is configured with a connection to ADFS. function Get-AdfsToken([string]$adfsAddress, [PSCredential]$credential) { $clientRealm = 'http://azureservices/AdminSite' $allowSelfSignCertificates = $true Add-Type -AssemblyName 'System.ServiceModel, Version=4.0.0.0, Culture=neutral, PublicKeyToken=b77a5c561934e089' Add-Type -AssemblyName 'System.IdentityModel, Version=4.0.0.0, Culture=neutral, PublicKeyToken=b77a5c561934e089' $identityProviderEndpoint = New-Object -TypeName System.ServiceModel.EndpointAddress -ArgumentList ($adfsAddress + '/adfs/services/trust/13/usernamemixed') $identityProviderBinding = New-Object -TypeName System.ServiceModel.WS2007HttpBinding -ArgumentList ([System.ServiceModel.SecurityMode]::TransportWithMessageCredential) $identityProviderBinding.Security.Message.EstablishSecurityContext = $false $identityProviderBinding.Security.Message.ClientCredentialType = 'UserName' $identityProviderBinding.Security.Transport.ClientCredentialType = 'None' $trustChannelFactory = New-Object -TypeName System.ServiceModel.Security.WSTrustChannelFactory -ArgumentList $identityProviderBinding, $identityProviderEndpoint $trustChannelFactory.TrustVersion = [System.ServiceModel.Security.TrustVersion]::WSTrust13 if ($allowSelfSignCertificates) { $certificateAuthentication = New-Object -TypeName System.ServiceModel.Security.X509ServiceCertificateAuthentication $certificateAuthentication.CertificateValidationMode = 'None' $trustChannelFactory.Credentials.ServiceCertificate.SslCertificateAuthentication = $certificateAuthentication } $ptr = [System.Runtime.InteropServices.Marshal]::SecureStringToCoTaskMemUnicode($credential.Password) $password = [System.Runtime.InteropServices.Marshal]::PtrToStringUni($ptr) [System.Runtime.InteropServices.Marshal]::ZeroFreeCoTaskMemUnicode($ptr) $trustChannelFactory.Credentials.SupportInteractive = $false $trustChannelFactory.Credentials.UserName.UserName = $credential.UserName $trustChannelFactory.Credentials.UserName.Password = $password #$credential.Password $rst = New-Object -TypeName System.IdentityModel.Protocols.WSTrust.RequestSecurityToken -ArgumentList ([System.IdentityModel.Protocols.WSTrust.RequestTypes]::Issue) $rst.AppliesTo = New-Object -TypeName System.IdentityModel.Protocols.WSTrust.EndpointReference -ArgumentList $clientRealm $rst.TokenType = 'urn:ietf:params:oauth:token-type:jwt' $rst.KeyType = [System.IdentityModel.Protocols.WSTrust.KeyTypes]::Bearer $rstr = New-Object -TypeName System.IdentityModel.Protocols.WSTrust.RequestSecurityTokenResponse $channel = $trustChannelFactory.CreateChannel() $token = $channel.Issue($rst, [ref] $rstr) $tokenString = ([System.IdentityModel.Tokens.GenericXmlSecurityToken]$token).TokenXml.InnerText; $result = [System.Text.Encoding]::UTF8.GetString([System.Convert]::FromBase64String($tokenString)) return $result } The use the function, we can define the variables, similarly to the following sample, and then leverage the function to get a valid token, as follows # Fill in values $adfsAddress = 'https://adfshost' $username = 'domain\\username' $password = 'password' $securePassword = ConvertTo-SecureString -String $password -AsPlainText -Force $credential = New-Object -TypeName System.Management.Automation.PSCredential -ArgumentList $username,$securePassword $token = Get-AdfsToken -adfsAddress $adfsAddress -credential $credential $token Note: All credits for this code are attributed to the original post and author.",
    
  },
  {    
    "id" : "http-0-0-0-0-4000-powershell-ad-guid-base64-1449740400",
    "site" : "damianflynn.com",
    "title": "AD: GUIDs in Base64",      
    "url": "http://0.0.0.0:4000/PowerShell-AD-GUID-Base64/",                    
    "categories" : ["Identity & Access Management","IT Pro/DevOps","Security"],
    "tags" : ["PowerShell","Azure","Active Directory","Azure AD","Federation","Windows Azure Pack"],
    "authors" : [],      
    "publishedDate" : "2015-12-10 09:40:00 +0000",
    "content" : "Just a short note this time. As I continue to work on multiple federation partners, and configuration endpoints, I continue to see a mix of formats being transmitted when passing GUID style values. These are appearing as either Base64, or raw GUID; so to assist I created a simple function which would allow me to see the GUIDs from my idP in both formats to assist with debugging. Get-ADUser -Filter * -SearchBase \"DC=DigiNerve,DC=NET\" | select UserPrincipalName, ObjectGUID, @{Label='ObjectGUIDBase64';Expression = {[System.Convert]::ToBase64String(($_.ObjectGUID).ToByteArray())}} This will be very useful as you expose Windows Azure Pack and other claim aware endpoints to federation partners.",
    
  },
  {    
    "id" : "http-0-0-0-0-4000-powershell-bing-wallpaper-1449009600",
    "site" : "damianflynn.com",
    "title": "Utility: Bing Wallpaper Downloader",      
    "url": "http://0.0.0.0:4000/PowerShell-Bing-Wallpaper/",                    
    "categories" : ["Identity & Access Management","IT Pro/DevOps","Security"],
    "tags" : ["PowerShell","Wallpaper"],
    "authors" : [],      
    "publishedDate" : "2015-12-01 22:40:00 +0000",
    "content" : "One of the more challenging issues with building a big proof of concept environment with lots of different ADFS instances, is to make each distinguishable. The simplest way to accomplish this is to provide a nice graphic, however my artistic abilities leave much to be desired. This lack of skill, requires a good Plan B: Grab the images which the nice people from Microsoft publish every day. workflow Download-BingWallpaper { $BingImageData=\"http://www.bing.com/HPImageArchive.aspx?format=js&amp;mbl=1&amp;idx=0&amp;n=1&amp;cc=us\" $ImageUrl = \"http://www.bing.com\" + ((Invoke-WebRequest -Uri $BingImageData | ConvertFrom-Json).images.url) $BingImagePayload = Invoke-WebRequest -Uri $ImageUrl Start-BitsTransfer -source $ImageUrl -Destination c:\\temp\\wallpaper.jpg } Now, to use this function we simply need to Download the graphic, and then take a look at the results. Download-BingWallpaper c:\\temp\\wallpaper.jpg Two points to note: I have hardcoded the download path and file name The Script is formatted as a workflow Why? this is a subset of a task which I will share in another post, to automatically download the graphic and update my ADFS servers theme.",
    
  },
  {    
    "id" : "http-0-0-0-0-4000-register-a-github-account-1447273800",
    "site" : "damianflynn.com",
    "title": "GIT: Register a GitHub Account",      
    "url": "http://0.0.0.0:4000/register-a-github-account/",                    
    "categories" : ["Developer","IT Pro/DevOps"],
    "tags" : ["Community","PowerShell","OpenSource","Git / GitHub"],
    "authors" : [],      
    "publishedDate" : "2015-11-11 20:30:00 +0000",
    "content" : "GitHub How to register for an account DEVOPS is everywhere and as hard as you might try to ignore it, sooner or later you are going to need to embrace at least some of the benfit’s which are offered. In this post we will introduce GIT, GITHUB, and register an account on this free service What is GIT? GIT, a version control solution is one of the most useful pieces of software you will ever embrace. As and IT Pro you will have been haunted by the plauge which is PowerShell, VBScript and many other ‘Programming’ or ‘Scripting’ solutions over the course of your creear; and if you are honest you will have had the unfortunate experience of getting the script finally working, only to add an extra tweak which totally broke all your work, and of course you no longer have a copy of that last good version. GIT is your new best buddy. When used as part of your daily workflow, those nightmares can be put to bed for ever. GIT is an open source project started by Linux creator Linus Torvalds; similar to other version control systems, GIT manages and stores revisions of projects, and can be used to manage any type of file, such as Word documents or Excel spreadsheet. Think of it as a filing system for every draft of a document. Some of Git’s predecessors, such as CVS and Subversion, have a central “repository” of all the files associated with a project, which required that when a user makes changes, those changes are made directly to the central repository. With distributed version control systems like Git, if you want to make a change to a project you copy the whole repository to your own system. You make your changes on your local copy, then you “check in” the changes to the central server. This enables offline use, and more granular change tracking since you don’t have to connect to the server every time you make a change. GIT distributed architecture implies that it has the built in ability to be installed on many systems, and can sync between them, while also offering you 100% control over any conflicting versions of files an scripts which you may have on different systems. We can actually go a step further, GIT is designed to be used by teams, so multiple people can work on the same project at the same time. This is an awesome piece of software, very lightweight, and if you are really interested, you should check out this free resource Pro Git 2.0 What is GitHub? You may have heard that GitHub is a code sharing and publishing service, or that it’s a social networking site for programmers. At the heart of GitHub is Git, GitHub is a Git repository hosting service, but it adds many of its own features. While Git is a command line tool, GitHub provides a Web-based graphical interface. It also provides access control and several collaboration features, such as a wikis and basic task management tools for every project. The flagship functionality of GitHub is “forking” – copying a repository from one user’s account to another. This enables you to take a project that you don’t have write access to and modify it under your own account. If you make changes you’d like to share, you can send a notification called a “pull request” to the original owner. That user can then, with a click of a button, merge the changes found in your repo with the original repo. These three features – fork, pull request and merge – are what make GitHub so powerful. When you submit a pull request, the project’s maintainer can see your profile, which includes all of your contributions on GitHub. If your patch is accepted, you get credit on the original site, and it shows up in your profile. It’s like a resume that helps the maintainer determine your reputation. The more people and projects on GitHub, the better idea picture a project maintainer can get of potential contributors. Patches can also be publicly discussed. In addition to the public facing open source repositories, GitHub also sells private repositories and on-premise instances of its software for enterprises. These solutions obviously can’t take full advantage of GitHub’s network effect, but they can take advantage of the collaboration features. That’s how GitHub makes money, but it’s not alone in this market. GitHub Account The sign up process for a GITHUB account is very straight forward. you simply need to point your browser to the GitHub.com site. Once on the site, locate the button for Sign up and click it. This will drop you on the page to Join GitHub, where we have 3 simple steps to complete. Setup a personal account: You will need to define a username for your account, along with a contact email address for update notifications; and of course a password for your new account. Choose your plan: Initially we will begin with the Free tier, which offeres No private repositories; once we get more comfortable with the service and using GIT, we can upgrade this plan as required. That’s it! Really! you now have a GITHUB account. You will be taken to the dashboard, from there we will need to navigate to the Personal Settings area, and complete your Email verification, failure to validate your mail address will prevent you from contributing to GitHub! Next Steps Now that you have a GitHub account, you will want to establish at least one repository and sync it with a local GIT repository you may already have or clone your new GitHub repository to your local system. To learn more about Git and GitHub, see: Pro Git 2.0",
    
  },
  {    
    "id" : "http-0-0-0-0-4000-updating-a-forked-repo-1446930120",
    "site" : "damianflynn.com",
    "title": "GIT: Updating a Forked Repository",      
    "url": "http://0.0.0.0:4000/updating-a-forked-repo/",                    
    "categories" : ["Developer","IT Pro/DevOps"],
    "tags" : ["Community","PowerShell","OpenSource","Git / GitHub"],
    "authors" : [],      
    "publishedDate" : "2015-11-07 21:02:00 +0000",
    "content" : "While recently working with a community project which is hosted using GIT, due to high frequency of activity in the main repository, I quickly found that I needed to re-sync my fork of the project with the main upstream copy. Additionally, this particular project happened to be structured using submodule’s enabling the project to be segregated into a number of independently managed repositories. In this post, I will cover first how to clone a local copy of your projects fork, including its submodule’s, fork and checkout a submodule including updating your references to use your fork of the submodule so you can manage it also as part of the project, then finally refresh your copy of the project from the upstream master repository to ensure your fork is updated to the current version. Forking The concept of forking is to create a linked copy of a repository for your own project, This parent and child relationship of repositories is more commonly refered to as the upstream (original or master) and the fork (our GIT copy). Our forked copy is where we will carry out all our work, checking in changes and creating tags and versions. To create the Fork, we will normally use a service like github.com where we have an account, We would normally locate the project we are interested in contributing to and using the User Interface, click on the Fork button, which will result in the interface updating, presenting a copy of the project in our account, with a link to the original project. If this project happens to be using submodule’s these will not be at this point forked to our repository, instead these will be referenced to their current repository. To pull a copy of the project locally from your GIT account including any potential submodule’s, as well as adding a reference to the upstream master we can use the following commands git clone https://github.com/&lt;myaccount&gt;/&lt;repositoryName&gt;.git --recursive cd &lt;repositoryName&gt; # add the upstream repo URL git remote add upstream https://github.com/&lt;upstream&gt;/&lt;repositoryName&gt;.git Forking Submodule’s In the event that the project does use submodule’s, and you happen to wish to customise the submodule you will then also want to fork a copy of one or more of the submodule to your GIT account, repeated depending on which and how many of the submodule’s you wish to edit. The procedure is exactly the same as we just executed with the initial main project. Once you have a fork of the submodule you will want to update your local copy of the submodule to point at your GIT fork of the project otherwise you will not be able to push changes back to your GIT account. The following commands will allow us change the references, first we can change to the submodule, and check the current upstream repository being used. cd &lt;repositoryName&gt;\\&lt;subModule&gt; git remote Now, remove the current upsteam, add our copy as Origin and the upstream master as Upstream git remote remove git remote add origin https://github.com/&lt;myAccount&gt;/&lt;repositoryName&gt;.git git remote add upstream https://github.com/&lt;upstream&gt;/&lt;repositoryName&gt;.git The benefits are that you can modify the fork to your needs and even open pull requests to the maintainer. If the maintainer updates his code you can then merge/update everything you want into your fork and keep your patches that are not approved in upstream. From time to time you then need to update the submodule in your projects because the upstream maintainer has added a feature or version you want to have. This cannot be done by a simple “git pull” because git submodules are only pointers to a specific commit revision in another repo and therefore have a detached HEAD. This is necessary because you always want to have a specific revision when cloning your main repo. The revision the submodule repo had when you added it. Not any future commits which you might not know of. So first of all you need to update your fork to get all upstream changes. This can be skipped if your submodule points directly to the 3rd-party repo ! git fetch upstream git merge upstream/master Now, you can update your copy of the repository, or if you have already applied updates to the repository, we are going to merge in these changes to our fork git add -A :/ git commit -a -m \"Commit message.\" git push origin master git push --tags At this point, our fork should be up-to-date with the original code. Updating remote Submodule’s In many cases we will also have submodules in our repository which may also need to be updated to the current revision or tag. If your repository has submodules then the following process will need to be repeated in each of the submodules you may have forked. cd /&lt;repositoryName&gt;/&lt;pathToSubmodule&gt; git checkout master git pull origin master git pull --tags # go to your desired tag or revision ID git checkout 2.1.1 cd /path/to/main/repo # now save the new submodule pointer git add /path/to/submodule/ git commit -m \"updated submodule to Tag 2.1.1\" git push",
    
  },
  {    
    "id" : "http-0-0-0-0-4000-stopping-sccm-endpoint-protection-1445504820",
    "site" : "damianflynn.com",
    "title": "SCCM: Stopping Endpoint Protection",      
    "url": "http://0.0.0.0:4000/stopping-sccm-endpoint-protection/",                    
    "categories" : ["IT Pro/DevOps","Supportability"],
    "tags" : ["PowerShell","System Center","Configuration Manager","Endpoint Protection"],
    "authors" : [],      
    "publishedDate" : "2015-10-22 09:07:00 +0000",
    "content" : "Scenario: A user is experiencing really poor workstation performance, and after some performance monitoring, we determine that the Endpoint Protection engine is trashing the Disk IO and CPU for no apparent reason. Familiar problem statement I guess. Bad Definations Normally, to address this; we would navigate the registry to locate the key which will inform us where on the system the definations happen to be stored. For reference the key in question is HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Microsoft Antimalware\\Signature Updates\\SignatureLocation After navigating to the defined folder on the disk, we will normally delete the files we find, and issue a reboot; after which windows will go fecth a fresh set of updates and we hopefully should be back in working order. Stopping Endpoint But, what do we do when this fails? And even after the normal replace definations process; we find the system is still unusable. so bad sometimes, that even running the previous process can take many times longer than we ever expected. We would just use Windows Services and Stop the service, taking the pressure off the system, so we can do a more intensive review of the issue. Of Course, that is no just so simple, the Windows Service which hosts the Malware Daemon is protected. What?? Simply put you do not get the options to starting and stopping the service unless you are SYSTEM! How to stop SCCM Endpoint Service You are going to need to go to sysinternal.com and grab a copy of the ever so important free PSEXEC utility. We will use this tool to open a session under the SYSTEM role, so that we can offer ourselves access to the manage the service Using PSEXEC we will start an interactive command session launching it using the SYSTEM account. psexec -s -i cmd.exe In the new Command window, we can issue the command whoami and we should be told that we are nt authority\\system Now, with our new persona, we can stop the service with the net command. net stop MsMpSvc Now, if this has worked, we should see in the Windows Services snap in, services.msc that the Malware Agent is now stopped.",
    
  },
  {    
    "id" : "http-0-0-0-0-4000-get-allocated-vlans-from-scvmm-1445466720",
    "site" : "damianflynn.com",
    "title": "SCVMM: Report the Currently Allocated VLANs",      
    "url": "http://0.0.0.0:4000/get-allocated-vlans-from-scvmm/",                    
    "categories" : ["IT Pro/DevOps","Monitoring & Management","Networking","Supportability","Virtual Machines"],
    "tags" : ["PowerShell","Virtual Machines / IaaS","Networking","Software Defined Networking","System Center","Virtual Machine Manager","Cloud","Windows Azure Pack"],
    "authors" : [],      
    "publishedDate" : "2015-10-21 22:32:00 +0000",
    "content" : "As we place existing Hyper-V Clusters under the management scope of Virtual Machine Manager, one of the first real tasks we need to undertake is the establishment of a good logical network implementation. Before we can get to deeply involved in that process, we first need to audit what we have in place and in use currently. This following function will recurse trough the nodes and VMs on the cluster, and provide back a unique list of VLANs which are currently occupied by the clusters resources. Function Get-ClusterVLANList { [CmdletBinding()] param ( [String] $Cluster ) Begin { #Create an arrary for the VLAN list $vlanid = @() Write-Output \"Cluster: $($ClusterName)\" foreach ($node in (Get-VMHostCluster -Name $ClusterName).nodes) { # Get a list of VMs hosted on the active Host Write-Output \"Hostname: $($node.ComputerName)\" foreach ($VM in (Get-SCVirtualMachine -VMHost $node.ComputerName)) { # Get a list of VLANs hosted on the active VM Write-Output \" VMname: $($vm.Name)\" $vlanid += (Get-SCVirtualMachine -Name $vm.name).VirtualNetworkAdapters.vlanid } } return ($vlandid | sort-object -unique) } } Just call this function with the name of the cluster, and let it do its work for you $VLANList = Get-ClusterVLANList -Cluster &lt;MyCluster&gt; The results will be placed in the variable $VLANList which we can use to help us on this journey.",
    
  },
  {    
    "id" : "http-0-0-0-0-4000-exchange-mail-delivery-latency-1444082820",
    "site" : "damianflynn.com",
    "title": "Exchange: Mail delivery latency KPI",      
    "url": "http://0.0.0.0:4000/exchange-mail-delivery-latency/",                    
    "categories" : ["Business Intelligence","IT Pro/DevOps","Monitoring & Management","Messaging Platforms"],
    "tags" : ["Powershell","Azure Auotmation","Service Management Automation","PowerBI","Dashboards","KPI","Exchange","SMTP"],
    "authors" : [],      
    "publishedDate" : "2015-10-05 22:07:00 +0000",
    "content" : "One of the new KPI’s which I have been asked recently to collect and report on is that of the average time to deliver email in our hybrid environment. Initially I considered this to be a horrible request as my first gut reaction was to write a workflow to connect with Exchange using the Exchange Web Services, send an email to a known external echo address, then monitor to the echo to return; reading the deliver header to determine the duration which has elapsed from the initial send, to the final recipe of the echo. I would need to repeat this process very frequently, storing the results in a database, which I could query to get back the average of all samples for the week, thus providing the value for my KPI. However, there are many ways to skin the cat, and as I truly begun to consider this objective, it became clear that if I just analysed the emails in my inbox for the same period of time, the headers of these mails would provide a very good source of data from which to build the same KPI results. Exchange 2010 and Beyond Now, once we realise that the mail environment is powered by Exchange 2010 or newer, things get even better. We have a fantastic feature which holds all the data we could ever be interested in, that of course is the ever so helpful Message Tracking Logs. In these logs, when a HUB server delivers a message (through the Store Driver) it generates a DELIVER event which contains the TimeStamp of when the message was delivered as well the MessageLatency; which is the difference between the original arrival time of the message and this timestamp. Let’s look at what these logs offer us, the following command will present all the properties of the first entry in the current logs. PS &gt; Get-TransportServer | Get-MessageTrackingLog | select * -First 1 RunspaceId : 4159009d-????-????-????-4edccd3e6c16 Timestamp : 06/09/2015 01:02:44 ClientIp : ClientHostname : Exchange2010 ServerIp : ServerHostname : Exchange2010 SourceContext : 08D2A38562DD7452;2015-09-06T00:02:44.134Z;0 ConnectorId : Source : STOREDRIVER EventId : DELIVER InternalMessageId : 3175 MessageId : &lt;6a85be58-????-????-????-913b4613e40e@CH???HUB13.???.gbl&gt; Recipients : {?????@?????.net} RecipientStatus : {} TotalBytes : 22903 RecipientCount : 1 RelatedRecipientAddress : Reference : MessageSubject : Healthy: \"My first test\" in app \"my blog site\" Sender : ??reply@??????.com ReturnPath : ??reply@??????.com MessageInfo : 2015-09-06T00:02:43.649Z;SRV=EXCHANGE2010.??????.net:TOTAL=0 MessageLatency : 00:00:00.5320000 MessageLatencyType : EndToEnd EventData : {[MailboxDatabaseName, ??????], [DatabaseHealth, -1]} WARNING: There are more results available than are currently displayed. To view them, increase the value of the ResultSize parameter. Now, Let’s check the mailbox, and find this matching message, so we can take a closer look at the mail header; why? so that we can verify the data in the log is trustable for our PKI Received: from smtp.???.com (207.46.200.23) by smtp.??????.net (???.??.??.??) with Microsoft SMTP Server (TLS) id 14.3.210.2; Sun, 6 Sep 2015 01:02:43 +0100 DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=microsoft.com; s=s1024; t=1441497762; h=From:Subject:Date:Message-ID:To:MIME-Version :Content-Type; bh=CAm79b/B//rjl74f3kAUnQ8udQgOsyg47G1c47VuwhI=; b=h6RA8Ye c/0IOi+YcOouJmgruX3mhBal3g5n+Tca+7E5v6EtrsPfH44412/9buynbD1FxLYdIBQLhgQ6I l0P/2p+eWTs8q0qjuqaTeWhF0Sh77r4T6xvUIyht7C6Tfa1D4PzeuU3wyUPc04NkF/4+pOd5j ycX4AYYkTbsqdPrCV0=; MIME-Version: 1.0 From: App Insights Alerts &lt;??reply@??????.com&gt; To: &lt;????@??????.com&gt; Date: Sun, 6 Sep 2015 00:02:42 +0000 Subject: Healthy: \"My first test\" in app \"my blog site\" Content-Type: multipart/alternative; boundary=\"--boundary_10198_64254f5b-bd41-47b6-a91a-85e6ac495da8\" Message-ID: &lt;6a85be58-d164-403a-9eae-913b4613e40e@CH1GMEHUB13.gme.gbl&gt; Return-Path: ??reply@??????.com X-DKIM-Signer: DkimX (v1.11.111) X-MS-Exchange-Organization-AuthSource: Exchange2010.??????.net X-MS-Exchange-Organization-AuthAs: Anonymous Looking at Line 3 we can see the send time stamp Sun, 6 Sep 2015 01:02:43 +0100, and when we check lower in the header we can see the receive time stamp of Sun, 6 Sep 2015 00:02:42 +0000. The difference between these is the duration the message took to be delivered, which in this case is 1 second. Now, refer to the MessageLatency from the Transport log, and it matches our result, but just a lot more accurately! in this case 00:00:00.5320000 Fantastic, so now this log is the database we need to support. Analyse a days’ worth of logs So, now we can get a list of all the hub servers with Get-TransportServer, then on each of these servers we can request the tracking log for the past 24 hours, which are Deliver type, finally we can return the MessageLatency for each message Get-TransportServer | Get-MessageTrackingLog -ResultSize Unlimited -Start (Get-Date).AddHours(-24) -EventID DELIVER | select MessageLatency Simply awesome, we now have a really great set of raw data to work from, and have the one value we truly need to understand. Calculate the Weeks KPI Extrapolating all this, we just need to expand the collection window to 7 days, and of course create an average from all the recorded Deliver records, using our favourite Message Latency property. PS &gt; Get-TransportServer | Get-MessageTrackingLog -ResultSize Unlimited -Start (Get-Date).AddHours(-168) -EventID DELIVER | Select @{ Label=\"LatencyMilliseconds\"; Expression={ $($_.MessageLatency).TotalMilliseconds } } | Measure-Object -Property LatencyMilliseconds -Average Count : 634 Average : 3383.67350157729 Sum : Maximum : Minimum : Property : LatencyMilliseconds And there we have it, all the data we could ever require for our KPI in a very nice line of PowerShell, gosh we could just show off and use a fancy one liner to report the average mail deliver over the last week in millseconds, with a very simple tweak: PS &gt; (Get-TransportServer | Get-MessageTrackingLog -ResultSize Unlimited -Start (Get-Date).AddHours(-168) -EventID DELIVER | Select @{ Label=\"LatencyMilliseconds\"; Expression={ $($_.MessageLatency).TotalMilliseconds } } | Measure-Object -Property LatencyMilliseconds -Average).Average 3383.67350157729 Data Collection using a Workflow But, to be really of any value, we will leverage the Service Manager Automation engine to schedule the data collection, and provide the data at the same predictable time each week. If you look at the code, you will see that in lines 100-124 are using a very verbose method of collecting the same data we completed in just one line earlier. After much initial issues I found that the MessageLatency data returned from the remote shell needed to be cast to [Timespan]. The results of calling this function will hand back a simple JSON object, for example { \"Latency\": 7994.0027454679948, \"Samples\": 41498, \"Status\": \"Success\", \"Feedback\": \"Located [41498] End to End Mail Messages, taking an average [7994.00274546799] milliseconds to route \\n\\r\" } Publishing the KPI Now, all we have remaining to do, is publish the KPI, this is the workflow I will be using to accomplish this magical reality All this does is create a simple helper function for adding or updating a record in our SharePoint list. Then we proceed to call our workflow to gather and calculate the latency information; this for me takes about 90 minutes to process 3 hubs for 7 days worth of logs… Once complete we then call the helper function, passing in the name of the KPI we are about to publish, along with the value of the KPI. Job Done!",
    
  },
  {    
    "id" : "http-0-0-0-0-4000-signiant-adfs3-federation-1443551100",
    "site" : "damianflynn.com",
    "title": "Federation: Signiant Media Shuttle",      
    "url": "http://0.0.0.0:4000/signiant-adfs3-federation/",                    
    "categories" : ["Identity & Access Management","IT Pro/DevOps","Security"],
    "tags" : ["Powershell","Active Directory","Federation","Windows Server 2016","Windows Server 2012"],
    "authors" : [],      
    "publishedDate" : "2015-09-29 18:25:00 +0000",
    "content" : "In this post we will look at how we can configure an ADFS 2012 Server with a new Relying Trust, and add the required claims for that trust, all using powershell. The sample is based on ADFS 2012 R2 using the native PowerShell module, and is been configured to enable a federation with a company called Signiant, and thier MediaShuttle service. To help explain what is happening here, I am going to include details from the documentation offered by Signant, related to the sections which are relevant only. As with any service to federate we have 2 sides to the equation: A. The Service Provider we are going to build the trust with to use our Identities, the Identity Provider B. Our Federation Service, offering the correct formated claims to the Service Consumer, the Relying Party Signiant Portal configuration We will being with configuring the Serice Provider to communicate with out Identity Provider (Federation Service) NOTE: The following is an extract from the Signiant documentation Configure the Identity Provider Metadata URL in Media Shuttle Launch the Media Shuttle Administration Console for your portal and go to the Security page. Enable the “Use external SAML 2.0 provider to manage member logins” option in the “Login is required” section. Type the metadata URL of your AD FS server in the “Identity Provider Metadata URL” field (eg. https:///FederationMetadata/20 07-06/FederationMetadata.xml). Copy the Media Shuttle Service Provider Metadata URL that is provided (eg. https://sample.mediashuttle.com/saml2/metadata/sp). This is required for the AD FS Relying Party Trust configuration step. Click Save. ADFS configuration Now, we will shift focus, and this time start on the work needed to configure our Identity Provider to offer the correct details to the provider (relying party). This can be quite tricky, and if you have never threaded this water before it can take some time. To help simplify this, lets first look at what the Signant documentation suggests we should do, then once we have attempted to do all this manually, I will take all that detail and add it to a PowerShell function so we can easily implement this any time we need in the future, or for other environments, for example Proof of Concepts. Manual Exercise NOTE: The following is an extract from the Signiant documentation Launch the AD FS 2.x Management Console. Go to AD FS 2.x &gt; Trust Relationships &gt; Relying Party Trusts. Select ‘Add Relying Party Trust’ from the Action menu to launch the Add Relying Party Trust Wizard. Click Start. Select the option to ‘Import data about the relying party published online’. Input the Media Shuttle Service Provider Metadata URL from Stage 1, and click Next. Enter your Media Shuttle portal name as the display name. Click Next. Select the option to ‘Permit all users…’. Click Next. Click Next to complete the wizard Enable the checkbox to ‘Open the Edit Claim Rules dialog’. Click Close. On the ‘Edit Claim Rules’ dialog On the Issuance Transform Rules tab, click the ‘Add Rule’ button and select ‘Send Claims using a Custom Rule’. Click Next. In the Claim Rule Name field, type “ Custom Claim\". In the Custom Rule field, copy and paste the following custom rule. c1:[Type == \"http://schemas.microsoft.com/ws/2008/06/identity/claims/windowsaccountname\"] &amp;&amp; c2:[Type == \"http://schemas.microsoft.com/ws/2008/06/identity/claims/authenticationinstant\"] =&gt; add( store = \"_OpaqueIdStore\", types = (\"https://sample.mediashuttle.com/internal/sessionid\"), query = \"{0};{1};{2};{3};{4}\", param = \"useEntropy\", param = c1.Value, param = c1.OriginalIssuer, param = \"\", param = c2.Value); Replace https://sample.mediashuttle.com with the URL of your Media Shuttle portal. Click Finish. * Click the ‘Add Rule’ button and select ‘Transform an Incoming Claim’. Click Next. In the Claim Rule Name field, type “ Claim Transform\". In the Incoming Claim Type field, type “https://sample.mediashuttle.com /internal/sessionid”, replacing https://sample.mediashuttle.com with the URL of your Media Shuttle portal. Note: This must exactly match the “types” parameter from the Custom Rule you entered in Step 11, above. In the Outgoing Claim Type field, select the “Name ID” option. In the Outgoing Name ID Format field, select the “Transient Identifier” option. Select the Pass Through All Claim Values option, then Click Finish. * Click the ‘Add Rule’ button and select ‘Send LDAP Attributes as Claims’. Click Next. In the Claim Rule Name field, type “ LDAP Attributes\". In the Attribute Store field, select the Active Directory option. In the LDAP Mapping fields, specify the following mappings: LDAP Attribute Outgoing Claim Type E-Mail-Addresses E-Mail Address User-Principal-Name UPN SAM-Account-Name Common Name Display-Name Name Given-Name Given Name Surname Surname Token Groups - Unqualified Names Role Click Finish. * Click Apply, then Click OK to close the Edit Claim Rules dialog. Select your portal in the Relying Party Trusts list, then select ‘Properties’ from the Action menu. Go to the Advanced tab. In the Secure Hash Algorithm field, select the “SHA-1” option. Click Apply. Click OK. Powershell Ok, that is pretty complicated stuff, and if you have seen the document from signant you will notice that i changed the formatting from a list of 23 points, to 4 main points with a lot of incuded steps, just to help see what is going on in the process. I recommend that if you have not implemented a federation before, that you try the manual steps, it will work if you get all the details correct. When you are done, then go back to your ADFS server, find that new Relying Trust and delete it!. Then using my powershell to try do the exact same thing, as we done manually. In the code, you will see that we follow all the main steps, exactly the same, but this time, we use the magic of Powershell to make it fast, and error free as possible. This is all I now need to issue, which will create federation really simply for us. Add-SigniantFederationRelyingTrust -Name \"My Signant Send Portal\" -Group \"!grp Signiant Send Portal Access\" -MetadataURL = \"https://myportal-send.mediashuttle.com/saml2/metadata/sp\" Next Steps The code is functional for the use with Signant, but more importantly this should proove to be quite useful to template out other federations which you may need to establish; with little pain. Enjoy!",
    
  },
  {    
    "id" : "http-0-0-0-0-4000-bitlocker-key-automation-1443108300",
    "site" : "damianflynn.com",
    "title": "BitLocker: Automation of Recovery Keys",      
    "url": "http://0.0.0.0:4000/bitlocker-key-automation/",                    
    "categories" : ["Identity & Access Management","IT Pro/DevOps","Security","Supportability"],
    "tags" : ["Powershell","Active Directory","Bitlocker","Delegation","RBAC","Azure Auotmation","Orchestrator","Service Management Automation","Service Manager","ITIL","Self Service","Windows Server 2012"],
    "authors" : [],      
    "publishedDate" : "2015-09-24 15:25:00 +0000",
    "content" : "Bitlocker is a protected service in Active Directory, the keys to the encryption when stored in Active Driectory are by default hosted in an Attribute which is not accessiable to users, and therefore the option of self service recovery of your keys is not possible. To assist in this challange we could proceed to delegate a group with global access to these keys but that itself is also a security issue. Therefore we need a more controlled approach to solving this riddle. Using tools like PowerShell we can add functions for auditing and access control which would allow us to delegate the end user access to keys, but ensure that the keys requested are keys for computers which the user is responsible for. We can make this determination based on information in our Configuration Database, and even form another AD attribute - Primary User. In this post we will look at the process around how to use powershell with the support of the great Service Management Automation engine to return the keys in a controled and secure manner. Later we will look at using service manager to offer this as an self service function for normal users, and present the results in a nice clean table. Delegate Access To BitLocker In order to delegate access to BitLocker Recovery Information objects in Active Directory to users that are not a member of the Domain Administrators group, we have to offer Full Control or the computer objects. We can implement this using either the GUI or PowerShell. But the person executing the following procedure must be a member of the Domain Administrators group. Active Directory Users and Computers UX Use the following procedure to enable access to BitLocker Recovery Information on the Domain level to a group named “!Delegation grp BitLocker Admins” in Active Directory: Launch Active Directory Users and Computers, and then Right Click the domain name in the left domain navigation tree pane, from the context menu select Delegate Control… The Delegation of Control Wizard, will be offered, click Next On the Users or Groups page, add the group we will delegate the permission to (ie. “!Delegation grp BitLocker Admins”) to the list and click Next On the Tasks to Delegate page, select Create a custom task to delegate and click Next Now, on the Active Directory Object Type page, select Only the following objects, once complete click Next: msFVE-RecoveryInformation objects On the Permissions page, select Full Control under Permissions and click Next Finally, on the Summary page we can click Finish Now members of the “!Delegation grp BitLocker Admins” group can read BitLocker Recovery Information in Active Directory. Powershell Workflow Now, we will use the following PowerShell workflow to accept the short name of the computer in the domain we wish to return the Bitlocker information for. The heart of this workflow is the following command from the Active Directory PowerShell module. Its task is to return the protected property msfve-recoverypassword, and to help deliver on this, we are providing credentials to the command. The credentials in question is a normal domain user, but this account is also a member of our newly created delegation group “!Delegation grp BitLocker Admins” as this provides the rights necessary to read this property. $msfveRecoveryPassword = get-ADObject -ldapfilter \"(msFVE-Recoverypassword=*)\" -Searchbase $compObj.distinguishedname -properties canonicalname,msfve-recoverypassword -Credential $MSOPCreds Now, with this understanding, we can see see how the rest of the flow works. The results of the process I am then returning in JSON format, as this allows me to safely consume the data either from a command line execution, a SharePoint triggered flow, or even a self service system like service manager. The results of calling this function will hand back a simple JSON object, for example { \"distinguishedname\": \"CN=TESTPC01,OU=Computers,OU=!Offices,DC=diginerve,DC=org\", \"Keys\": [ { \"Date\": \"2011-10-07T19:00:27-00:00\", \"PasswordID\": \"1296273E-9578-4270-83E1-8EE0ECD9F492\", \"RecoveryPassword\": \"359612-188265-349613-035805-620840-183678-559251-433235\" }, { \"Date\": \"2011-01-31T17:55:51-00:00\", \"PasswordID\": \"83F48728-D5C5-437B-AF74-C0E961F1FE8A\", \"RecoveryPassword\": \"103774-233376-031141-392139-277002-103873-031086-150953\" }, { \"Date\": \"2010-02-04T14:15:24-00:00\", \"PasswordID\": \"17025B19-8DB0-4084-8E7B-EDD750160E67\", \"RecoveryPassword\": \"321596-696630-367400-169224-536580-039468-027170-286132\" } ], \"Name\": \"TESTPC01\" } Consuming the result is also very simple, and will be a topic for another post as we explore how we can leverage this work.",
    
  },
  {    
    "id" : "http-0-0-0-0-4000-windows-2003-support-is-over-1436916300",
    "site" : "damianflynn.com",
    "title": "Windows 2003: End Of Support",      
    "url": "http://0.0.0.0:4000/windows-2003-support-is-over/",                    
    "categories" : ["Announcements","IT Pro/DevOps","Security","Supportability"],
    "tags" : ["Windows Server 2003"],
    "authors" : [],      
    "publishedDate" : "2015-07-14 23:25:00 +0000",
    "content" : "Still on Windows Server 2003? Be aware that as of July 14th there is no support anymore for Windows Server 2003. Of course if you have deep pockets you can sell your soul to Microsoft in order to still receive patches/fixes. http://blogs.technet.com/b/server-cloud/archive/2015/07/14/reminder-support-for-windows-server-2003-ends-today.aspx",
    
  },
  {    
    "id" : "http-0-0-0-0-4000-adfs-publishing-via-azure-traffic-manager-1425489736",
    "site" : "damianflynn.com",
    "title": "ADFS Publishing via Azure Traffic Manager",      
    "url": "http://0.0.0.0:4000/adfs-publishing-via-azure-traffic-manager/",                    
    "categories" : ["Announcements","Business Intelligence","Cloud Strategy","Data Warehouse","Database","Developer","Events","Identity & Access Management","Internet Of Things","IT Pro/DevOps","Monitoring & Management","Messaging Platforms","Mobile","Networking","Smart Home/Buildings","Security","Storage, Backup & Recovery","Supportability","Updatees","Virtual Machines","Web"],
    "tags" : ["Powershell","Azure","Federation","Active Directory","Resource Manager","Web Sites","Traffic Manager","Networking","Firewalls / Network Security Groups","Load Ballancers","DNS","Replication","Windows Server 2012"],
    "authors" : ["Damian Flynn"],      
    "publishedDate" : "2015-03-04 17:22:16 +0000",
    "content" : "Start with a connection from your workstation to the Traffic Manager, this may require that you use a settings file if you are not configured for federated identity, which assuming this is your first Federation Service to publish publically, might well be the case. Azure Using a Microsoft Accounts If you are using a Microsoft Account for your authentication, then we will need to get our Azure Publishing Settings file, and place it on your workstation. To locate this file, you can use the following command which will open your browser. Once the browser is open, you will need to authenticate with your account details, after which you will be provided with a link to download the setting file PS&gt; &lt;strong&gt;Get-AzurePublishSettingsFile&lt;/strong&gt; Continue to download the Settings XML file, to your machine, then back on the PowerShell prompt so that we can register this account information on our workstation PS&gt; &lt;strong&gt;dir .\\Downloads&lt;/strong&gt; Directory: C:\\Users\\Damain\\Downloads Mode LastWriteTime Length Name ---- ------------- ------ ---- -a--- 2/17/2015 8:55 AM 7443 2-17-2015-credentials.publishsettings PS&gt; &lt;strong&gt;Import-AzurePublishSettingsFile '.\\ 2-17-2015-credentials.publishsettings' –verbose&lt;/strong&gt; VERBOSE: Setting: Microsoft.Azure.Common.Extensions.Models.AzureSubscription as the default and current subscription. To view other subscriptions use Get-AzureSubscription Id : d1d475c9-5a29-4a54-a36c-aa257705612f Name : My Sandbox Environment : AzureCloud Account : Cxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx4 Properties : {[SupportedModes, AzureServiceManagement], [Default, True]} Id : 3719257d-381a-45ca-a7aa-ea741ab966e1 Name : My Production Environment : AzureCloud Account : 3xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx4 Properties : {[SupportedModes, AzureServiceManagement]} Assuming there are no issues with the XML file, we should see any subscriptions which are associated with the account presented back to us, after the file has being imported, as you can see in the above example. Next, let’s check what accounts were imported as part of this process PS&gt; &lt;strong&gt;Get-AzureAccount&lt;/strong&gt; Id Type Subscriptions Tenants -- ---- ------------- ------- Cxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx4 Certificate d1d475c9-xxxx-xxxx-xxxx-xxxxxxxx612f 3xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx4 Certificate 3719257d-xxxx-xxxx-xxxx-xxxxxxxx66e1 Azure Traffic Manager Setting up the Azure Traffic Manager is quite trivial, all we need to know is the FQDN which we will publish the service as, note that this is going to terminate with the mandatory trafficmanager.net suffix, and we will alias this with our corporate DNS later. Geo-Awareness As azure is distributed, we have the ability to take into account geo-balancing with our implementation, educating Azure of where or services are hosted, in relation to the Azure points of presence over the globe. To determine the current locations of Azure we can use the following PowerShell commands PS&gt; &lt;strong&gt;get-azurelocation | select Name&lt;/strong&gt; Name ---- West Europe North Europe East US 2 Central US South Central US West US North Central US East US Southeast Asia East Asia Japan West Japan East Brazil South Selecting the Subscription to work with We will begin by selecting the Azure Subscription we will be adding this service to; this is of course redundant if you only have a single subscription, as it will be selected by default; but when you have multiple subscriptions, none of these will be selected automatically. To select your subscription which you are going to work with, we can start with the simple PowerShell command PS&gt; &lt;strong&gt;Select-AzureSubscription -SubscriptionName \"My Production\" &lt;/strong&gt; Add a new Traffic Manager Service We will continue to manage the Traffic Manager from PowerShell, as the current Web UI will not permit us to manage services when we wish to use external endpoints; the portal will offer the ability to monitor the services however. While creating a new Traffic Manager Service, we begin with a Profile which we will name with a friendly label to help identify the profile, in addition we will then define what the service will be published as (the domain name), the Load Balancing Method to use, a TTL for the service, and how to validate the keep alive on the endpoints of the service. PS&gt; &lt;strong&gt;$profile = New-AzureTrafficManagerProfile -Name \"Production ADFS\" -DomainName \"mysts.trafficmanager.net\" -LoadBalancingMethod \"Performance\" -Ttl 30 -MonitorProtocol \"Http\" -MonitorPort 80 -MonitorRelativePath \"/adfs/probe\" &lt;/strong&gt; In the example above the following options have being selected to support ADFS as a service Name: Production ADFS Domain Name: mysts.trafficmanager.net Load Balancing Method: Performance TTL: 30 seconds * Monitoring * Protocol: HTTP * TCP Port: 80 * Relative Path: /adfs/probe _This path is published only from WAP Service on HTTP, and requires at least the August 2014 Windows 2012 R2 roll up to be deployed. _ We can check the new profile has been added, using the following command PS&gt; &lt;strong&gt;Get-AzureTrafficManagerProfile&lt;/strong&gt; Name DomainName Status ---- ---------- ------ Production ADFS mysts.trafficmanager.net Enabled To get specific details, we can just look at the values which have been returned to our handling variable, which was called $profile PS&gt; &lt;strong&gt;$profile&lt;/strong&gt; TimeToLiveInSeconds : 300 MonitorRelativePath : / MonitorPort : 80 MonitorProtocol : Http LoadBalancingMethod : Performance Endpoints : {} MonitorStatus : Inactive Name : Production ADFS DomainName : mysts.trafficmanager.net Status : Enabled Adding an endpoint to the new Service With the service now registered, we can begin with the process of adding the endpoints of the services we are about to balance. As you add the endpoints, you will notice that you are not adding these with specific protocols, or ports; this is not currently configurable in the current version of Traffic Manager. Adding external endpoints is possible only from PowerShell. The following command will allow us to add an endpoint to the profile, set its status as enabled, define the domain name of the service which we are adding, for example ‘adfs-node1.diginerve.net’, indicating the location of where we are hosting this service relative to the Azure locations. PS&gt; &lt;strong&gt;Add-AzureTrafficManagerEndpoint -TrafficManagerProfile $Profile -Status \"Enabled\" -Type \"Any\" -DomainName \"adfs-node1.diginerve.net\" -Location \"East US\"&lt;/strong&gt; TimeToLiveInSeconds : 300 MonitorRelativePath : /adfs/probe MonitorPort : 80 MonitorProtocol : Http LoadBalancingMethod : Performance Endpoints : {adfs-node1.diginerve.net} MonitorStatus : Inactive Name : Production ADFS DomainName : mysts.trafficmanager.net Status : Enabled It is important to note that this change is not actually deployed to Azure Traffic Manager yet, but instead staged on your PowerShell session. This allows us to add multiple endpoints, and check the configuration is correct, by inspecting the variable’s values, for example the following allows us to look closer at the Endpoints. PS&gt; &lt;strong&gt;$Profile.Endpoints&lt;/strong&gt; DomainName : adfs-node1.diginerve.net Location : East US Type : Any Status : Enabled MonitorStatus : Online Weight : 1 MinChildEndpoints : Publish the new Profile Settings When we are ready to proceed with the publishing of the new settings, all we need to do is pass this details to the Set Azure Traffic Manager Profile command, as illustrated below PS&gt; &lt;strong&gt;$Profile | Set-AzureTrafficManagerProfile -verbose&lt;/strong&gt; TimeToLiveInSeconds : 300 MonitorRelativePath : /adfs/probe MonitorPort : 80 MonitorProtocol : Http LoadBalancingMethod : Performance Endpoints : {adfs-node1.diginerve.net} MonitorStatus : CheckingEndpoints Name : Production ADFS DomainName : mysts.trafficmanager.net Status : Enabled Query Azure Traffic Manager for its Current Configuration Finally, we will loop back to Azure, and ask our PowerShell session to go back to Azure and query the current state of the traffic manager configuration. PS&gt; &lt;strong&gt;$Profile = Get-AzureTrafficManagerProfile -Name “Production ADFS” &lt;/strong&gt; And as before we can now inspect the values in the variable as returned from Azures Traffic Manager PS&gt; &lt;strong&gt;$Profile&lt;/strong&gt; TimeToLiveInSeconds : 300 MonitorRelativePath : /adfs/probe MonitorPort : 80 MonitorProtocol : Http LoadBalancingMethod : Performance Endpoints : {adfs-node1.diginerve.net} MonitorStatus : Online Name : Production ADFS DomainName : mysts.trafficmanager.net Status : Enabled Now, that was pretty easy right!",
    
  },
  {    
    "id" : "http-0-0-0-0-4000-techdays-online-uk-2015-2-1422919870",
    "site" : "damianflynn.com",
    "title": "TechDays Online UK 2015",      
    "url": "http://0.0.0.0:4000/techdays-online-uk-2015-2/",                    
    "categories" : ["Announcements","Business Intelligence","Cloud Strategy","Data Warehouse","Database","Developer","Events","Identity & Access Management","Internet of Things","IT Pro/DevOps","Monitoring & Management","Messaging Platforms","Mobile","Networking","Smart Home/Buildings","Security","Storage, Backup & Recovery","Supportability","Updates","Virtual Machines","Web"],
    "tags" : ["MVP","Webinar","Live Event","Community","Hyper-V","Orchestrator","Service Management Automation","Operations Manager","Operations Management Suite","Service Manager","Virtual Machine Manager","ITIL","Self Service","Windows Azure Pack","Windows Server 2012"],
    "authors" : ["Damian Flynn"],      
    "publishedDate" : "2015-02-02 23:31:10 +0000",
    "content" : "Tuesday February 3rd 2015, sees the start of this year’s FREE Online (or in person) Technical event hosted by the Microsoft UK team. This is my 3rd time working on the event, and this year for the first time I am dumping the Skype connection and joining the team Live in Reading to present on the Day 2 - The Journey to the Cloud. TechDays is an amazing event, and covers both Development and Operations tracks, with some really great information on how to get your procedures around the current generation of MS Technologies, with some very well-known names getting involved, Including Microsoft’s very own Jeffrey Snover and Scott Hanselman, and if you pay any attention to the Microsoft Rumour machines, then Mary Jo Foley is a name you will instantly recognise. To get involved, all you need is a browser and some time; then OPEN THIS LINK For more information on Day 1 and Day 3, please check this link for the latest details **Time** **Session** **Speaker** **Overview** **09:30-09:40** Overview of TechDays Online 2015 Day 2 Andrew Fryer, Microsoft UK An overview of the sessions for Day 1 of TechDays Online 2015 and how to make the most of your participation using the online chat tool to interact with our experts and where to find more information on the topics of particular interest to you. **09:40-10:15** What's new Windows Server /Hyper –V - a technical preview Gordon McKenna, Microsoft Most Valued Professional (MVP) Coming Soon! **10:30-11:05** How to find out what's happening in your datacentre with Azure Insights Sam Erskine, Microsoft Most Valued Professional (MVP) How do you view and execute IT Service Management (ITSM) for your datacentre and cloud services. What if you are not yet in the cloud and just have a traditional data centre? This session provides some of the many answers to these questions by introducing you to Microsoft Azure Operational Insights (OpInsights). You will learn how you can start benefiting from this flagship Microsoft Service with virtually no change to your datacentre. You will get a practical scenario based Introduction to OpInsights, learn about the three setup/connection options available. You explore the ITSM facets known as Intelligent Packs on multiple device interfaces. Finally you get an introduction to the multi-tenancy features. This session is for IT Pros , Managers and anyone with a vested interest in continual Organisational IT Service improvement. **11:20-11:55 ** Host your own cloud with the Windows Azure Pack Damien Flynn, Microsoft Most Valued Professional (MVP) Not using System Center? Unsure about Azure? Curious about Cloud? In this session we will demonstrate the Windows Azure Pack and how to go from zero to cloud in little time, initially delivering self-service databases, websites, and finally infrastructure in a consistent manner, service orientated manner. **12:10-12:45 ** Taking scripting to the next level with Service Management / Azure Automation Jonathan Noble, Microsoft Most Valued Professional (MVP) By now you should be sold on the advantages of systems automation, and hopefully you're using PowerShell to help you with that. Now you can use your PowerShell experience to create workflow-based runbooks to add service automation to your private or public clouds using SMA or Azure Automation. DevOps-simplify your infrastructure to save money, improve reliability and get some of your life back! **13:30-14:05 ** A new home for your old applications Susan Smith, Microsoft UK This session will focus on Containerisation using Docker on Azure. The concept of Containerisation will be introduced and compared with other Virtualisation Technologies, followed by an overview of Docker. The Technical Demos will introduce elementary Azure tasks, such as creating a Docker Host on Azure. Then a step-by-step Technical Demo will illustrate how you can re-invent your Legacy applications through migration to Docker on Azure, by Deconstructing and then Containerising your Legacy App. **14:20-14:55 ** 20% + of Azure runs on Linux - why is this important and how to do it well? Boris Devouge, Microsoft UK With Microsoft Azure providing Infrastructure as a Service (IaaS) and strong of recent announcements showing that 20% of workloads in Azure run on Linux, this session will explore the best practices and different options to run and/or migrate Linux based workloads in the Cloud. We will also review the large subset of Open Source Software (OSS) and technologies available on Azure in area such as Test/Dev, CICD, Big Data (Hadoop) and containerisation (Docker). **15:10-15:45 ** DevOps in Microsoft Azure with Chef and Puppet for heterogeneous cloud environments Tarun Arora, Microsoft Most Valued Professional (MVP)   **16:00-16:35 ** Make Azure your DMZ Simon Skinner, Microsoft Most Valued Professional (MVP) The advantages of using Azure today are more than beneficial to any organization, however true not all understand the baby sips we can take to start this journey. We are in the mature era of the web, a rapidly growing struggle to get representation or even full blown commerce site in full operation. Many companies what to have their site/s integrated into their internal system but without impacting on their networks or bandwidth, both have a cost. The solution could be simpler than you think, Azure! Make Azure 'your' DMZ. There is more resilience and bandwidth than most DMZs and your company stays in control. You choose the IP Range and can even control the traffic through the firewall, join my session and learn just how easy this can be. **16:50-17:25 ** Microsoft Corporate Keynote and Interview - Jeffrey Snover Jeffrey Snover, Distinguished Engineer at Microsoft This session will focus on the open source interoperability that Powershell provides for other platforms and also provide an update on the Open Management Infrastructure (OMI) initiative that Microsoft has introduced with the Open Group. **17:25-17:35 ** Wrap-up of TechDays Online Day 2 Andrew Fryer, Microsoft UK  ",
    
  },
  {    
    "id" : "http-0-0-0-0-4000-cisco-champion-2015-1421764385",
    "site" : "damianflynn.com",
    "title": "2015 Cisco Champion!",      
    "url": "http://0.0.0.0:4000/cisco-champion-2015/",                    
    "categories" : ["Announcements"],
    "tags" : ["Achivements","Cisco Champion","Community"],
    "authors" : ["Damian Flynn"],      
    "publishedDate" : "2015-01-20 14:33:05 +0000",
    "content" : "Well now I am totally lost for words! Over the last few years I have focused primarily on the fabric which binds the magic and power of our clouds and services hosted within the data centre, with some emphasis networking technologies and software defined networks. Of course I have singled out some specific brands to which I have personal experience with, but never did I expect to be getting an awarded recognition for the work I have done and/or shared on this platform. But today that is exactly what has just happened! Congratulations: Welcome to the Cisco Champions Program 2015! _Because of your excellent contributions to the IT community, you have been chosen out of hundreds of applicants, to be a member of the Cisco Champions team in 2015. Congratulations! _ Cisco Champions are seasoned IT technical experts and influencers who enjoy sharing their knowledge, expertise, and thoughts across the social web and with Cisco. The Cisco Champions program encompasses different areas of interest, such as Data Center, Internet of Things, Enterprise Networks, Collaboration and Security. Cisco Champions are located all over the world. This is amazing",
    
  },
  {    
    "id" : "http-0-0-0-0-4000-sma-get-orchestratorrunbook-cannot-bind-1421745491",
    "site" : "damianflynn.com",
    "title": "SMA: Get-OrchestratorRunbook Cannot Bind",      
    "url": "http://0.0.0.0:4000/sma-get-orchestratorrunbook-cannot-bind/",                    
    "categories" : ["IT Pro/DevOps","Monitoring & Management","Supportability"],
    "tags" : ["PowerShell","Synchronization","Orchestrator","Service Management Automation","SQL Server","Windows Azure Pack"],
    "authors" : ["Damian Flynn"],      
    "publishedDate" : "2015-01-20 09:18:11 +0000",
    "content" : "As you create and publish Runbooks in SMA, some of these may require to execute runbooks which are hosted on Orchestrator. Over time these might all be working just as expected, however; one day out of the blue your latest masterpiece of Orchestration and Workflows may decide to turn evil and start failing with no logical explanation evident… The only tell tail sign is visible in the history log for the Workflow which is failing, with this real ugly message appearing The source of this error is the SMA command Get-OrchestratorRunbook which in our workflow would be defined as similar to line 28 in the sample below. So, what is the issue? All the parameter’s are correct – $url – points to the correct location in Orchestrator ODATA Web Services $PSUserCred – references real credentials with access to the Orchestrator ODATA Web Services $MyRunbookPath – is the correct path to our target runbook Everything checks out; we have many more runbooks hosted on the same Orchestrator environment, all working fine, and to make things worse; many of them are also been successfully called from SMA at this time… Investigation We being the diagnostics looking at the error log as shown above, but sadly it really does not offer a lot of details, cryptic to be honest. So we need to figure out what is happening at this call, which results in the error. To assist, we will create a function of our own, to simulate what this SMA function is trying to do, so that we can get a View into the workings of this issue. The function will attempt to establish a connection with the ODATA service, then search for our runbook; once located the runbook will then be checked to see if we have parameters defined on the runbook; In the case where the runbook is not located, we will be informed that this is the case also. Calling this function is also very simple, and similar to the code we use in SMA (on purpose); passing in parameters including the location of the Orchestrator ODATA URL, credentials to authenticate with the service, and the name of the runbook which we are interested in. As an example, let’s consider the query of an older runbook which is working fine at the moment. PS&gt; $myCreds = Get-Credential PS&gt; Get-OrchestratorRunbook -RunbookName \"3.1.1. My Original Runbook\" -Server \"api.orchestrator.diginerve.net\" -Credentials $myCreds 8204a38b-fdc9-4381-9368-8da459bb793d (\\3. Management\\3.1. Workitems\\3.1.1. My Original Runbook) c8a4550b-2328-4fae-9241-321ca17e3ab6 (Mode) 03637ac2-a875-4555-a300-93af698c4602 (WorkItemID) From the output of this command, we can quickly see that the runbook is located, and it had two parameters detected. Now, lets try this again, this time using the name of the runbook which is giving us trouble from SMA. PS&gt; Get-OrchestratorRunbook -RunbookName \"3.1.2. My New Runbook\" -Server \"api.orchestrator.diginerve.net\" -Credentials $myCreds Runbook Not Found As you can see, the results are unexpected, with the function reporting that our runbook is not found… So at this point, we go back, triple check the runbook name in orchestrator, its permissions and in general that everything looks correct. Once satisfied, we can retest, and hopefully get the details of our runbook. But, technology been technology – we know that its not going to be so simple, after all SMA has been failing on this runbook consistently, so we really have a reproduction of the problem with our test. Deduction With the data to confirm that our runbook is indeed created, named, published and delegated we know that the work in orchestrator is done correctly. Similarly, in SMA we know that it is also not broken, in this case we have confirmed the error message which is reported is indeed valid, as the ODATA interface is not exposing our runbook. Solution With the issue clearly on the Orchestrator ODATA service, what we are experiencing here is a known issue with this web service, where it can fail to keep up-to-date with all the runbooks which are registered in our environment. To resolve this, we have some options, but the simplest and fastest method is to use the Orchestrator Health Checker. NOTE: I have posted details on this tool – Install and Configure the Health Checker, followed by a post on Using the Health Checker _ From the **Options **menu, we simple need to select the option **Flush Web Service Cache **and then select the appropriate option for rebuilding the ODATA endpoint, personally I will select **Full Re-build. **This will take a few moments depending on the size and complexity of our Orchestrator environment; but once complete a popup will be presented to confirm the action is done. Now, finally, we can loop back and run our test function again. Assuming everything has worked out, we should now be able to locate the runbook, and expose its parameter’s PS&gt; Get-OrchestratorRunbook -RunbookName \"3.1.2. My New Runbook\" -Server \"api.orchestrator.diginerve.net\" -Credentials $myCreds 39314a8d-c89f-4fe0-adf9-3aade8ed7286 (\\3. Management\\3.1. Workitems\\3.1.1. My New Runbook) 130e7f87-d08f-4201-b912-953459828b69 (WorkItemID) With this now working, we can finally loop back to SMA, and validate the workflow is working correctly.",
    
  },
  {    
    "id" : "http-0-0-0-0-4000-2015-microsoft-mvp-1421705292",
    "site" : "damianflynn.com",
    "title": "2015 Microsoft MVP",      
    "url": "http://0.0.0.0:4000/2015-microsoft-mvp/",                    
    "categories" : ["Announcements"],
    "tags" : ["Achivements","MVP","Community"],
    "authors" : ["Damian Flynn"],      
    "publishedDate" : "2015-01-19 22:08:12 +0000",
    "content" : "Wow, its already Mid January and this poor blog has been getting a whole lot of neglect. Well don’t give up just yet, as things are about to get a little busy around here. January 1st was once again a really great day with the arrival of my 5th confirmation mail from Microsoft, honoured to be endowed once again as a Cloud and Data Centre MVP. So what happened to 2014 content, well quite a lot of that ended up being posted on a 3rd party blog site - petri.com; but for 2015 I am back home where I really belong. with Windows 10 in the air I can confirm we will have a lot to cover; I have also nuked out my lab over the last weeks, and have begun a complete ground up rebuild, and I have chosen to document this in all its gory details, which should make from some very interesting how-to content. On a personal note, after 17 years of being very happily married, last September we were endowed with the miracle of a little girl, who to say has turned my blogging and book writing time upside down is nothing but an understatement, though I do think that there might be some form of routine peering trough the cracks again - of course this is now jinxed…",
    
  },
  {    
    "id" : "http-0-0-0-0-4000-powershell-deployment-toolkit-v3-0-desired-state-configuration-1414406562",
    "site" : "damianflynn.com",
    "title": "PowerShell Deployment Toolkit V3.0 – Desired State Configuration",      
    "url": "http://0.0.0.0:4000/powershell-deployment-toolkit-v3-0-desired-state-configuration/",                    
    "categories" : ["Cloud Strategy","IT Pro/DevOps","Monitoring & Management","Virtual Machines"],
    "tags" : ["PowerShell","Desired State","Hyper-V","PowerShell Deployment Toolkit","System Center","Configuration Manager","Orchestrator","Service Management Automation","Operations Manager","Operations Management Suite","Service Manager","Service Provider Framework","Virtual Machine Manager","SQL Server","Self Service","Cloud","Windows Azure Pack","Windows Server 2012"],
    "authors" : ["Damian Flynn"],      
    "publishedDate" : "2014-10-27 10:42:42 +0000",
    "content" : "It should come as no surprise that I am an avid fan of the PowerShell deployment toolkit; given the number of posts I have published on this blog and over on perti.com A quick reminder of some of these posts will include: Deploying System Center Using the PowerShell Deployment Toolkit PowerShell Deployment Toolkit: Introducing Variables.XML But, now with the opening of TechEd Europe 2014, the world is about to be shaken all over again, as Desired State Configuration takes grip of one of my favourite utilities. If you have not figured out what all the fuss is about, then please, indulge yourself, as this is going nowhere, and we have a lot more amazing stuff to come. The following are some of the posts I posted on the Petri site over the last few months, and could not be more appropriate, than they could be, with the news today we are about to discover Deploying a Desired State Configuration Web Host Using PowerShell Deploying a Desired State Configuration Web Host Using DSC Desired State Configuration Host Deployment: Local Configuration Manager Desired State Configuration and Local Configuration Manager What Can I Configure Using Desired State Configuration? How Do I Create a Desired State Configuration? How to Publish a Desired State Configuration Using Community Desired State Configuration Resources Using Community Desired State Configuration Resources How to Participate in the Desired State Configuration (DSC) Community How Do I Create My Own Desired State Configuration (DSC) Resource? Where Do I Add the Code for My Desired State Configuration (DSC) Module? The Guru that is Rob Willis, (who also is about to get Married in the new few days – Congratulations Rob!); has been quietly rebuilding his work of art, the PowerShell deployment tool kit, to deliver today as part of the Desired State Resource Kit – Wave 8 – the xDEPLOY resource; when you crack open this kit you will see the first of this amazing work materialize with the support for the following System Center components included SQL (MS SQL Server) - https://gallery.technet.microsoft.com/xSQLServer-PowerShell-12d76584 VMM (MS Virtual Machine Manager) - https://gallery.technet.microsoft.com/xSCVMM-PowerShell-Desired-0025abcc SPF (MS Service Provider Framework) - https://gallery.technet.microsoft.com/xSCSPF-PowerShell-Desired-128739a5 SMA (MS Service Management Automation) - https://gallery.technet.microsoft.com/xSCSMA-PowerShell-Desired-ee9ed838 SR (MS Service Reporting) - https://gallery.technet.microsoft.com/xSCSR-PowerShell-Desired-195ad3c3 OM (MS Operations Manager) - https://gallery.technet.microsoft.com/xSCOM-PowerShell-Desired-052fc73c DPM (MS Data Protection Manager) - https://gallery.technet.microsoft.com/xSCDPM-PowerShell-Desired-f86f6777 All of these components, are supplied in the kith with individual component examples; in the Windows 2012 R2 series. But, that’s not all, as we are now in the area of threshold, I have been told by an informed insider, that this kit also includes support to deploy System Center Technical Preview, on Windows Server Technical Preview. Go Download NOW! And check back later from some samples!",
    
  },
  {    
    "id" : "http-0-0-0-0-4000-configuring-service-management-automation-repeating-schedules-1413279903",
    "site" : "damianflynn.com",
    "title": "Configuring Service Management Automation for Repeating Schedules",      
    "url": "http://0.0.0.0:4000/configuring-service-management-automation-repeating-schedules/",                    
    "categories" : ["IT Pro/DevOps","Monitoring & Management"],
    "tags" : ["PowerShell","Azure Automation","Service Management Automation","Self Service","Cloud","Windows Azure Pack","Windows Server 2012"],
    "authors" : ["Damian Flynn"],      
    "publishedDate" : "2014-10-14 09:45:03 +0000",
    "content" : "Service Management Automation is one of these tools which quickly becomes the central location to host and execute all our scripting activities. One of the more useful options for the service is the ability to schedule the execution of these scripts so that we can set and forget, allowing SMA to take over from what was once the task of the Windows Scheduler. However, the first time you are faced with the requirement to execute a flow on a schedule of 30 minutes for example, you will quickly release that this is still a V1 release, as the minimum granularity which we can define for execution is one day! OPPS! Now What?!? Never fear, there is an answer, but it is not so pretty. As we can see for the above screen grab, we can define the Hour and Minutes at which the schedule should occur daily, therefore we can easily create a schedule to trigger for each of our required intervals. In this case we would create a schedule for example to Execute Flow at 00:30 **then, save this and create another schedule **Execute Flow at 01:00, and repeat this cycle. Resulting in no less than 48 schedule entries, to tackle our requirement. While this might not be ideal, using the web UI only leads to increasing the anxiety, so I have chosen to use PowerShell to make our life a whole lot less stressful. PowerShell to the Rescue! I have created a simple function in PowerShell which will loop through the process of creating each of our schedules, providing just some very basic details, including the prefix for the schedule name, for example “**Execute Flow At **”, along with the interval for which these schedules names should be created for in minutes, and then of course the HTTPS endpoint of our SMA Service. An example of how I call this function would be as follows ADD-SMARecurringSchedule –Interval 20 –ScheduleNamePrefix \"Execute Flow at \" –WebServiceEndpoint https://pdc-sc-sma01 The results is a labour of love, with all the requested schedules created and ready for our consumption. Workflows We are not done yet however, all that we have succeed to do yet is define the schedules. Now we must proceed to associate some, or all of these with the workflow which we require to be triggered. On the Web UI we can navigate to the workflow in question, and from the associated Schedule page, we can click on the Schedule icon in the drawer, and select the option _Use Existing _which will present the dialog to ‘_Select a schedule’ _offering only the options which have not already been associated with the workflow. From here we can select one of the Schedules and click on the _Tick _then confirm our choice. Again, you will quickly get the V1.0 feel for the portal when you realise that we cannot multi-select options here, and we are once again facing the daunting reality of having to repeat this exercise far too many times; thus back to PowerShell. PowerShell to Associate our Recurring Schedules to the Workflow Leveraging the schedules we created in the previous step, I have created a second function which leverages the same logic loop as before, but this time also accepts a runbook name, to which the function will add the recurring schedule to recursively. In a very similar experience to before, the command which I will issue would look as follows ADD-SMARunbookRecurringSchedule –Interval 20 –ScheduleNamePrefix \"Execute Flow at \" –WebServiceEndpoint https://pdc-sc-sma01 –RunbookName \"My Special Runbook\" One completed, the result is just as we expect, and the previously defined Schedules are now associated with the runbook, ensuring it will trigged at the desired interval. The Module Enough already, the following is the code which I have created to make our lives easier. [getgit repoid=”PowerShell” userid=”DamianFlynn” path=”Scripts/SMA/Create-SMARecurringSchedules.psm1” language=”powershell”] Feel free to fork and update if you like.",
    
  },
  {    
    "id" : "http-0-0-0-0-4000-downloading-scom-management-packs-using-powershell-1409070885",
    "site" : "damianflynn.com",
    "title": "Downloading SCOM Management Packs using PowerShell",      
    "url": "http://0.0.0.0:4000/downloading-scom-management-packs-using-powershell/",                    
    "categories" : ["IT Pro/DevOps","Monitoring & Management"],
    "tags" : ["Community","PowerShell","OpenSource","Git / GitHub","System Center","Operations Manager"],
    "authors" : ["Damian Flynn"],      
    "publishedDate" : "2014-08-26 16:34:45 +0000",
    "content" : "One of the most important features of using SCOM, is the vast knowledge which is offered thought the use of the Management Packs. With power always comes challenges, and the key challenge in this case, is locating and downloading all these packs; and also staying up to date. As you are hopefully aware, my buddy Stan over at the CloudAdministrator blog has published and maintained a script up on the Technet Galleries site designed to assist in allowing us to build this repository for our daily use. His script parses the SCOM Wiki page which the MS team maintain related to all the new and updated management packs, and from this reaches out, and grabs a nice local copy of the files for us. Wanting to use this script to do some automation and notifications related to the packs which have been updates, added new, or simply re-downloaded I quickly became blocked due to its monolithic structure. As a bit of a fiend for PowerShell modules, I set to split the script into private functions, change some of the parsing logic, and ultimately create a single PowerShell Commandlet to do the work, while telling me of its progress, and formatting the output as true objects which I can now use to group by MP Status, and so on. As I also spend a fair amount of time working with the rest of the System Center Suite, I wanted to also change the logging to the normal CMTrace format which I have become familiar with, but not to upset current users of the script, simply added a switch called -CMTrace to the command, to have the new format enabled. The updated work, with the blessing and overview of Stan is back on the TechNet Gallery, ready for your consumption. This is not the final destination for this script however, I had a plan which drove me to this middle ground, so in the coming days expect to see yet another “edition” of the script which will be destined for the Gallery also - More later. Thanks to Stan and the community for the continued sharing, and we look forward to your comments and input.",
    
  },
  {    
    "id" : "http-0-0-0-0-4000-software-defined-networking-with-windows-server-and-system-center-jump-start-1394278948",
    "site" : "damianflynn.com",
    "title": "Software-Defined Networking with Windows Server and System Center Jump Start",      
    "url": "http://0.0.0.0:4000/software-defined-networking-with-windows-server-and-system-center-jump-start/",                    
    "categories" : ["Announcements","Cloud Strategy","Events","IT Pro/DevOps","Networking","Virtual Machines"],
    "tags" : ["Webinar","Live Event","Community","Networking","Software Defined Networking","Hyper-V","System Center","Service Provider Framework","Virtual Machine Manager","Cloud","Windows Azure Pack","Windows Server 2012"],
    "authors" : ["Damian Flynn"],      
    "publishedDate" : "2014-03-08 11:42:28 +0000",
    "content" : "Free online event with live Q&amp;A with the Microsoft Networking Team: http://aka.ms/SftDnet Wednesday, March 19th from 8am – 1pm PST Are you exploring new networking strategies for your datacenter? Want to simplify the process? Software-defined networking (SDN) can streamline datacenter implementation through self-service provisioning, take the complexity out of network management, and help increase security with fully isolated environments. Intrigued? Bring specific questions, and get answers from the team who built this popular solution! Windows Server 2012 R2 and System Center 2012 R2 are being used with SDN implementations in some of the largest datacenters in the world, and this Jump Start can help you apply lessons learned from those networks to your own environment. From overall best practices to deep technical guidance, this demo-rich session gives you what you need to get started, plus in-depth Q&amp;A with top experts who have real-world SDN experience. Don’t miss it! Register here: http://aka.ms/SftDnet",
    
  },
  {    
    "id" : "http-0-0-0-0-4000-preview-azure-with-mpls-1393012320",
    "site" : "damianflynn.com",
    "title": "Preview Azure with MPLS!",      
    "url": "http://0.0.0.0:4000/preview-azure-with-mpls/",                    
    "categories" : ["Announcements","Cloud Strategy","IT Pro/DevOps","Networking","Updates","Virtual Machines","Web"],
    "tags" : ["Azure","Networking","Software Defined Networking","Cloud","Windows Azure Pack"],
    "authors" : ["Damian Flynn"],      
    "publishedDate" : "2014-02-21 19:52:00 +0000",
    "content" : "Wow, only yesterday I realised that I missed the announcement, and today Microsoft take another step and move this from concept to realty, by offering the solution in Preview Form! You can get started using this link - http://www.windowsazure.com/en-us/services/preview/ The first question to come to mind will be Price, which is rather encouraging, on a monthly plan a 1Gb port will cost only €224, with 15Tb outbound included (€0.027/GB after) and Unlimited Inbound… via an Exchange; or if you are working with a Network Service Provider, a 100Mbps port will cost €671 with unlimited in and outbound traffic. These include dual ports as standard for redundancy, and are 50% discount prices for launch, but even at the full cost, these are numbers we can work with, when we start comparing apples to apples. Finally, Azure can begin to look like a true datacenter alternative; the next budgeting cycle will be interesting :) To tease a little more, this is an extract from the over posted here - http://www.windowsazure.com/en-us/services/expressroute/ #### Experience a faster, private connection to Windows Azure &gt; &gt; Windows Azure ExpressRoute enables you to create private connections between Azure datacenters and infrastructure that’s on your premises or in a colocation environment. ExpressRoute connections do not go over the public Internet, and offer more reliability, faster speeds, lower latencies and higher security than typical connections over the Internet. In some cases, using ExpressRoute connections to transfer data between on-premises and Azure can also yield significant cost benefits. &gt; &gt; With ExpressRoute, you can establish connections to Azure at an ExpressRoute location (Exchange Provider facility) or directly connect to Azure from your existing WAN network (such as a MPLS VPN) provided by a network service provider. &gt; &gt; ![ExpressRoute connection options](http://www.windowsazure.com/images/page/services/expressroute/diagram.png) &gt; &gt; Currently, ExpressRoute is available in the US through the following partners: &gt; &gt; [![image](/assets/posts/2014/02/image_thumb.png)](/assets/posts/2014/02/image.png) &gt; &gt;",
    
  },
  {    
    "id" : "http-0-0-0-0-4000-connecting-to-azure-with-mpls-1392938340",
    "site" : "damianflynn.com",
    "title": "Connecting to Azure with MPLS",      
    "url": "http://0.0.0.0:4000/connecting-to-azure-with-mpls/",                    
    "categories" : ["Announcements","Cloud Strategy","IT Pro/DevOps","Networking","Updates","Virtual Machines","Web"],
    "tags" : ["Azure","Networking","Software Defined Networking","Cloud","Windows Azure Pack"],
    "authors" : ["Damian Flynn"],      
    "publishedDate" : "2014-02-20 23:19:00 +0000",
    "content" : "I must not have been paying attention, but having a discussion with fellow MVP Ed Horley, I just realised that I missed what is a very important announcement for organisations or enterprises which would wish to embrace Azure as a true data-center alternative. What am I referring to? MPLS of course. Apparently on the 18th of September, while I was making my way back from System Center Universe in Switzerland; the good people of AT&amp;T and Microsoft announced an agreement to bridge the clouds with this magical networking technology. This is an extract from that same announcement DALLAS — Sept. 18, 2013 — [AT&amp;T](http://www.att.com/gen/landing-pages?pid=3309)** and Microsoft Corp. today announced they will deliver a security-enhanced and reliable [cloud solution](http://www.business.att.com/enterprise/Portfolio/cloud/) that will allow enterprise customers to connect to Microsoft’s cloud platform using a private network. &gt; &gt; The new solution will use breakthrough cloud integration technology from AT&amp;T to pair [virtual private networking](http://www.business.att.com/enterprise/Family/network-services/ip-vpn/) with Microsoft’s industry-leading Windows Azure cloud platform. Customers of the solution are expected to benefit from the enterprise-grade security of virtual private networking, with as much as 50 percent lower latency than the public Internet, and access to cloud resources from any site using almost any wired or wireless device. &gt; &gt; “This is a game changer for businesses that have been seeking a more secure way to reap the benefits of cloud services,” said Andy Geisse, CEO, AT&amp;T Business Solutions. “By bringing the security and performance of our virtual private network to Windows Azure, we expect to energize enterprise demand for cloud solutions.” &gt; &gt; Enterprises continue to cite concerns about security and reliable performance in their decision to adopt cloud computing. Working together, Microsoft and AT&amp;T will address these concerns by enabling enterprise customers to quickly and reliably connect applications and services from their own datacenters (private clouds) to the Windows Azure cloud service using the protective confines and high transmission speeds of a highly-secure virtual private network. &gt; &gt; You can find the Full details here - http://www.microsoft.com/en-us/news/press/2013/sep13/09-18msattpr.aspx And now, when you start budgeting for 2015, you need to take another look at your true costs, and potential new options… Just saying ;)",
    
  },
  {    
    "id" : "http-0-0-0-0-4000-book-3-microsoft-system-center-building-a-virtualized-network-solution-1392820980",
    "site" : "damianflynn.com",
    "title": "Book 3: Microsoft System Center: Building a Virtualized Network Solution",      
    "url": "http://0.0.0.0:4000/book-3-microsoft-system-center-building-a-virtualized-network-solution/",                    
    "categories" : ["Announcements","Cloud Strategy","IT Pro/DevOps","Monitoring & Management","Networking","Virtual Machines"],
    "tags" : ["Achivements","MVP","Cisco Champion","Book","Community","Networking","Software Defined Networking","Hyper-V","System Center","Virtual Machine Manager","SQL Server","Cloud","Windows Azure Pack","Windows Server 2012"],
    "authors" : ["Damian Flynn"],      
    "publishedDate" : "2014-02-19 14:43:00 +0000",
    "content" : "I am delighted to see that my 3rd book on Microsoft Private Cloud OS has gone public a few moments ago. This book is currently available for free as an eBook, or for around €10 in print from Amazon. You can use the links below as a short cut to get the different versions: [bullets color=”#ffffff” type=”theme” effect=”fadeInUp”] [bullet icon=”fa-gift”]Download the PDF (6.68 MB)[/bullet] [bullet icon=”fa-gift”]Download the EPUB file (25.4 MB)[/bullet] [bullet icon=”fa-gift”]Download the Mobi for Kindle file (45.4 MB)[/bullet] [/bullets] None of this would have been possible without the co-operation and dedication of Nigel Cain (MS), who over the last 12 months, I have had the pleasure of blogging with on the System Center VMM blogs on this SDN topic, and from their to this book. My co-Author Michel is back, after working on the earlier book on Hyper-V Configuration and Installation; and the guy that you will get when you call MS support on this great topic, Alvin was there to add his experience from the front line. We also had a lot of technical reviewers on the project, almost to many to name, but I will call out my MVP colleagues Stan and Hans who were exposed to an early edit! Thanks’ Guys, and of course a thank you to each and everyone of the review team, with a special call out to the SCVMM Networking PM, Greg Cusanza. This book is focused on the SP1 version of SCVMM, we are already working on the updates to cover the additions added in R2, including the cool NVGRE Gateway. Your feedback on this version is going to be greatly appreciated, and suggestions for the update version are welcomed – please comment on the post, mail me, or hit me on Twitter. Enjoy!",
    
  },
  {    
    "id" : "http-0-0-0-0-4000-system-center-2012-r2-and-sql-server-1392231960",
    "site" : "damianflynn.com",
    "title": "System Center 2012 R2 and SQL Server",      
    "url": "http://0.0.0.0:4000/system-center-2012-r2-and-sql-server/",                    
    "categories" : ["Database","IT Pro/DevOps","Monitoring & Management","Security","Storage, Backup & Recovery","Supportability","Virtual Machines"],
    "tags" : ["White Paper / Case Study","Community","PowerShell Deployment Toolkit","System Center","Configuration Manager","Orchestrator","Service Management Automation","Operations Manager","Service Manager","Service Provider Framework","Virtual Machine Manager","SQL Server","Cloud","Windows Server 2012"],
    "authors" : ["Damian Flynn"],      
    "publishedDate" : "2014-02-12 19:06:00 +0000",
    "content" : "System Center is nothing without data, and Microsoft SQL Server is the central repository for all our data; which essentially boils down two a very simple point; we need to ensure that SQL runs at its optimal, and is always available. My good friend, Paul Keely with Infront Consulting has burned up some of his free hours to gather the latest and greatest facts related to tuning SQL to host our environment, and the results are up for grabs right now for FREE, from the TechNet Gallery! If you are slightly interested in System Center, this book covers an area which is essentially untouched in any writer documents, and builds on his earlier revision published a few months ago. In Pauls Own words: _In this guide I have decided to be very much more focused on SQL and its interaction with System Center and indeed Server 2012.  In the last guide I felt that a lot of people were already installing SQL for quite some time and there was a lot of knowledge out there on SQL installs but not best practices.  SQL 2012 and Server 2012 have brought some real game changers to the story, and because of that this guide is going to go into way more detail on areas like setup and some of the options we have here.  We are not going to walk you through a basic SQL setup, there are a ton of guides for that on the internet, instead we are going to look at some of the more complex areas like clustering, AlwaysOn, Storage Spaces and SQL Azure.  If you are new to this subject then I suggest that you download the guide above and use in in conjunction to this one._ [bullets color=”#ffffff” type=”theme” effect=”fadeInUp”] [bullet icon=”fa-gift”]Go Get You Copy NOW![/bullet] [/bullets] Stop delaying, get reading!",
    
  },
  {    
    "id" : "http-0-0-0-0-4000-system-centre-universe-2014-north-america-1391099520",
    "site" : "damianflynn.com",
    "title": "System Centre Universe 2014 North America",      
    "url": "http://0.0.0.0:4000/system-centre-universe-2014-north-america/",                    
    "categories" : ["Cloud Strategy","Events","IT Pro/DevOps","Monitoring & Management","Virtual Machines"],
    "tags" : ["Conference","Live Event","Community","Networking","Software Defined Networking","Hyper-V","System Center","Orchestrator","Service Management Automation","Operations Manager","Service Manager","Service Provider Framework","Virtual Machine Manager","Self Service","Cloud","Windows Azure Pack","Windows Server 2012"],
    "authors" : ["Damian Flynn"],      
    "publishedDate" : "2014-01-30 16:32:00 +0000",
    "content" : "Year 3, and we are ready for off. This year see’s no less than 3 independent conferences focused on the latest and greatest technologies from Microsoft Windows and System Center. Today is the day the first of these amazing conferences launches, and I am going to be one of the 2000+ virtual attendees to tune in for the 8 hours of amazing content, which for the curious can be located right here: http://www.systemcenteruniverse.com/agenda.htm Heading today’s sessions is the amazing Kent Agerlund, with an free look at Configuration Manager 2012 R2. Of course, all this starts in the middle of our working day in Europe, and thanks to the magic of streaming, we have the option to pause and rewind, while we juggle calls, and get some real work done. Superman Kent heads up my workstation with a new Temporary screen specifically for the event! BTW – if you are in Europe, book your ticket to the event in Basil, Switzerland, which i am delighted to be part of the presentation team again this year - http://www.systemcenteruniverse.ch/ And for my friends in Asia, March 13th is your date; grab your ticket now right here http://www.systemcenteruniverse.asia/ and join my good friends Raymond, Marteen and Cameron, along with my co-author Nigel on what will be a fantasic day also.",
    
  },
  {    
    "id" : "http-0-0-0-0-4000-techdays-berlin-2013-deliver-your-cloud-like-a-hoster-1389802500",
    "site" : "damianflynn.com",
    "title": "Techdays Berlin 2013 - Deliver Your Cloud Like a Hoster",      
    "url": "http://0.0.0.0:4000/techdays-berlin-2013-deliver-your-cloud-like-a-hoster/",                    
    "categories" : ["Cloud Strategy","Events","IT Pro/DevOps","Monitoring & Management","Virtual Machines"],
    "tags" : ["Conference","Live Event","Community","Networking","Software Defined Networking","Hyper-V","System Center","Orchestrator","Service Management Automation","Operations Manager","Service Manager","Service Provider Framework","Virtual Machine Manager","Self Service","Cloud","Windows Azure Pack","Windows Server 2012"],
    "authors" : ["Damian Flynn"],      
    "publishedDate" : "2014-01-15 16:15:00 +0000",
    "content" : "In November 2013, I had the pleasure of joining the speaker list, for presenting at the first ever TechDays Berlin event. Hosted in what can only be described as a beautiful building in the majestic city of Berlin. The presentation can be streamed by simply clicking on the image below, as we begin to discuss some of options in the Windows System Centre Stack to deliver your cloud as a hoster.",
    
  },
  {    
    "id" : "http-0-0-0-0-4000-system-centre-universe-2014-dach-1389718080",
    "site" : "damianflynn.com",
    "title": "System Centre Universe 2014 DACH",      
    "url": "http://0.0.0.0:4000/system-centre-universe-2014-dach/",                    
    "categories" : ["Cloud Strategy","Events","IT Pro/DevOps","Monitoring & Management","Virtual Machines"],
    "tags" : ["Conference","Live Event","Community","Networking","Software Defined Networking","Hyper-V","System Center","Orchestrator","Service Management Automation","Operations Manager","Service Provider Framework","Virtual Machine Manager","Cloud","Windows Azure Pack","Windows Server 2012"],
    "authors" : ["Damian Flynn"],      
    "publishedDate" : "2014-01-14 16:48:00 +0000",
    "content" : "Marcel, Thomas, and the team at ITNEX are doing it again. Mark you calendars for the 17th, 18th and 19th of September, and join us in the Congress Center, Basel, Switzerland. I am delighted to be re-joining the presentation team again this year, and can promise that we will have some fantastic content and new concepts to tear down, and build out. There are currently no plans for streaming, so book the tickets now to ensure you get a front and center seat; and yes many of the presentations will be in English, so no excuses, as my German is, well, shameful.",
    
  },
  {    
    "id" : "http-0-0-0-0-4000-cloud-os-community-windows-azure-pack-2013-introduction-1389543240",
    "site" : "damianflynn.com",
    "title": "Cloud OS Community - Windows Azure Pack 2013 Introduction",      
    "url": "http://0.0.0.0:4000/cloud-os-community-windows-azure-pack-2013-introduction/",                    
    "categories" : ["Cloud Strategy","Events","IT Pro/DevOps","Virtual Machines"],
    "tags" : ["Webinar","Community","Hyper-V","System Center","Service Management Automation","Operations Manager","Service Provider Framework","Virtual Machine Manager","Self Service","Cloud","Windows Azure Pack","Windows Server 2012"],
    "authors" : ["Damian Flynn"],      
    "publishedDate" : "2014-01-12 16:14:00 +0000",
    "content" : "Kicking off the Year, We start with the good stuff. In co-operation with the Cloud OS Community, we begin with an overview of the Windows Azure Pack 2013 Framework.",
    
  },
  {    
    "id" : "http-0-0-0-0-4000-mvp-2014-system-center-cloud-and-datacenter-1388592960",
    "site" : "damianflynn.com",
    "title": "MVP 2014: SYSTEM CENTER CLOUD AND DATACENTER",      
    "url": "http://0.0.0.0:4000/mvp-2014-system-center-cloud-and-datacenter/",                    
    "categories" : ["Announcements"],
    "tags" : ["Achivements","MVP","Community"],
    "authors" : ["Damian Flynn"],      
    "publishedDate" : "2014-01-01 16:16:00 +0000",
    "content" : "A Picture says 1000 Words, and as it is January 1st we have some celebrating to do. Of course its New Years Day which is cool for sure, but not as cool as getting the new for the 4th year in succession that I have been awarded the coveted Microsoft MVP again. Thank you,  thank Microsoft, and here is hoping your year is also off to a great start.",
    
  },
  {    
    "id" : "http-0-0-0-0-4000-site-updates-1386371760",
    "site" : "damianflynn.com",
    "title": "Site Updates",      
    "url": "http://0.0.0.0:4000/site-updates/",                    
    "categories" : ["Announcements","Updates"],
    "tags" : ["MVP","Community","Networking","Active Directory","System Center","Orchestrator","Operations Manager","Virtual Machine Manager","Cloud","Windows Azure Pack"],
    "authors" : ["Damian Flynn"],      
    "publishedDate" : "2013-12-06 23:16:00 +0000",
    "content" : "I am afraid that over the last few months my personal blog site has taken a back seat, as I focused on a heavy workload; but rest assured I have some good plans on how to make this site even more valuable as we move into 2014. So what have i being working on? Well first off, on the blogging side I have focused my efforts on updating the Petri Knowledge Base with a lot of new content on System Center technologies; I have been a fan of Petri for many years, due to the fact that unlike many sites its content was free, simple and to the point. Much of that has not changed, but as Software has begun to get more complicated, so are the simple how-to guides. So, make sure you book mark www.petri.co.il and check out the great content on that site, from some familiar names. There there are the presentations, and over the last few months its been very busy. Ill post some links to the sessions which made it to be recorded, and I have also started to upload a lot of the slide decks to slide share if you would like to review. And if all that was not enough, I have done it again, and with a Nigel Cain (my Microsoft partner in crime whom worked with me on the Networking posts we published on the Technet blogs earlier this year), Michel Luescher (one of my Co-Authors on the Hyper-V Installation and Configuration Book), and Alvin Morales (The MS Guy you will speak to when you open a P1 support case for SCVMM issues) we have attached yet another book to add to our growing collection. This one will be available later January, so treat yourself to a nice Christmas gift and pre-order today - Microsoft System Center: Building A Visualized Network!",
    
  },
  {    
    "id" : "http-0-0-0-0-4000-tech-days-online-uk-2013-1385914260",
    "site" : "damianflynn.com",
    "title": "Tech.Days Online UK 2013",      
    "url": "http://0.0.0.0:4000/tech-days-online-uk-2013/",                    
    "categories" : ["Announcements","Cloud Strategy","Events","IT Pro/DevOps","Updates","Virtual Machines"],
    "tags" : ["Webinar","Live Event","Community","Networking","Software Defined Networking","System Center","Virtual Machine Manager","Cloud","Windows Azure Pack","Windows Server 2012"],
    "authors" : ["Damian Flynn"],      
    "publishedDate" : "2013-12-01 16:11:00 +0000",
    "content" : "In 20 minutes, learn how we can leverage SCVMM to manage your Data Centre storage systems. For the presentation we created a new Windows Storage Spaces environment in a 100% virtual world, and then learn how to leverage the power of SCVMM to bring this to life. [title=”SC Virtual Machine Manager 2012 R2 Storage Management” thumb=”/wp-content/uploads/2014/01/techdaysonlineuk2013.png”]https://vts.inxpo.com/scripts/Server.nxp?LASCmd=AI:1;F:SF!42000&amp;EventKey=115555",
    
  },
  {    
    "id" : "http-0-0-0-0-4000-joining-petri-co-il-as-wssc-author-1374167400",
    "site" : "damianflynn.com",
    "title": "Joining PETRI.CO.IL As WS/SC Author",      
    "url": "http://0.0.0.0:4000/joining-petri-co-il-as-wssc-author/",                    
    "categories" : ["Announcements","Updates"],
    "tags" : ["MVP","PowerShell","Desired State","Networking","Software Defined Networking","PowerShell Deployment Toolkit","System Center","Azure Automation","Orchestrator","Service Management Automation","Operations Manager","Service Manager","Service Provider Framework","Virtual Machine Manager","Cloud","Windows Azure Pack","Windows Server 2012"],
    "authors" : ["Damian Flynn"],      
    "publishedDate" : "2013-07-18 17:10:00 +0000",
    "content" : "As we spend time, fine turning our support mechanisms, one of the go-to locations which I spent a lot of time in my early years leveraging was the somewhat infamous “Petri.co.il IT Knowledgebase”. We, now I am about to announce something which I never  expected to announce, but I am delighted to be joining the authoring team, with sole focus on Windows Server / System Center. There is a lot to cover, and I have not yet figured out how I am going to segregate the information which will be posted here, and also on the Knowledgebase, but I am sure that over time this I will indeed sort out. So, If you are interested in keeping up with my sharing’s, I suggest you add the new RSS feed http://www.petri.co.il/author/damian-flynn/feed I have lots to share, and as i get some of the content organised, I will post some links here to the series as they pull together. Interesting times ahead",
    
  },
  {    
    "id" : "http-0-0-0-0-4000-r2-preview-new-free-offerings-1373584560",
    "site" : "damianflynn.com",
    "title": "System Center 2012 R2 Preview – New Free Offerings",      
    "url": "http://0.0.0.0:4000/r2-preview-new-free-offerings/",                    
    "categories" : ["Announcements","IT Pro/DevOps","Updates"],
    "tags" : ["System Center","Configuration Manager","Endpoint Protection","Azure Automation","Orchestrator","Service Management Automation","Operations Manager","Service Manager","Service Provider Framework","Virtual Machine Manager","Windows Server 2012"],
    "authors" : ["Damian Flynn"],      
    "publishedDate" : "2013-07-11 23:16:00 +0000",
    "content" : "Assuming you are playing in the lab with the new R2 Bits, then I have some helpful nuggets for you to digest. The first of these, is that Microsoft have now made available the new Integration packs for the R2 Preview series of product, If like me, you are a bit of an Orchestrator nerd, then you will want the link J http://www.microsoft.com/en-us/download/details.aspx?id=39622&amp;WT.mc_id=rss_alldownloads_all Now, as good as this is, I have something better. What have you planned for Monday (July 15th 2013)? Well, if you truly want to learn a lot more about the R2 preview products, and see what it really takes to get these running in your lab, then all I can say is, Cancel all standing appointments, and mark out 8 hours, starting at 9am. Got your attention right? So here is the inside story: This is a live, public, free, online event, all you have to do is sign up today: http://aka.ms/SCR2JS. This event will be recorded and available on the Microsoft Virtual Academy (MVA) several weeks later, so even if you cannot make the live event, sign up anyway so that you receive a notification once the course is available on the MVA. COURSE OUTLINE 01 Introduction to the Cloud OS 02 Infrastructure Provisioning 03 Infrastructure Monitoring 04 Application Management 05 Automation 06 IT Service Management Symon and Kenon will show the goods, while a bunch of MVP’s (me Included), will be manning the Q&amp;A sessions to help you out, it’s going to be a lot of fun and learning.",
    
  },
  {    
    "id" : "http-0-0-0-0-4000-managing-hyper-v-nvgre-with-powershell-1373325360",
    "site" : "damianflynn.com",
    "title": "Managing Hyper-V NVGRE with PowerShell",      
    "url": "http://0.0.0.0:4000/managing-hyper-v-nvgre-with-powershell/",                    
    "categories" : ["Cloud Strategy","IT Pro/DevOps","Monitoring & Management","Networking"],
    "tags" : ["MVP","White Paper / Case Study","Webinar","Live Event","Community","PowerShell","Networking","Software Defined Networking","Hyper-V","System Center","Virtual Machine Manager","Cloud","Windows Azure Pack","Windows Server 2012"],
    "authors" : ["Damian Flynn"],      
    "publishedDate" : "2013-07-08 23:16:00 +0000",
    "content" : "If you are still wrapping your head around the NVGRE technology which Microsoft has introduced in Windows 2012, and want to get down and dirty into how this stuff actually ticks, then I have some great news for you. Following the recent presentation and whitepapers which were just published a few weeks ago, as we “Unravelling the Network”, explaining at a higher level just how these technologies play together to create a seamless implementation for software defined networking, we are now going to deep dive. In association with the Microsoft MVP Pro Speaker series, on Tuesday July 9th at 8am EST I will present a 1 hour session demonstrating what needs to happen on the Hosts and VMs to bring NVGRE to life, using nothing but PowerShell. This presentation is targeted as the Level 300/400 so I am going to really recommend that you take an hour upfront and watch the presentation with Savision if you have not yet had the opportunity. Otherwise, please spread the word, and join me tomorrow for what promises to be an interesting session, with content not presented before on the How To, for NVGRE. UPDATE: Recording is now attached to this post All you need is a copy of Lync, or use the WebApp, and connect with this URL https://join.microsoft.com/meet/karrieo/5KH311B3 Hope to see you there Damian",
    
  },
  {    
    "id" : "http-0-0-0-0-4000-mp-control-manager-not-responding-1373066280",
    "site" : "damianflynn.com",
    "title": "SCCM - MP Control Manager not responding",      
    "url": "http://0.0.0.0:4000/mp-control-manager-not-responding/",                    
    "categories" : ["IT Pro/DevOps","Supportability"],
    "tags" : ["Web Sites","PKI","DNS","System Center","Configuration Manager","SQL Server","Windows Server 2012"],
    "authors" : ["Damian Flynn"],      
    "publishedDate" : "2013-07-05 23:18:00 +0000",
    "content" : "The days starts off with a ugly red mark on your SCCM component health, but your a professional, and these are normally par for the course, taking only a few minutes of your highly skilled fingers to put this behind you. But. Sadly as you quickly start to learn, that’s not how the ball is going to role today. Nope, this one is going to be a total plain. The simple message “MP Control Manager detected management point is not responding to HTTP requests. The HTTP status code and text is 403, Forbidden.” is about to take center stage. The investigation begins when we take a closer look at the log, only to be presented in a repeating message that the MP is not working for HTTP traffic. Quickly, you connect over to the problem Management Point, and located the current IIS logs for this node. Quickly scrolling to a point where you can match the time stamps from the Control Manager with the events in IIS, so that you can get a view of what IIS is really up to. What you see, however looks a little odd, the issue which IIS is reporting is not HTTP, but actually HTTPS (note the Port is listed in the log below as 443). The error is indeed 403, but again looking at the log it is not a simple 403.0 HTTP code, but actually is a 403.16 HTTP code. A quick search on this one, and now I am very confused, as the code translates to Client certificate is untrusted or invalid? Ok, what give? All the certificates involved here are being issues from a new CA, I have triple checked to ensure that the root chain is in place, and that all the certificates issued by this new CA are of sound mind and body. So why is IIS reporting this error? After a few minutes for searching on the web, I reached the MS KB 952061, which when summarised basically says that my root certificate is not in the root certificate store, and therefore IIS is rejecting the connection. Well, sorry IIS, but that is clearly not the case, I checked again, just to keep my sanity and my root certificate is indeed in the Trusted Root Certification Authorities store on the computer. So, more searching, as part of the error I also see that we have a code 2148204809 which is appearing every time I get this HTTP code. This one turned up yet another interesting message The operating system reported error 2148204809: A certificate chain processed, but terminated in a root certificate which is not trusted by the trust provider. Now, how could this be the problem? After all, IIS has the correct client cert offered to it, and the matching Root in its store, so a match should be obvious? Well apparently not. So back for some verification, and I happened to find this article which is related to Lync 2013, but while hosted on Windows Server 2012 – KB 2795828. In this article, the cause explanation started to raise an eyebrow. This issue occurs because a certificate that is not self-signed was installed in the Trusted Root Certification Authorities store. This is an incorrect configuration that can cause HTTP communication between Lync servers to fail with an untrusted root certificate error. Lync Server 2013 deployments in Windows Server 2012 may experience this issue because Windows Server 2012 implements checks for a higher level of trust for certificate authentication. I like what I read now, and there is a simple PowerShell command which will help me validate that my server is not affected by this misconfiguration. Running the command should return nothing, as the root store should only have root certificates. PS &gt; Get-Childitem cert:\\LocalMachine\\root -Recurse | Where-Object {$_.Issuer -ne $_.Subject} | Format-List * PSPath                   : Microsoft.PowerShell.SecurityCertificate::LocalMachineroot3DE2837CFE43D183879F1DF065CCD2CA40545FB1 PSParentPath             : Microsoft.PowerShell.SecurityCertificate::LocalMachineroot PSChildName              : 3DE2837CFE43D183879F1DF065CCD2CA40545FB1 PSDrive                  : Cert PSProvider               : Microsoft.PowerShell.SecurityCertificate PSIsContainer            : False EnhancedKeyUsageList     : {} DnsNameList              : {HIDDEN BY BLOGGER} SendAsTrustedIssuer      : False EnrollmentPolicyEndPoint : Microsoft.CertificateServices.Commands.EnrollmentEndPointProperty EnrollmentServerEndPoint : Microsoft.CertificateServices.Commands.EnrollmentEndPointProperty PolicyId                 : Archived                 : False Extensions               : {System.Security.Cryptography.Oid, System.Security.Cryptography.Oid, System.Security.Cryptography.Oid, System.Security.Cryptography.Oid...} FriendlyName             : IssuerName               : System.Security.Cryptography.X509Certificates.X500DistinguishedName NotAfter                 : 1/2/2016 6:05:05 AM NotBefore                : 3/30/2006 4:57:11 AM HasPrivateKey            : False PrivateKey               : PublicKey                : System.Security.Cryptography.X509Certificates.PublicKey RawData                  : {48, 130, 3, 135...} SerialNumber             : 41B5B1E9 SubjectName              : System.Security.Cryptography.X509Certificates.X500DistinguishedName SignatureAlgorithm       : System.Security.Cryptography.Oid Thumbprint               : 3DE2837CFE43D183879F1DF065CCD2CA40545FB1 Version                  : 3 Handle                   : 656525007248 Issuer                   : CN=Org  Root CA, O=Org Subject                  : CN=Org BI CA, O=Org, C=FI However, that’s not what happened, Instead I have a certificate in the Root Store, which is not expected to be there. Bingo! Now, looking at this certificate, I know that this has nothing to do with my SCCM environment, and getting rid of this is not going to be a problem. But where did it come from? Group Policy! Sure enough the certificate is published to the Root Store and not the Intermediate Store using Group Policy. So first things first, I get the root issue fixed, and then lets clean out this bad certificate from my local store. PS &gt; Get-Childitem cert:\\LocalMachine\\root -Recurse | Where-Object {$_.Issuer -ne $_.Subject} | del PS &gt; Get-Childitem cert:\\LocalMachine\\root -Recurse | Where-Object {$_.Issuer -ne $_.Subject} | Format-List * PS &gt; Now, all that remains is that the server gets a quick reboot. Once the server is back online, I can quickly confirm in the IIS logs that my 400.16 is now replaced with a 200.0 message and everything is once more rosy in the world. Happy Hunting",
    
  },
  {    
    "id" : "http-0-0-0-0-4000-cisco-nexus-1000v-hyper-v-1373040360",
    "site" : "damianflynn.com",
    "title": "Cisco Nexus 1000v Hyper-V",      
    "url": "http://0.0.0.0:4000/cisco-nexus-1000v-hyper-v/",                    
    "categories" : ["Announcements","Cloud Strategy","IT Pro/DevOps","Virtual Machines"],
    "tags" : ["Community","Networking","Software Defined Networking","System Center","Virtual Machine Manager","Cloud","Windows Azure Pack","Windows Server 2012"],
    "authors" : ["Damian Flynn"],      
    "publishedDate" : "2013-07-05 16:06:00 +0000",
    "content" : "Available now, the most advanced software switch for Hyper-V and Virtual Machine Manager from Cisco. And its absolutely Free! Learn about its amazing features, how it works, its Integration with SCVMM, and how you can try it out. Skoot over to Cisco.com now and get your copy",
    
  },
  {    
    "id" : "http-0-0-0-0-4000-microsoft-is-retiring-the-technet-subscription-service-1372720740",
    "site" : "damianflynn.com",
    "title": "Technet is retiring",      
    "url": "http://0.0.0.0:4000/microsoft-is-retiring-the-technet-subscription-service/",                    
    "categories" : ["Announcements","IT Pro/DevOps"],
    "tags" : ["TechNet"],
    "authors" : ["Damian Flynn"],      
    "publishedDate" : "2013-07-01 23:19:00 +0000",
    "content" : "Wow, after many many years of Subscriptions, and in more recent times as a Free MVP benefit, this evening I received a mail from Microsoft advising that the Technet Subscription is to be retired. The following is an extract of the mail, outlining the reasons and what to expect: __ As IT trends and business dynamics have evolved, so has Microsoft’s set of offerings for IT professionals who are looking to learn, evaluate and deploy Microsoft technologies and services. In recent years, we have seen a usage shift from paid to free evaluation experiences and resources. As a result, Microsoft has decided to retire the TechNet Subscriptions service and will discontinue sales on August 31, 2013. Subscribers with active accounts may continue to access program benefits until their current subscription period concludes. We are committed to helping customers through this transition phase and will remain focused on providing IT professionals with free access to a broad set of TechNet assets that support the needs of IT professionals around the world. Improved Free Offerings for IT Professionals Include: TechNet Evaluation Center: Free evaluation software with no feature limits, available for 30-180 days. Includes rich evaluation resources and TechNet Virtual Labs, which enable you to evaluate software without the need to install bits locally. Microsoft Virtual Academy: Free online learning site, with over 200 expert-led technical training courses across more than 15 Microsoft technologies with more added weekly. TechNet Forums: Free online forums where IT professionals can ask technical questions and receive rapid responses from members of the community. Please note, MSDN Subscriptions provide a paid set of offerings that are also available for those who require access to evaluation software beyond what the above free offerings provide. Thank you for your understanding as we increase focus on growing and investing in our free offerings to better meet the needs of the IT professional community. - TechNet Subscription Team I guess MSDN is going to be a very important resource, especially as we now get ‘Credits’ for Azure services.",
    
  },
  {    
    "id" : "http-0-0-0-0-4000-unravelling-the-network-1370992800",
    "site" : "damianflynn.com",
    "title": "Unravelling the Network",      
    "url": "http://0.0.0.0:4000/unravelling-the-network/",                    
    "categories" : ["Cloud Strategy","IT Pro/DevOps","Monitoring & Management","Networking"],
    "tags" : ["MVP","White Paper / Case Study","Webinar","Live Event","Community","PowerShell","Networking","Software Defined Networking","Hyper-V","System Center","Virtual Machine Manager","Cloud","Windows Azure Pack","Windows Server 2012"],
    "authors" : ["Damian Flynn"],      
    "publishedDate" : "2013-06-11 23:20:00 +0000",
    "content" : "In co-operation with Savision, I have recently published a white paper over viewing the new networking features in Windows Server 2012, and how these are managed in SCVMM 2012 SP1 The synopsis of the paper is defined in a single sentence: “With the launch of Windows Server 2012 Hyper-V, Microsoft introduced to the world its solution for Software Defined Networks, enabling System Center Virtual Machine Manager 2012 SP1 as their management platform of choice” Supporting this paper, I am hosting two live Webinars on this topic, the first today, and again on Thursday; the presentation will also be recorded for later review. If you are starting into a SCVMM 2012 SP1 project, or curious about what Software Defined Networks is all about, then surf over to the Savision site, and download the paper, and while you are there, connect to one of the live presentations. http://www.savision.com/resources/news/unravelling-network-scvmm-2012-sp1-free-whitepaper-and-webinars Damian",
    
  },
  {    
    "id" : "http-0-0-0-0-4000-news-scu2013-speaker-1370101860",
    "site" : "damianflynn.com",
    "title": "System Centre Universe 2013 Europe",      
    "url": "http://0.0.0.0:4000/news-scu2013-speaker/",                    
    "categories" : ["Announcements","Events","IT Pro/DevOps","Monitoring & Management","Networking"],
    "tags" : ["MVP","Conference","Live Event","Community","Networking","Software Defined Networking","Hyper-V","PowerShell Deployment Toolkit","Microsoft Deployment Toolkit","System Center","Orchestrator","Service Management Automation","Operations Manager","Service Manager","Service Provider Framework","Virtual Machine Manager","Cloud","Windows Azure Pack","Windows Server 2012"],
    "authors" : ["Damian Flynn"],      
    "publishedDate" : "2013-06-01 15:51:00 +0000",
    "content" : "Hot of the press, I am delighted to be join the panel of speakers to present at System Centre Universe DACH, which is planned for September in Switzerland! More information as the time gets closer…",
    
  },
  {    
    "id" : "http-0-0-0-0-4000-powershell-deployment-toolkit-1366932060",
    "site" : "damianflynn.com",
    "title": "PowerShell Deployment Toolkit",      
    "url": "http://0.0.0.0:4000/powershell-deployment-toolkit/",                    
    "categories" : ["IT Pro/DevOps","Monitoring & Management"],
    "tags" : ["Community","PowerShell","Desired State","Active Directory","Hyper-V","PowerShell Deployment Toolkit","Microsoft Deployment Toolkit","System Center","Configuration Manager","Orchestrator","Service Management Automation","Operations Manager","Service Manager","Service Provider Framework","Virtual Machine Manager","Self Service","Cloud","Windows Azure Pack"],
    "authors" : ["Damian Flynn"],      
    "publishedDate" : "2013-04-25 23:21:00 +0000",
    "content" : "Over the last 8 or so weeks, if you have been following the Microsoft Blog “Building Clouds”, you would have being introduced to the genius that is “Rob Willis”. Rob has created a Powershell 3.0 workflow enabled script, which replaces the Junk System Center Unified Installer which was delivered as part of the 2012 suite. Now, I normally don’t take pleasure in basing the work for the great engineers within Microsoft, but in the case of the Unified Installer, there will have to be an exception. If you have any requirement to deliver a lab for working in System Center, the only real option was to grab a coffee, and lock away a couple of hours to click trough the installers, figure out what will go where, and finally get some connectors into place, before you could finally say “Bingo”. At which point the Snapshot feature of Hyper-V becomes your very best mate. The PowerShell Deployment Toolkit (PDT) changes this forever (or at least I hope it will), Rob has leveraged the power of PowerShell workflows (and if you have no idea what I’m speaking about – start researching and mark my words this is going to play a big part of our future). And leveraging just 2 XML files has a complete unattended solution for installing SC2012SP1. These files will go out, download EVERYTHING you will need, and then with a little help, create all the VMs for the Lab, before finally getting to grips with the payload and deploying System Center. Rob covers this process in detail on the Building Clouds blog – so go read that, and check out his MMS 2013 presentation if you need to see this work (and you do)! What I want to cover here, is just a simple trick – the 2 XML files I mentioned are pretty cool, but only one of these is really where we will spend our time, this is called Variable.xml **and the other has all the logic for checking dependencies for the components, unpacking, installing, integrating, etc.; it’s a thought read, but if you like a challenge – then crack open **Workflow.xml Configuration Roles One of the beauties of this solution is that I can use the PDT to customise how my lab should be deployed, all with a few simple lines of XML, and I do mean simple – this is an example extract from my current file Here, you can see that I am going to deploy 2 computers to my lab, one of which will be a Management console, onto which I will deploy all the different suite consoles, including the SQL management tools; and to the second server I will deploy SCVMM, but its database will be hosted on a shared SQL server with a dedicated Instance for SCVMM. One of my first challenges was to figure out what roles I can actually deploy with PDT, and the answer to that is locked in the XML of **Workflow.xml **so for this post, I want, to show you a simple trick to gather out this list of options, so that you can also get started on creating an awesome **Variables.xml **for your personal labs. PS&gt; $workflowxml = [XML] (Get-Content .Workflow.xml) PS&gt; $workflowxml.Installer.Roles.Role Select Component, Name This will then pump out to screen in the Name column all the Roles which you can define for your configuration Now go play…",
    
  },
  {    
    "id" : "http-0-0-0-0-4000-free-deep-dive-sessions-1366759320",
    "site" : "damianflynn.com",
    "title": "Free MVP Deep Dive Sessions on System Center",      
    "url": "http://0.0.0.0:4000/free-deep-dive-sessions/",                    
    "categories" : ["Announcements","Events","IT Pro/DevOps","Monitoring & Management","Networking"],
    "tags" : ["MVP","Webinar","Live Event","Community","Networking","Software Defined Networking","Hyper-V","PowerShell Deployment Toolkit","Microsoft Deployment Toolkit","System Center","Orchestrator","Service Management Automation","Operations Manager","Service Manager","Service Provider Framework","Virtual Machine Manager","ITIL","Self Service","Cloud","Windows Azure Pack","Windows Server 2012"],
    "authors" : ["Damian Flynn"],      
    "publishedDate" : "2013-04-23 23:22:00 +0000",
    "content" : "If you enjoy MMS and Tech-Ed, either physically or virtually then you are going to love this! In association with Microsoft a number of MVPs from the Windows Server / System Center disciplines have organised to run weekly technical presentations, illustrating some of the awesome features of the products, and solutions which you can deliver from the software suites. The content is going to be Level 300 and better, which translates to Geek Fest J Looking at the line-up of presenters and topics for the next 12 weeks, you are going to be in for treat, with some of the long established heavy hitters out in force, to swing open the lid and starting fiddling with the inner workings, just to make your life easier. The only requirement is a copy of Lync 2013 client (Download), and an hour of your time, at 4pm GMT once a week. And just for Bonus shots, we are adding an extra presentation this week on Thursday! The first session is tonight (23rd April 2013) at 4pm GMT Automating Hyper-V administration with PowerShell And on Thursday we have, at the same time of 4pm GTM Keeping your CIO Happy: Executive Score carding with SQL, SharePoint and Operations Manager! I will also be joining in the fun a little later with a deep dive on Network Virtualisation ‘Under the Hood with PowerShell’, but you have to wait a few weeks J So – CLICK HERE NOW and get the calendar appointments!",
    
  },
  {    
    "id" : "http-0-0-0-0-4000-windows-server-2012-hyper-v-installation-and-configuration-guide-1364229960",
    "site" : "damianflynn.com",
    "title": "Windows Server 2012 Hyper-V Installation and Configuration Guide",      
    "url": "http://0.0.0.0:4000/windows-server-2012-hyper-v-installation-and-configuration-guide/",                    
    "categories" : ["Announcements"],
    "tags" : ["Books","General","Hyper-V","Achivements"],
    "authors" : ["Damian Flynn"],      
    "publishedDate" : "2013-03-25 16:46:00 +0000",
    "content" : "I am still getting it a little hard to believe, but we done it again, and have published book 2 The original team are somewhat back, however this time Hans opted for the role of technical editor rather than author (good call Mate). We also looped in from Microsoft a great mate, and extremely talented Michel Luescher Over the months, we adopted to a new name for the book, and we are hoping after you read this book, you will understand and agree – Simply put, we call this the Hyper-V 2012 Bible! Head over to Amazon right now and order a copy, and if you have a copy, head over anyway, and leave your honest feedback! Please!",
    
  },
  {    
    "id" : "http-0-0-0-0-4000-nexus-1000v-online-lab-1362698520",
    "site" : "damianflynn.com",
    "title": "Nexus 1000v Online Lab",      
    "url": "http://0.0.0.0:4000/nexus-1000v-online-lab/",                    
    "categories" : ["Announcements","Cloud Strategy","IT Pro/DevOps","Virtual Machines"],
    "tags" : ["Community","Networking","Software Defined Networking","System Center","Virtual Machine Manager","Cloud","Windows Azure Pack","Windows Server 2012"],
    "authors" : ["Damian Flynn"],      
    "publishedDate" : "2013-03-07 23:22:00 +0000",
    "content" : "Yesterday, was an all-important day with the Public Beta launch of the Nexus 1000v for Hyper-V. If you missed the Live Presentation, the recording and deck are now available on the Cisco Nexus Community website. https://communities.cisco.com/community/technology/datacenter/nexus1000v Delivering the first truly distributed switch for the Hyper-V platform, a lot of new concepts and possibilities are truly now a reality. Join the Beta I can only urge you to follow the steps in the slide below, and join the Beta program Then within a few hours you will be greeted with a message similar to the following, directing you to the Beta site, and an active community Online Lab Alternatively, if you are resource locked, I can suggest you take a look at the online labs hosted publically. This is not specifically the newest Nexus 1000v, and you will not get to see the hypervisor or management consoles, but will give you access to see how the Nexus 1000v seamlessly integrates with other networking hardware and devices, from the network engineers perspective. Check out this free resource (Contributions accepted / No affiliation) at http://www.sharontools.com/online-lab/ Cheers Damian",
    
  },
  {    
    "id" : "http-0-0-0-0-4000-vmm-stop-the-refreshers-to-remove-locked-systems-1362612180",
    "site" : "damianflynn.com",
    "title": "VMM – Stop the Refreshers to remove locked systems",      
    "url": "http://0.0.0.0:4000/vmm-stop-the-refreshers-to-remove-locked-systems/",                    
    "categories" : ["IT Pro/DevOps","Monitoring & Management","Supportability","Virtual Machines"],
    "tags" : ["Hyper-V","System Center","Virtual Machine Manager","SQL Server","Self Service","Cloud","Windows Azure Pack","Windows Server 2012"],
    "authors" : ["Damian Flynn"],      
    "publishedDate" : "2013-03-06 23:23:00 +0000",
    "content" : "_[alert type=”danger” close=”no”] The following post, is “At your own risk”, and only to be used when you can find no other option….[/alert] _ In the odd scenario, when VMM just keeps getting in its own way, and simple jobs keep failing, due to jobs you cannot see, trapping you, sometimes need to call in the heavy guns and stop VMM from hurting itself. The Refresher model has being around since Day 1, and if you have used VMM for any amount of time, it will be nothing new to you, so when jobs need to just stop, you need to flick the switch, and turn them off for a little time, while you get some time to fix the root problems. How? Start with the registry and on your VMM server, navigate to the following location HKLM\\Software\\Microsoft\\Microsoft System Center\\Virtual Machine Manager\\Server\\Settings Under this location, you may or may not see the following Keys – these all affect the Refreshers in VMM, which you are going to turn off J. Note if you have these already defined, note the current values; as one you fix this problem, you do need to turn back on the refreshers, otherwise VMM will have no idea what is happening on the hosts – no a thing you would really want… **VMUpdateInterval ** * VM Refresher, Defaults to 30 Minutes **VMPerformanceUpdateInterval ** **VMPropertiesUpdateInterval ** * VM light refresher (subset of properties from the VM Refreshers), Defaults to 2 Minutes **SQMUpdateInterval ** **HostUpdateInterval ** * Host and User Role Refresher, Defaults to 30 Minutes If you set the value to any one of these to 0, this has the effect of switching off the refresher. Now, once you have these all turned off, you are going to need to restart the VMM Service, so that it reads its new rules, and effectively stands down. At this time you should be able to move forward with no more locking jobs active. Remember, to go back and remove these Keys again, or reset them to their original values once you are done, and then restart the VMM services to re-activate the service. Good Luck, and remember that this is MVP Sharing, and not CSS supporting ;) Damian",
    
  },
  {    
    "id" : "http-0-0-0-0-4000-cisco-nexus-1000v-overview-1362525840",
    "site" : "damianflynn.com",
    "title": "Cisco Nexus 1000v Overview",      
    "url": "http://0.0.0.0:4000/cisco-nexus-1000v-overview/",                    
    "categories" : ["Announcements","Cloud Strategy","IT Pro/DevOps","Virtual Machines"],
    "tags" : ["Networking","Software Defined Networking","System Center","Virtual Machine Manager","Cloud","Windows Azure Pack","Windows Server 2012"],
    "authors" : ["Damian Flynn"],      
    "publishedDate" : "2013-03-05 23:24:00 +0000",
    "content" : "The Cisco Nexus 1000V is a Layer 2 distributed virtual switch, originally developed and implemented in 2009 for VMware vSphere 4.0. The Nexus 1000v is crafted from the same operating system which Cisco utilizes for their physical Nexus switches, Cisco NX-OS. Additionally, the Nexus 1000v is fully standards compliant, ensuring that you do not require to have physical Nexus switches to bridge the physical and virtual worlds, permitting you to utilize your current infrastructure. Environment which are already Nexus enabled will benefit from a single unified experience spanning both the physical and virtual ecosystems, with the Nexus 1000v seamless integrating into your current network management solutions, from Cisco and 3rd Parties, e.g. SolarWinds. Structure The Nexus 1000v is composed of two components, mapping from the modular physical enterprise world, to the Virtual world. **Virtual Supervisor Module (VSM) ** * The VSM are essentially implemented as a Virtual Machine running on your chose Hypervisor (Hyper-V of course), or a physical appliance such as the Nexus 1100 * The VSM is analogous to the Supervisor which you find in normal enterprise class modular switches. * VSMs are normally deployed in pairs, similar to their physical counterparts and are responsible for all the switch's logic actions – the CPU of the environment in an Active/Passive configuration, hosting the NX-OS environment. **Virtual Ethernet Module (VEM) ** * The VEM is essentially an Extensible Switch Filter deployed to the extensible switch on each Hyper-V host * The VEM is analogues to the Line cards found in the modular switch, which host the Ethernet ports to which we connect our systems, in this case Virtual Network Interfaces of the VM. * A VSM supports up to 64 VEMs in a distributed logical switch model, with each VEM represented in NX-OS of the VSM as line cards (with the port description automatically representing the VM Name) Deployment Deploying the Nexus 1000v is completed tough to use of a simple installation wizard, which generates 2 Virtual Machines and attaches an installation ISO to deploy the Supervisor operating systems (NX-OS) and their heartbeat for High availability. The Extensible switch modules are then deployed to the relevant hosts, and create a tunnel back to the VSM for configuration; these modules can be manually deployed, or through the use of SCVMM 2012 SP1 can be automatically distributed and enabled. Connecting the Nexus 1000V virtual network with the physical upstream network, is achieved by utilizing the host network adaptors connected to the extensible switch. These can be bonded to create large trunks of 6 active ports, and 6 passive ports per host. These ports can also be distributed across multiple physical switches for high availability. The Nexus 1000v is an enterprise class, proof of concept ready solution, which strategically redefines the network for cloud. Can you afford to miss out… Regardless of your environments side, you need to consider the Nexus 1000v as core network component for Virtualization/Cloud project. Its proven enterprise history, Cisco’s dedication to NX-OS with over 2000 active engineers, and its unbeatable production costs. The Nexus 1000v will be available in 2 Editions, the following table illustrates the cost and feature comparison Investigate this platform now, learn about vPath and the Cisco virtual services vision; treat your Hypervisor, treat your business. Learn more - https://communities.cisco.com/community/technology/datacenter/nexus1000v Happy Networking Damian",
    
  },
  {    
    "id" : "http-0-0-0-0-4000-cisco-nexus-1000v-public-beta-program-1362180240",
    "site" : "damianflynn.com",
    "title": "Cisco Nexus 1000v Public Beta Program",      
    "url": "http://0.0.0.0:4000/cisco-nexus-1000v-public-beta-program/",                    
    "categories" : ["Announcements","Cloud Strategy","Events","IT Pro/DevOps","Updates","Virtual Machines"],
    "tags" : ["Webinar","Live Event","Community","Networking","Software Defined Networking","System Center","Virtual Machine Manager","Cloud","Windows Azure Pack","Windows Server 2012"],
    "authors" : ["Damian Flynn"],      
    "publishedDate" : "2013-03-01 23:24:00 +0000",
    "content" : "Brace yourself, the fun is about to truly begin. On Wednesday March 6th, Cisco will unveil its greatly anticipated Distributed Switch solution for Windows Server 2012 and Hyper-V; the Nexus 1000V. What is the Nexus 1000V? Cisco Nexus® 1000V Series Switches provide a comprehensive and extensible architectural platform for virtual machine (VM) and cloud networking. The switches are designed to accelerate server virtualization and multitenant cloud deployments in a secure and operationally transparent manner. Integrated into the Windows Server 2012 Extensible Switch, and fully compatible with System Center Virtual Machine Manager 2012 SP1, the Cisco® Nexus 1000V Series provides: Advanced virtual machine networking based on Cisco NX-OS operating system and IEEE 802.1Q switching technology Cisco vPath technology for efficient and optimized integration of virtual network services Tight integration with System Center Virtual Machine Manager 2012 SP1 Layer 2 Switching with Transmit side Rate Limiting Security Policy Mobility, inbuilt support for Private VLANs with local PVLAN Enforcement Provisioning Port Profiles with deep Integration with SCVMM Traffic Visibility, including VM Migration Tracking, NetFlow v.9 with NDE, Cisco Discovery Protocol v.2 And so much more These capabilities help ensure that the virtual machine is a basic building block of the data center, with full switching capabilities and a variety of Layer 4 through 7 services in both dedicated and multitenant cloud environments. Learn More Join Cisco Gunnar Anderson (Cisco), Appaji Malla (Cisco) and myself (Damian Flynn) on Wednesday March 6th at 9am PST as we present the Nexus 1000V, Overview some of the amazing features which this solution enables, and most importantly share the details on how you can start working with the Beta Release right away. Follow this link to register for the launch, and we will look forward to a great online event, and an opportunity to show you the future, and answer your questions. Share. Please click the buttons below, to tweet, share and re-blog what is clearly one of the most important events in the Microsoft Private Cloud strategy so far in 2013. See you Wednesday Damian Learn more @ https://communities.cisco.com/community/technology/datacenter/nexus1000v/",
    
  },
  {    
    "id" : "http-0-0-0-0-4000-im-back-1362093900",
    "site" : "damianflynn.com",
    "title": "I’m Back!",      
    "url": "http://0.0.0.0:4000/im-back/",                    
    "categories" : ["Announcements","Updates"],
    "tags" : ["MVP","Community","Networking","Active Directory","System Center","Orchestrator","Operations Manager","Virtual Machine Manager","Cloud","Windows Azure Pack"],
    "authors" : ["Damian Flynn"],      
    "publishedDate" : "2013-02-28 23:25:00 +0000",
    "content" : "Good gosh, looks like I have been ignoring my own blog, which is just wrong in so many ways. I have not being slacking, that I promise; the last 2 months have being a complete nightmare which oh so much happening. But it is all good. The year started off on a high again as I was renewed for the 3rd year in a row as a MVP – so first and foremost, thank you to all that have helped make this real again both online and offline. Once again the focus of choice is Cloud and Datacenter Management, which translates to Windows Server / System Center, or the Cool Stuff… The beginning of January was swallowed up with the final edits for the second book, which from the drafts I had the opportunity to read from my fellow authors, namely Aidan, Patrick and Michel, this will be a bible you are going to want to have on your desk at all times; some of the details and reference trapped inside the 550 page is amazing; I even found myself looking up some details from Michel’s chapter yesterday when debating with my colleague whether he should not leave the Hyper-V host reserve alone – Awesome (And no – I don’t have copies yet either, just the private drafts). This one is on the shelves in March, so be sure to check it out. While all this work was happening, we also kicked off the year we the amazing VIRTy Alley event, digging deep into the depths of VMWare and Hyper-V. If you missed this event, don’t panic – it is still available online, so you can grab the slides and watch the presentations - and please no laughing. The information is at this URL: http://www.vkernel.com/virtu-alley/presentations. My topic for the event is the new Software Defined Networks feature in Windows 2012; which I also cover in even more detail in the book! Then we have Orchestrator, this is my System Centre Blood at the moment for my Day Job, as we get close to wrapping up a new SaaS solution, and a major project to move ID Management to SAP. Both of these projects are leveraging the Microsoft Forefront Identity Management 2010 R2, Synchronisation service to keep our AD and Line of Business applications in sync; and with the power of Orchestrator we manage, control and log all the steps hundreds of times a day. Cool stuff! I did need to go under the hood and published a new Integration Management Pack for FIM out on Codeplex (http://scointegrationpacks.codeplex.com/releases ) just in case you might need to do the same someday (there is an update in my local repository ready for publishing – ill sort that out on Monday, Promise! Which has 1 new activity). I also had the opportunity to contribute the feature article on the just release TechNet UK February 2013 newsletter, with some pointers on Orchestrator SP1, and the community Integration packs… Then it was off to Boston for a week, before heading on to the as always brilliant MVP Summit; getting the opportunity to meet so many Guru’s and heckle the Microsoft Product groups. Before finally returning back to the currently “Smelly” Irish countryside. If you are by now tiered reading this, then I’ll just sneak in a plug to the blog postings which I have being co-authoring with Nigel Cain from Microsoft, and the VMM team, covering all that is new and confusing in the world of Network Virtualization; these are LONG and detailed posts, with many more to come, but Highly recommended reading if you are considering deploying VMM 2012 SP1 to production. This will link you to the latest post we published - http://blogs.technet.com/b/scvmm/archive/2013/02/14/networking-in-vmm-2012-sp1-logical-networks-part-i.aspx What’s next? Next week, Cisco will present to the world the new Nexus 1000v Virtual Switch for Windows 2012, and VMM; I am joining the Launch party guess presenters list on March 6th so some slides to wrap up for that. Then its MMS, together with Nigel, we will take to the podium to chat about our SP1 experiences, and try to share some tricks, trips and traps! I have a surprise poster to share with you also, but it’s not ready just yet, it is still a work in progress, but as a tease, here is a sneak peek… See you soon Damian",
    
  },
  {    
    "id" : "http-0-0-0-0-4000-mvp-2013-system-center-cloud-and-datacenter-1357054440",
    "site" : "damianflynn.com",
    "title": "MVP 2013: System Center Cloud and Datacenter",      
    "url": "http://0.0.0.0:4000/mvp-2013-system-center-cloud-and-datacenter/",                    
    "categories" : ["Announcements"],
    "tags" : ["Achivements","MVP","Community"],
    "authors" : ["Damian Flynn"],      
    "publishedDate" : "2013-01-01 15:34:00 +0000",
    "content" : "After 2 amazing years, meeting and working with some amazing people, completing my first book ever (I am still trying to get over that one). New Year’s day has become interesting as we cook and monitor the email box, in the anticipation that lighting might just strike again. And, I am so excited to say it did! Thank you everyone! Dear Damian Flynn, Congratulations! We are pleased to present you with the 2013 Microsoft® MVP Award! This award is given to exceptional technical community leaders who actively share their high quality, real world expertise with others. We appreciate your outstanding contributions in System Center Cloud and Datacenter technical communities during the past year.",
    
  },
  {    
    "id" : "http-0-0-0-0-4000-exchange-health-1294873680",
    "site" : "damianflynn.com",
    "title": "Exchange Health",      
    "url": "http://0.0.0.0:4000/exchange-health/",                    
    "categories" : ["Business Intelligence","IT Pro/DevOps","Monitoring & Management","Messaging Platforms","Supportability"],
    "tags" : ["PowerShell","Dashboards","KPI","Exchange","ACTION GIST"],
    "authors" : ["Damian Flynn"],      
    "publishedDate" : "2011-01-12 23:08:00 +0000",
    "content" : "Part of the daily tasks on keeping the Core systems up and humming, is to keep a close on on the exchange environment. 2 of the metrics which I use as an early warning indicator is the length of the mail queues, and on the Exchange 2010 Servers is the Copy status of the DAG stores. To simplify this process I have created a simple PS function called Get-ExchangeHealth, with these 2 simply commands This is just added to my profile of commands, so that its available when required.",
    
  },
  {    
    "id" : "http-0-0-0-0-4000-microsoft-mvp-vm-2011-1294094760",
    "site" : "damianflynn.com",
    "title": "Microsoft MVP VM 2011!",      
    "url": "http://0.0.0.0:4000/microsoft-mvp-vm-2011/",                    
    "categories" : ["Announcements"],
    "tags" : ["Achivements","MVP","Community"],
    "authors" : ["Damian Flynn"],      
    "publishedDate" : "2011-01-03 22:46:00 +0000",
    "content" : "Holy Singing Batman, I was delighted to find out late last year that i was been Nominated as a candidate for a Microsoft Most Valuable Professional award, but did not really expect my work to be good enough for this recognition. But Saturday January 1st arrived and among all the New Year wishes there was a very special email in my inbox Dear Damain Flynn, Congratulations! We are pleased to present you with the 2011 Microsoft® MVP Award! This award is given to exceptional technical community leaders who actively share their high quality, real world expertise with others. We appreciate your outstanding contributions in Virtual Machine technical communities during the past year.",
    
  },
  {    
    "id" : "http-0-0-0-0-4000-automation-engine-decisions-1292453160",
    "site" : "damianflynn.com",
    "title": "Automation Engine… Decisions!",      
    "url": "http://0.0.0.0:4000/automation-engine-decisions/",                    
    "categories" : ["Developer","Smart Home/Buildings"],
    "tags" : ["Lighting","Live TV / PVR","Audio Video","Security","HVAC","Presence","Command and Control"],
    "authors" : ["Damian Flynn"],      
    "publishedDate" : "2010-12-15 22:46:00 +0000",
    "content" : "For the last few years I have been building an automation solution for my home, and like all good solutions I needed a good engine to power all the workflows, logic decisions and of course the user interfaces which would be presented around my home. After spending may hours reviewing the numerous options available at the time I finally selected to use the CQC Automation Engine to take this role. Over the years I followed the progress of this product trough to its V1.0 release, right trough to its V3.1 version. I learned a lot from the fantastic community which generated a lot of friends, contacts and support in the dark nights which stuff just refused to work. Some of the characters over the years departed to different technologies, while others invested and shared vast amounts of time and energy extended the support for the product. During the course of my time, I always struggled to get my mind around the bespoke programming concepts and language which was exposed for our usage; and as I deployed more technologies this challenge continued to grow to blocking point. Finally, a number of changes in the Licencing and Support terms for the product kicked me to investigate alternative options. The idea of having to start over researching engines was daunting; at the same time I was offered a option to certify as a “Control 4” developer, however previous experience on that platform had left a bad taste, and thus I decided to move on with my quest. Elve I had the pleasure of locating an ex-CQC user, whom had aborted the platform about 3 years earlier, who decided to try and hit the issue head on and code his own engine, which at the time was called the J9 engine. This has since being converted to a professional solution and is marketed as “Elve” by CodeCore Technologies. One of the biggest attractions to the solution, is that Johnny and his team, ported the best functions of the CQC platform, and built from the ground up a .NET solution, the SDK is very familiar and support has been simply fantastic. However, as a pre 1.0 version of the product, the SDK is progressing rapidly, and the number of supported devices and appliances is still quite limited. However, the team are acutely aware of this, and encourage any potential developers to create and contribute drivers; in return they are offering a free Professional Licence (Worth $800) and assistance in creating the driver. ### My first Driver After a few weeks starting and stopping, trying some different approaches and getting acquainted to the new framework, I decided it was time to create a driver for my whole house audio processor, the “Xantech MRC88” 16 Zone system. The first step was a study of the protocol document for controlling the device; then reviewing some of the sample drivers included in the SDK the time was right to start my development attempts. After a few hours, some emails and a lot of debugging, my first driver’s basic functionality was up and running. The best news however was a few weeks later, I had fully completed the driver, and the icing on the cake was to learn that my driver would be included in the 1.0 version of the products release. Yahoo! Next A few weeks into using the product, the conversion from CQC to ELVE has been totally painless, although I do admit that not having all the drivers I was accustomed to available to me a little limiting. However, the structure of the product, along with its SDK have facilitated for a very customisable environment. Development has been very enjoyable, and I already have my eyes set on some more drivers, long live the Elve’s.",
    
  },
  {    
    "id" : "http-0-0-0-0-4000-deploying-vmmssp-2-0-to-our-lab-1287528000",
    "site" : "damianflynn.com",
    "title": "Deploying VMMSSP 2.0 To Our Lab",      
    "url": "http://0.0.0.0:4000/deploying-vmmssp-2-0-to-our-lab/",                    
    "categories" : ["Cloud Strategy","IT Pro/DevOps","Virtual Machines"],
    "tags" : ["Hyper-V","System Center","Virtual Machine Manager","Dashboards","Self Service","Cloud"],
    "authors" : ["Damian Flynn"],      
    "publishedDate" : "2010-10-19 22:40:00 +0000",
    "content" : "System Centre - Self Service Portal V2 As part of the Solution Accelerator program Microsoft has released an extension to the System Centre Virtual Machine Manager 2007 R2 application, which extends greatly the functions and features of the original Self Service portal which shipped as part of the original application suite. Currently know as VMMSSP2 (that’s Virtual Machine Manager, Self Service Portal version 2) this toolkit runs along side an existing SCVMM deployment, and by using the power of the SCVMM Command Shell, integrates as a new extension without breaking or destroying your existing investment in SCVMM and any its customisation. Lab Setup Guest Virtual Machines There will be one new virtual machine being introduced into the same lab I set up for the purpose of learning, which we disused recently as we installed SCVMM and SQL Server. All existing systems in that lab will remain in place as we extend the environment with these new technologies. We will introduce one new virtual machine, which will be hosted as a 2008 R2 Enterprise (Standard can be used) x64 (x64 required) Member Server’s. This server will contain the VMMSSP 2.0 Installation Assumptions You have a domain that contains at least one Server 2003 SP2 Domain Controller (DC) You have configured the IP settings accordingly for all servers to be on the same subnet. I have provided the IP scheme of my lab below, but this will vary depending on your needs and Virtualization Software configuration. You have already installed a SQL server to host the VMMSSP Databases. VMMSSP does support local SQL installations, but I rather using a real SQL server so to mimic what I would do in a Production environment. I have posted a guide on how to do this here Computer Details The Following table outlines the detail **SCVMM 2007 R2 Server** **Name** DIGI-SCVMM2-01 **IPv4** 172.16.1.50 **IPv6** **Processors** 2 **RAM** 1.5Gb **Disk** 1 – 40Gb (OS and Application) **OS** Windows 2008 R2 **Applications** MS SCVMM 2007 R2 Server MS SCVMM 2007 R2 Console MS SCVMM 2007 R2 Portal VMMSSP Preparation Before we got into the fun stuff, I am going to make an assumption here that you have already a SCVMM 2007 R2 environment already installed and running (and If not, get going!). The install process will be in 2 stages, First we will install the Pre-Required software and modules, and once complete we will move on to the actual installation of the new VMMSSP2 toolkit. Once all this is done, we will take a look back and start the first use configuration steps. Prepare Installation As always, we are going to prepare some of the build tools before we start the process, The Core OS, just needs some roles and features enabled for SCVMM Server to function away happily. We can do the following process from Powershell or from the GUI, the call is totally yours on which method you are most comfortable with. Out of habit I generally like to do things once or twice in the GUI just to get a feel for the workflow, but it does not take long for my curiosity to grow about what is actually happening on my servers, and thus I will then try the process again from the shell, and enable verbose outputs just to keep me busy reading while the server works away. Roles and Features The following software needs to be installed on the server so as to complete the base system installation. Using the server manager, or the command shell, lets get the following list of modules installed on the server. .NET Framework Message Queuing Message Queuing Services Message Queuing Server Directory Services Integration IIS Web Server Common Feature Static Content Default Document HTTP Redirection Application Development ASP.NET .NET Extensibility ISAPI Extensions ISAPI Filters Security Basic Authentication Windows Authentication Digest Authentication Performance Static Content Compression Dynamic Content Compression IIS 6 Management Compatibility IIS Management Console Management Services IIS 6 Management Compatibility IIS 6 Metabase Compatibility IIS 6 Management Console SQL Server You know already that in my lab, we will be using my Shared SQL server - DIGI-SQL02 for the database. VMM 2007 R2 Admin Console Ok, the last of the pre-required software modules needed, is the System Centre Virtual Machine Manager 2007 R2 Administrator Console. Installing this will ensure that the correct Powershell modules are available on the server for the VMMSSPv2 services to communicate with our VMM2007R2 environment. Once, installed, give the node a quick reboot, just to be sure everything has set in correctly, and we will get started on the installation of the new tool kit. I’ll not go trough the steps as we have covered that in the posting for installing VMM 2007 R2 earlier. Service Accounts Excellent, that was not to painful right. Next we will organize some service accounts for use with our SQL installation. This should save us a little time later waiting for replication as an example to be completed. **Service Name** **AD Account** **Password** VMMSSP2 Service Agent DigiNerve!Digi.svc.VMMSSP2 ******* VMM SSP 2.0 Deployment And finally, we are ready to begin. Just one final stop before we kick off the process, lets take a moment to visualize how this Portal integrates with the rest of the Lab. Architecture Brilliantly simple, the new toolkit utilizes A Web Site to provide its interface for your users, Both Admins and End Users A Windows Service which builds uses Message Queuing and Windows Workflows, to interface with the SCVMM Command Shell. And a dedicated SQL repository to retain its information. The VMMSSP2 Toolkit for System Centre Setup Wizard installs both of the toolkit components. You can install all of the components at once, or you can choose to install either the Web portal component or the server component. The database component installs automatically when you install the server component of the toolkit. When you install the Web portal component, it connects to that database. Important: You must have administrator permissions on the computers on which you intend to install the toolkit components. You also must be a member of the local Administrators group on the computer running SQL Server. Before you run the setup wizard, ensure that you have the following information available for the Web Portal Component installation - as an example: Web site name for URL Selfhost.domain.com Port Number 80 Application pool name VMMSSP2 Application pool identity DOMAINVMMSSP2-Service For the Server component installation we can use : Windows Configuration Foundation (WCF) - Port number for HTTP endpoint 8080 Windows Configuration Foundation (WCF) - Port number for TCP endpoint 8000 Service Account DOMAINVMMSSP2-Service User names of administrators (for the VMMSSP2 Admins role) DOMAINUsername Finally for the Database settings: SQL Server Name Digi-SQL01 SQL Server instance name DefaultInstance Database name VMMSSP Authentication information DOMAINUsername Installing VMMSSP Toolkit. Before we start the installation process, we need to Add the service account as a local Admin on the node, for example add DOMAINVMMSSP2-Service to the Local Administrators group in server manager Once your ready, we can launch the installation wizard and get going. After a few seconds we will be greeted with the Welcome page, simply Click on Install Review and accept the license agreement, and then click Next. Click Toolkit server component, click Toolkit Web portal component, and then click Next. On the Check Prerequisites page, wait for the wizard to complete the prerequisite checks, and then review the results. If any of the prerequisites are missing, follow the instructions provided. When all of the prerequisites are present, click Next. Accept or change the file location, and then click Next. Enter the database configuration information. At the bottom of the page, make sure that Create a new database named is selected (the database name is not editable in the current release of the VMMSSP). When you finish the database configuration, click Next. Type the user name, password, and domain of an account for the VMMSSP service to use. Make sure that this account has the Log on as a service right. Click Test account to make sure that this account functions. When finished, click Next. Enter the settings that the Web portal component uses to communicate with the server component. These settings include the name of the computer that runs the server component, and the port numbers of two Windows Communication Foundation (WCF) endpoints. When finished, click Next. In the Data Center administrators box, type the user names of the accounts that you want to be able to administer the toolkit. In the Web portal, these users will be members of the DCIT Admins role and will have full administrative permissions to the toolkit. Type the name of the Data Center that the toolkit will interact with. When finished, click Next. Enter the IIS configuration information for the Web portal component, and then click Next. Important Remember to use the toolkit service account as the application pool identity. On the Install the components page, review the settings that you selected, and then click Install. When the installation finishes, click Close. Now, the installation will continue as the server is configured for us. Finally, after a few minutes of work, we will get the final result page. We can Click on Close to finish the process. Just for sanity, Check the Service account is running, and if not start it. Next check that the Pool in IIS is also running Magic - your are all set, now we can test that the installation worked. Open the web browser and type in the server name we just installed on (or use localhost if the browser is opened on same server), after a few seconds we should be greeted with a screen similar to this If this is what you have the pleasure of viewing, then Congratulations, you have VMMSSP2 installed and ready for configuration.",
    
  },
  {    
    "id" : "http-0-0-0-0-4000-sagetv-and-dvblogic-live-tv-1286750280",
    "site" : "damianflynn.com",
    "title": "SageTV and DVBLogic: Live TV!",      
    "url": "http://0.0.0.0:4000/sagetv-and-dvblogic-live-tv/",                    
    "categories" : ["Developer","Smart Home/Buildings"],
    "tags" : ["Live TV / PVR / IPTV","Audio Video"],
    "authors" : ["Damian Flynn"],      
    "publishedDate" : "2010-10-10 22:38:00 +0000",
    "content" : "2010Over the past 3 postings, we introduced the concept of the solution we were going to deploy. Then carefully we introduced, installed and finally configured the DVBLogic applications, before finally testing that these are all working correctly. We followed up in the previous article, with a step by step on installing SageTV, running trough the configuration wizard, and installing a plugin to make our Live TV experience even richer. In this Article we are going to focus on binding these two suites together, so to get our Live TV service working, powered by the DVBLogic applications. Currently,The process is still to complicated, however with the correct encouragement the teams in Frey and DVBLogic will continue to work together and make this process so much simpler for us. Let’s recap on the tools we listed in the first post, and address the parts which are still remaining. DVBLogic TV Source 3.1 VLC SageTV Server DigiGuide (Or other Suitable XMLTV source for your geographic location) DG2XMLTV Converter Stephanes XMLTV Import Tool for Sage Damian’s DVBlogic2SageTV utility EPG Services Ok, the first of the solutions which is not ‘Optimal’ is the supply of EPG data for our program guide. DVBLogic has some nice features which we did not pay attention to, which allows us to provide or gather EPG information from our sources, and broadcast this information over our virtual Satellites to the Virtual DVB-S2 cards, however currently this is not working 100% for the SageTV setup, and needs some more attention. For this reason I am going to provide the data directly to SageTV for now. DigiGuide In the Ireland and the UK there are a few options to collect EPG sources from, but many of these are not optimal. I have have pretty good success with Microsoft’s Guide for Mediacenter, however there is some question on the licenses around using this data for non-media centre purposes. If you are interested, you can use the tool MC2XML which will gather the guide data from Microsoft’s Servers and generate an XMLTV file ready for processing in the next step. However, I have found a very nice tool called DigiGuide which provides a good 14 day supply of Guide data. But, after a native install this data is not available in XMLTV format, therefore we need to apply a few simple steps which will permit us to generate the XMLTV file we require. Install DigiGuide First off, we download and install DigiGuide from www.digiguide.com. Start off the installation and we will be instantly requested for the location to install the tool to, we can accept the default, and click next. Before you know it, the tool is installed and ready for configuration The main screen is automatically presented as soon as installation is complete, and we can proceed with the Configuration Wizard. The first question is as simple as selecting the Country for which you will be subscribing to TV Channels. After this, we are asked to identify the provider / package for TV we are receiving. This will help DigiGuide to present all the relevant channel available in our region. Then we can select the Categories which we normally watch, or subscribe to. We are then offered the option of assigning preferences on the different channels categories we watch. As we are not going to be actually using the DigiGuide application for normal reference of the guide, but rather using it as source for our XMLTV export, this selection presents no value for us. That’s should be it, We can finish off the wizard, and give DigiGuide a little time to work. Once everything is updated we will see the guide information, and we can progress with the XML support for DigiGuide ## DigiGuide to XML Now, We have the TV Guide, but so far this really has no value for us in its current format. We need to export the data which this tool is providing us, and convert it into a useful format. To achieve that, we need the data in XMLTV format. Make sure you have a copy of DG2XML Unpack this Archive, and Inside you will find DG2XML.EXE and a folder DG2XML.dgscheme.web. We will move DG2XML.dgscheme.web folder into the DigiGuide Server Skins folder .DigiGuide TV Guideschemesserver skins We will make a new Folder for the DG2XML.exe utility, calling it simply DG2XML And finally, we copy the **DG2XML.exe **utility in this folder Next, lets start up DigiGuide, and configure the Web Server. In DigiGuide we select the Menu Tools -&gt; Customize -&gt; Web Service -&gt; Enable Web Service We should set the password to password That wraps up our current work in DigiGuide, you can close the Customise dialogue, and minimize the application out of the way. Now, we should be able to test that the XML Conversion tools are working, so lets pop open a command prompt and navigate to the DG2XML installation folder. Once there we will run the DG2XML utility and if it works correctly we should be presented with a new epgdata.xml file, generated from the **DigiGuide **information. (You could also just double click on _DG2XML.exe _while in explorer, and the tool should execute) In the DG2XML folder you should see a new file created called _epgdata.xml _which is our new XMLTV file. DVBLogic To SageTV We are making great progress, and have almost all the requirements ready to get these applications communicating with each other. The next step in the quest is to create the Frequency files which SageTV requires, so that it can Tune into the Channels we are broadcasting on our Virtual Tuners. First, you need to grab a copy of my utility DVBLogic2SageTV Extract the contents of this archive into your SageTV directory, so that the files are located in the .SageTVSageTV folder, and you should now have 2 new .EXE files in place, as per the screen shot below. **DVBLogic2SageTV_x64.exe **is to be used on x64 versions of windows **DVBLogic2SageTV_x86.exe **is to be used on normal 32-bit versions of windows You now, simply double click on the relevant version of this utility for your Windows Architecture. If you have problems generating these files, open the Command Window, and run the Utility again. This you you can view the status messages generated by the utility which should help us understand what the issue may be. Assuming everything has worked out correctly, the utility will now have created 4 new Frequency files for SageTV to use. Each file will have different content, depending on what channels if any DVBLogic is broadcasting on the Virtual DVB-S2 card. An example of the content looks as follows Excellent, another step closer to complete. SageTV We are now at the last stage of the configuration, during this stage we will Setup Video Sources in SageTV. XMLTV Importer If you have not already downloaded, then grab the SageTV XMLT Importer After Installing the tool, We need to process the XMLTV files to be SageTV ready. Launching the tool, we will be presented with the main interface Configure the XMLTV Grabber Now, we click on the top option XMLTV Files and Grabber to see what XMLTV sources we have configured. Since this is our first run we will just see one source called Default. We are going to remove this Default, and then click on Add a new XMLTV source. We will now be presented with the **XMLTV Files and Grabber **screen where we get to define more detail for the DigiGuide Feed. First we change the Source Name to read as DigiGuide. Next, Using the Browse button we locate the egpdata.xml file which was created when we tested that the DG2XML tool was working correctly. Once set the XMLTV File will be set to a setting similar to C:program filesdg2xmlepgdata.xml Finally we can tell the system to call the XML Grabber each time it runs, so to get the most recent information available in DigiGuide. to do this we set the Grabber to the folder we placed the DG2XML.exe file, this should resemble c:program filesdg2xmldg2xml.exe Now that we have provided the main configuration data, we can click next, where the tool will then go and read the XMLTV file to check that it is formatted correctly, and allow us make some fine tuning adjustments. We are presented with One random program found in the XMLTV File, where we have the ability to verify and check that everything is correct. If your time zone is different from the DigiGuide source, we can use the Time Offset setting to apply any modifications needed. Next we can move on to the the last section of the configuration. Here we are offered a process to help determine when programs are part of a series. Using the DigiGuide service, we will set the tool to use the option Based on episode title or program Description. We can now click OK to complete the edit process. returning back to the XMLTV Files and Grabber. Channel Line Ups This is the next most important part of the process. Now that we have our XMLTV guide in place, we need to teach the system how to match programs in the XMLTV file to the actually channels we are receiving on the DVB Enhancer. Launching into this section of the Wizard lets us customise the details for our TV Viewing. Click on the option Add a new lineup which will open the window Channel Lineups page in the window First we will start with assigning a Name **for the feed, in this case **DVBLink – Virtual Satellite 1 Lineup Since this stuff is very customised, I am not going to share this on the public servers, as it will just create a lot of rubbish for other, therefore I recommend clicking on Do not upload this lineup to the online database Next, we will set the Country, Region, Type and Source information. As I am based in the West Of Ireland my specifics will be Country – Ireland Region – West Type – Digital Source – Other Provider – None In the Frq file **drop down list, we should be presented with the four files created with the DVBLogic2SageTV tool. As we are creating the line up for _Virtual Satellite 1 _we should select **DVBLink Capture #1-0-DVB-S.frq file. Ready, lets click on **Next. ** On the next page, we will be presented with a list of the channels which were imported from the frequency file, including the channel numbers which we provided while configuring DVBLogic. Thanks to the import file, we don’t need to do a lot of work here, the main focus will be on the last column, XMLTV where you can select the guide data for the channel from the drop down list. This list is of course the data stored in the XMLTV guide we created from DigiGuide, and setup in this tool on the first tab. ID Unique name to identify the channel Name A unique name for the Channel, this name is provided from the information in the DVBLogic Virtual Satellite Call Sign _This is the Name which we will see in the EPG for the channel, I like to edit this to something nicer._ Number This is the **Logic (Guide) Channel Number** [For your TV Channel Number] and also **Physical (Tuning) Channel Number** Time Offset Some Channels may be focused for different time zones in Europe, so we can make adjustments here to bring these channels into the same time zone as the guide to get the EPG data to correspond correctly HD Provides us an opportunity to define that this channel broadcasts in Hi-Def mainly, or Only XMLTV Data This is a dropdown list of all the channels we have detected in the XMLTV file, with the channels name, and its guide Channel Number should we decided to use the channel same numbers We have Four Virtual Satellite Cards, so we need to repeat this process for the remaining 3 cards. Categories and Ratings The next Configuration we focus on will be the Categories. Categories This is used for improving the readability of the Program Guide when it is displayed. We click on Edit categories for reliable recordings and easily readable program guide Before we start the configuration, we have two different Categories lists which we will be matching up. On the Left we have the Categories which Sage utilizes for its guide, and on the right is the categories which we get in the DigiGuide XMLTV Feed. **SageTV EPG** **DigiGuide XMLTV** Serie Movie News Sport Recreation Music Educational Magazine Children Other Technology, Political, Gardening, Business and Finance, Education, Scientific Documentary, Motoring, Adult Entertainment, Reality Show, Quiz Show, Special Interest, Fashion, Nature, Kids Drama, Arts, Animation, Comedy, Chat Show, Travel, Entertainment, DIY, Cookery, Game Show, Religious, Health, Sitcom, Consumer, History Documentary, Drama, Talk Show, Soap, Documentary, Series, Science Fiction Series, Film, News, Sport, Music, Magazine Programme, and Children's Our objective is to Match the Categories on the Left to the Categories on the right. This process is of course subjective to your viewing preferences, but as a guide to assist, this is the breakdown I am currently utilizing **SageTV Guide** **Keyword Matcher** **Keyword Excluder** **DigiGuide Category** Serie Serie Series Science Fiction Series Movie movie film drama kids Film Drama News news scientific business political News Scientific Documentary Business and Finance Political Sport Sport Sport Recreation Sitcom soap entertainment comedy adult Soap Sitcom Entertainment Comedy Music Music Music Educational education technology gardening nature Cookery DIY Technology DIY Cookery Gardening Education Nature Magazine magazine documentary motoring health consumer scientific Magazine Programme Health Motoring Consumer History Documentary Documentary Children Children Kids Animation Childrens Kids Drama Animation General Talk Show Religious Game Show Travel Chat Show Arts Fashion Special Interest Quiz Show Adult Entertainment Talk Show Religious Game Show Travel Chat Show Arts Fashion Special Interest Quiz Show Adult Entertainment The result of this exercise will look a little similar to the results below. Ratings Again, each guide will support different rating grades deepening on the broadcaster or the countries standards. In order for SageTV to process these with some degree of meaning, we are offered the opportunity in the tool to view all the Ratings detected in the XMLTV data file, and match these to the pre-defined ratings which SageTV Understands Advanced Options We are offered the ability to tag programs in the XMLTV File with Star Ratings and HD flags if the data is available. Using this page you can customise some of the settings to match your personal requirements Update Guide Data Finally, we should have all the correct details in place, so we can proceed and Update the Guide Data into SageTV Click on Start Import, and the tool will connect to the DigiGuide Source, Update the XMLTV File, Map the Channels, add any custom groups, and settings to the programs on the channel, including categories, ratings etc, and finally update the SageTV system with the new information. SageTV Video Sources We can now start-up SageTV again, and use the Setup Wizard to add the new Virtual DVB-S2 cards. From SageTV main menu, press Setup, then Setup Video Sources. As we have so far skipped this step in SageTV, the first time we open the Setup Video Sources we will be directly presented the **Add Video Source **Screen. We will be repeating this process for each of the DVBLink Capture cards, so we will start with the 1st one, by selecting **DVBLink Capture #1 **and clicking enter. On the next screen, we will be presented with the different input types available on this device, In this case there is only one – Digital TV Tuner which you need to select. We will then be asked which Satellite we will be tuning. Since we are not tuning a real satellite, we are going to tell a white lie, and select any one of these. After we have selected one, we can click on Done which will move us forward to the next screen We are next asked to define what type tuner we are using. The decision is not critical here, but I normally select the first one, as its a Digital source. So, select User Tuner on Capture Card to tune Cable TV Next, we need to define where we are gathering the Guide Data from. I expect you can guess this is going to be the first one, XMLTV. Now, we need to determine where the Data maybe delivered from, Again sticking with the Digital option, I suggest we go with Cable or Satellite We are then asked for our ZIP code!, like Ireland here, excuse me, what the heck is a ZIP Code right. Well not to panic – we are not needing this, and instead will type 00000 as the ZIP code, which is SageTV speak for, use XMLTV dummy. Now, when you hit enter, we get presented with a list of the Line-ups we created in the XMLTV Import tool a little while earlier. Select the one from the list which matches the Virtual Satellite tuner you are configuring now, (remember we will be repeating this 4 times!) Now, we just need to wait a little, while SageTV works We next need to decide on the quality of the guide information, which in this case, is good, so I am selecting Extended Basic Services Next, we are dropped back into the Channel Setup, and instantly prompted to Scan For Channels, however we will be selecting No – I will scan for channels later as all the channels on this virtual satellite are now in place and ready for us! Great, we now should see the channels we included in the XMLTV Channel List a little earlier in the Channel Setup screen. We can now select the option Done with Channel Setup This will return is back to the Source Wizard Summary where can now see that the New DVBLink Capture #1 Digital TV Tuner : DVBLink – Virtual Satellite 1 Lineup is now assigned That is now us complete with the first tuner. You can repeat this process for the remaining 3 Virtual Tuners remaining, and the Channel Line-up’s you created. Live TV Ok, we are Done! Use the SageTV menu, and open the Program Guide where you can now see your channel list, and guide data. And, of course now is as good a time as any to see that you can select one of these channels and view them. For example, we can highlight NCIS LA on Sky2 and SageTV will ask what we would like to do The obvious choice is to Watch Now which if everything is good will show your show on screen Conclusion It was a fun journey, and this last stage sure could have some simpler steps on how to do the task, but taking all in, it is a major improvement on the previous methods, and so far – it has proven to be very stable Enjoy your trip, and I hope you have as much success as i have in pulling this together Finally, a Big thank you to SageTV team, DVBLogic Team, and Stephane for his XMLTV Importer -Damian.",
    
  },
  {    
    "id" : "http-0-0-0-0-4000-sagetv-and-dvblogic-sagetv-1286577360",
    "site" : "damianflynn.com",
    "title": "SageTV and DVBLogic: SageTV",      
    "url": "http://0.0.0.0:4000/sagetv-and-dvblogic-sagetv/",                    
    "categories" : ["Developer","Smart Home/Buildings"],
    "tags" : ["Live TV / PVR / IPTV","Audio Video"],
    "authors" : ["Damian Flynn"],      
    "publishedDate" : "2010-10-08 22:36:00 +0000",
    "content" : "We continue on the path to HTPC bliss, we now are going to focus on the heart of the solution, and walk trough the installation of the core technology. &gt; &gt; In this post, we will install Sage TV 7, and get us to the point where we can connect to our new Satellite Broadcasts! Installation With nothing to waste, we start off the SageTV_Setup.exe and before we know what is happening the Welcome splash is on the screen. I think you know what to do with the Next button, and after some tireless reading of really interesting license agreements, we can click on I accept the license agreement and then that nice **Next **button again. Now, we need to declare who we are for the registration, and decide who can use this install. In most cases this last option will not be very important, as we will be running the server headless, which means simply, that its just going to be running in the background, and we will use other computers, or the really sexy HD300 (or previous version if you have them) extenders. The next screen we get is just checking the folder where SageTV will be installed, and if we have any special IR or Capture cards to support. Nothing special here, so unless you have needs, just click on Next That is it for the questions, now we can get on with the installation. The process will not take very long… … and before you know it, we get the message that we are all complete. Once you click on the Finish button you will notice hiding in the background there is an installer for Java waiting for our attention. SageTV requires Java to be installed for it to function correctly, so we will work trough this wizard also. Start with a simple click on Accept to run the installation, and again within a few seconds… …Java is now installed and ready also. Configuration Now, on the desktop you will see an Icon on the screen for SageTV, so double click on the Icon. Each time SageTV starts it checks to see if we have a license, and if not, presents us with the option to provide one, or run in trial mode. You can Enter your registration key, or just Click on **OK. **SageTV will continue starting up After a few more seconds SageTV will present us with the Configuration Wizard. The first page of the wizard is Select Your Country which is easy to figure out (I hope!). Navigation is pretty simple, generally using the arrow keys, and enter to make your selection’s. Page 2, is to chose the Language we will be using The following page we get to Enable SageTV Server so that we can connect SageTV Clients to the server. Next step is to also Enable Media Extender and Placeshifters so these can also connect to the server SageTV has a really cool feature for remote users to use the server, which they call Placeshifter, to take advantage of this we also need to create user accounts so that these connections can be secured Click on Configure Placeshifter users and we will see a dialogue with some options. Just select Add New User and hit enter. we will be A keyboard will then appear where we can now supply the user name for the account which will be used for Place Shifting. After you hit Enter, the display will refresh and we now need to provide a password for this account You will be asked to retype the password to confirm it, and then we should see that the new accounts has been added is is now Enabled for use Once you Close the user list, you will be returned back to the wizard, where we can now Continue On the next page, we get the option to Configure Place shifter Internet Connection, after testing we can simply press on Continue On the next page, we get to decide to auto start SageTV with windows. As we plan to use this as a server, I prefer to run headless and run as a service, so I am going with the default option No – Don’t Load SageTV We next need to tell SageTV what aspect ratio we are using on our display, and then select the option Done The next two screens allow you to adjust the picture to address any over or under scan on the display. On the next menu, SageTV offers how to deal with playing video when the menus are active As we have chosen to keep the video going, now we are asked how we would like to see this presented. Pretty obvious, but since we are going to be running as a headless server, the default option works just perfect. SageTV has some magic which it can use to figure out what you would like to record. Been pretty fussy about TV, I like to make my own decisions on which shows to record. Time for a weather check, we can tell SageTV our local town, and how we like the weather presented. Similar to the User accounts, these options will popup a dialogue where we can type in the town name, and select the units of measure for the weather reports. After all is set, we can then Continue Parental Control – No thanks, I am skipping over this one Now, time to tell SageTV where to find our media, The defaults might work for you, or you may need to point to a server where you have your media stored. Time to tell SageTV which Video standard you will be using, PAL is the choice for most of Europe. When SageTV records your shows, or needs to create Time Shifting files, we need to tell SageTV were it can put these, and how much disk to use (or not) The Next stage of the wizard allows us to setup the video sources we will be using for the TV Feeds The Source setup wizard will show us any currently installed sources, and offer us the option to Add New Source to the system, or Finish Source Setup once we are done so that we can continue with the main wizards remaining steps. You are not going to appreciate this, but for now I am going to skip installing the sources, as the first time around there are a few things we need to do, in order to ensure that we get the process correct and ensure we have a good experience. I will cover this in detail in another blog post. Back on the main wizard, we now get the opportunity to setup video playback. Not sounding like a parrot, but the server is going to be running headless, so I really do not care about video playback on this machine, after all that is why we use extenders! (Of Course you may have other plans, in which case, you may actually be interested in running these tests to get the video on the computer optimal) And then Finally, we get to the End Page where we have the option to go back and review some of the choices we made, or Return to the Main Menu which is where I am going. Nice work so far. Home Screen Well, after answering all those questions, we are now finally presented with the Home Screen Before we close and and convert over the SageTV system to run in Headless mode, I am going to install a plug-in which I fell is a requirement to provide a good experience when watching live TV. Installing a Plug-In Navigate down the menu to the Setup option, the press the Right Arrow, to fold out the sub options. On this menu, now scroll down to SageTV Pligins and again press on the Right Arrow. Yet another sub menu with now fold out, and we can again select the option All Available Plugins and once again press the Right Arrow You will now be presented the Plugin’s interface, and as this is our first visit we are presented with an information disclaimer. After reading this, we can select the option Do not show this message again and be dropped into the Plugin nirvana As you will quickly see, there are many Plugins to select from, and you can read about each one either in the SageTV forums, or by selecting one of the plugins and choosing the information button. To begin, our Cursor is located in the top Horizontal menu, with the **General **plugin category highlighted. Using the Arrow keys, I will navigate over to the category UI Mod and press enter, this will refresh the list of plugin’s to match this new category choice and I will then scroll down the list until I locate the plugin MiniGuide. Now, Pressing enter on the Plugin, its Options dialogue will popup, and I can now select the choice to Install Plugin, but before we do, there is no harm in clicking on View Plugin Details so that we can actually learn what this plugin is actually used for. In the case of the MiniGuide, the description is not very useful, so a visit to the forums will help out with understand what the real purpose of the plugin is, or in this case a simple picture will help. The guide appears at the bottom of the LiveTV screen, and allows you to use the remote to browse what is on now or later on your TV channels without actually changing channels or requiring you to go back to the main program guide, making the process less intrusive. Pressing on the Left Arrow once finished reading, returns you to the previous screen, where you can again press the enter button to pop up the Options dialogue, and now proceed with the Install Plugin process This will then start the process, collecting any dependencies on the way and also installing those as required also. After a few moments, once all the requirements have been downloaded and installed SageTV will inform us that the Install is complete After the Install is complete, SageTV will then detect if any type of restart or reload is required, and prompt you if that is the case Selecting Yes, will then cause the SageTV system to restart or reload as required. Normally this will drop you back in the home screen, unless your using an extender and we required a restart, in which case you will need to power up the extender again. That’s all there is to it. You now have your First plugin installed and ready for use a little later. I am pretty sure you will have spotted a lot of others on the list which appear to be interesting, so go ahead, and do some investigations. Service Mode Finally, we are now ready to switch SageTV over to Service Mode so that we can run Headless, and use the Extenders, Clients or Place shifters for our TV and Media consumption. We will first close down our SageTV installation, so that there is nothing running. SageTV may popup a warning asking if you are sure you want to shutdown. We are Once SageTV has stopped, we can now navigate our start menu, and we will locate SageTVServiceControl.exe which is what we now want to launch After Clicking to Launch to tool we will be presented with its interface. The process here is pretty simple. We first need to decided which account we would like SageTV to use while it is running. If you are going to have all your media on this server, then we can safely go with the default _LocalSystem _account, however if you have all your media stored on a different server, then LocalSystem is not a good option, as this account will not have permission to access media not local to the server SageTV is running on. If this is your scenario, you should have a user account, possibly on a domain which has access to the media. I generally use a domain structure and create an account especially for SageTV to use. In this case you will need to click on the option Change User and provide the username, and password for this account.",
    
  },
  {    
    "id" : "http-0-0-0-0-4000-sagetv-with-dvblogic-install-tvsource-1286404440",
    "site" : "damianflynn.com",
    "title": "SageTV with DVBLogic: Install TVSource",      
    "url": "http://0.0.0.0:4000/sagetv-with-dvblogic-install-tvsource/",                    
    "categories" : ["Developer","Smart Home/Buildings"],
    "tags" : ["Live TV / PVR / IPTV","Audio Video"],
    "authors" : ["Damian Flynn"],      
    "publishedDate" : "2010-10-06 22:34:00 +0000",
    "content" : "In the first post, we took a quick look at the modules we were going to deploy as part of our fresh build on the HTPC, and also put the server hardware together which is going to be the foundation for the build out. &gt; &gt; In this post, we will take the next steps, and introduce DVB Logic, Install some of their software suites, and then configure the software. VLC Before we get into the fun stuff of setting up DVBLink, we are first going to install VLC. This all round tool will allow us to verify the functionality of DVBLink later on in the process. Grab a copy of the latest version from www.videolan.org and install it on server. This process is very painless and should be completed within a few short minutes. All the defaults are fine for our requirements, so i simply click Next trough all the wizard steps. After we are complete, VLC will launch. We don’t need to use this currently, so we can just close out for now. DVBLogic The DVBLink product suite is designed to seamlessly integrate Satellite, Cable and IPTV sources into Virtual DVB-S2 Cards irrespective of the original source medium. The suite includes support for MPEG-2 and MPEG-4 HD video, multichannel audio, EPG information, teletext and subtitles. The suite has two main components, DVBLink Server and DVBLink Source. From the Image above, we can see the DVBLink Server presents two interfaces for other software to connect with; as a set of 4 Virtual DVB-S2 Capture cards, and also a UPNP Server. We will use the UPNP Server option to check everything is working correctly, and for the rest of the project we will be concentrating on the Virtual DVB-S2 cards. The second component are the Source modules, Currently DVBLogic have a number of Source Modules which can be used with the DVBLink server, which is quite impressive, and addresses most combinations you will ever require. **TVSource 3.1** Locally installed DVB/ATSC Tuner Cards are published to the Server using this component **Dreambox 3.1** Linux DVB Set-top boxes running PLi, Gemini or Niablosat Images can be used as Video Source **IPTV 3.1** IP TV Client, supports sources from IPTV Channels published or your private IP Camera’s **DVBDream** PC Based DVB Viewing Software which can be used as a Video Source **HDPVR** Hauppauge HD capture device, useful for connection analogue devices so they can be used as a Video Source For this scope of this project, we are going to use just one of the components, the TVSource Module, which we will be configuring to use the TBS 6980 DVB-S2 cards we installed already. Installation Ok, time for the the fun stuff, we are going to install this amazing piece of software. Let’s start by downloading the TV Source software. You can grab the latest release code at http://www.dvblogic.com/download.php, currently at version 3.1. After downloading the package, you can extract and and we should see two components to install. DVBLink Server We will begin the installation with the DVBLink server, this application as we introduced a little earlier is the core of DVBLogic’s DVBLink product, once installed we will have 4 new Virtual DVB-S Cards presented to the operating system, and a new UPNP server running. Lets launch the installer, and click on Next to progress to the next screen of the wizard. We are then presented with the License Agreement, which we need to accept, and then click on Next. Next, we select and accept the installation location for the server software, and once satisfied we again click on Next Now, we are ready to being the installation process, Click on Install and the wizard will start on its magic. After a few seconds of installation, we will be prompted to verify that we are OK to trust and install the drivers required to create the Virtual DVB Tuners. This is extremely important, so I normally click on Always trust software from “Tabekc” and then click on Install to allow the installation to progress The installation should then continue for a few more seconds And then finally, we will be presented with a message to confirm that the Install has completed. Nice work, at this point we now have the Server and its associated Virtual DVB-S cards installed. We are now ready to proceed to the next step What actually happened? If you are curious like me, you can now check to see what actually has occurred under the ‘hood’. Right Click on “My Computer” and select the option Manage. This will launch the “Computer Manager”, where we can see the “Device Manager” listed under “System Tools” on the left hand tree. Clicking on this “Device Manager” the Right pane will redraw, and we can expand the option “Sound, video and game controllers” where we should see our 4 new “DVBLink Capture” virtual devices appear. DVBLink TVSource At this stage we can now install any of the other DVBLink components, As we mentioned at the start of the process I am going to just focus on the TVSource module. This will shortly be configured to serve our recently installed the TBS 6980 DVB-S2 Cards to the DVBLink Server Module, In the same expanded folder as we located the server module, we can now proceed and launch the installation process for DVBLink TVSource. After launching the installer, and click on Next to progress to the next screen of the wizard. We are then presented with the License Agreement, which we need to accept, and then click on Next. Now, we are ready to being the installation process, Click on Install and the wizard will start on its magic. The installation should then continue for a few more seconds And then finally, we will be presented with a message to confirm that the Install has completed. Now, that was easy right. Configuration You will by now also have noticed that the **DVBLink Configuration **windows will be displayed on screen, and we can dive straight into configuration of source. (Note – you can always open this tool at any time, from the All Programs in the Start Menu find the DVBLink programs group and open DVBLink Server Configuration.) On the mail page we can see tabs on the top and bottom of the main window, Currently we have just one Tab at the top of the window called Sources, which is automatically selected for us. Along the bottom of the window we have additional tabs, which read Sources and also Server Configuration. For the most part we are not going to see any additional tab’s along the top of the work space appear; DVBLogic have some additional software called the _Network Server _which we have not even mentioned until now, which will result in a new tab been presented here, but that is beyond our current requirements. So, the Configuration work we will be doing with DVBLink is going to be between the two tabs on the bottom of the workspace, and before we can really gain value from the second tab, we need to provide some more information on the first one…. Stage 1 - Sources Ok, enough chatter, lets get going. On the Source tab we have 2 panes. The left pane is a list of Source Types we can add (known as templates). As you can see we have 2 available to us. If you decided to install some of the other DVBLink source components, these will create additional templates for us. Click on TVSource **and then the top arrorw should glow Green to signify that we can now **Click on the Arrow. After clicking, a dialogue will popup asking us to provide a name for this TV Source we are adding. You can name as you like, personally I am going to use a simple name to indicate easily which source this entry is referencing. As an example, I have 2 TBS 6980 cards, both of which have 2 DVB-S2 feeds, so that will result in a total of 4 sources to be added once I am fully done. As a result I am going to call these TBS-A-1 to signify the TBS Card, Card A, and Source 1. Once you are happy with your chosen name, click on OK. Now, on the right pane, we get to see that our new source is added to the list. In this pane we can now quickly see that the source is from the TVSource template, it will be a Video stream and its status is currently down (Red). Now, we need to configure this new source. To do that we need to click on the ‘…’ **button in the far right column of this source. After clicking, we should get a new window **TVSource Configuration, with another set of tab’s across the top. We will be sitting on the Device Tab, where we get to see a list of the physically installed TVSource devices which are available on our server. In the shot below you will see 2 devices, which are the 2 DVB-S2 ports on my TBS-6980 card (in the shot below only one of my TBS 6980’s is installed in the server) Now, we need to select one of these 2 ports to be used for this source configuration, (we will add a second source shortly and select the other port then). So since I named the source as TBS-A-01, im going to pick the first port by clicking **on the **Tick Box for that port, and the text under status should change to read Active. Note that the Type I did not change from _Satellite** **_because these really are Satellite inputs, but I recommend you change these to match the technology you are using! Cool, that was not to hard. Now we can move on to the next tab in the view, which is called Headends. A bit of an odd name, but the theory is very simple. The software simply wants to know what this source is actually connected to, that is, what is on the end of the input! Looking at the column headers, these have some satellite specific questions, but don’t worry to much about that, this is not hard to figure out either. Of course if you changed the Type on the device tab, these these headers would be a little different to match your chosen technology. Ok, In my initial setup I am keeping things very simple, and each input is cabled directly to an LNB feed on my satellite dish, and that dish is pointing at the stars for Sky UK, which we know is on a satellite called “Eurobird 1 - Astra 2B/C/D” at an orbit of 28.2 W I simply Click on the Add **button, and this pops in a line in the list of _Headends, _Since I am just using 1 Headend with this source, I do not need to make changes to the **Diseqc **options, but simple select my target satellite from the drop down list under **Provider. Next we can move on to the Scan tab and select the Scan button to start the scan of the services connected to our headend. In my case that of course is the satellite for Sky UK. Once the scanning begins we should see some of the services been broadcast been detected. The complete scan may take a little while to finish up, but its a necessary evil, as the TV companies like to move stuff around up there Once scanning is complete, we will have the opportunity to test that everything is working correctly, which will give is the confidence we need later when we get around to connecting this software with other applications. So, lets click on the last tab Channels. Ok, so this is going to be very painless. In the top option we need to fill in the Path to VLC executable which I suggested you install earlier. You can click on the button to the right ‘…’ to pop one a browser to help locate the program. Now, think of a channel, any channel, just be sure its not encrypted. you can click on the name of your Headend and expand down the tree to find a perfect test channel, or just use the Find option and type in part of the channel name, then click **the **button with the spy glass. Now, you should have your chosen channel highlighted. Excellent. Now click on the Preview window, and we should see VLC start, and after a few seconds you should be watching your chosen channel. Great, we have now completed the procedure to setup this source. You should repeat this process for each source you want enabled. Stage 2 – Server Configuration Ok, we are not ready to move on with the next stage of the setup. One way of thinking about this process is to consider that you are now the TV Broadcaster, and you now have the power to decided what channels you would like broadcast. You have already located a few hundred possible video feeds in the previous steps while configuring the sources, so here we are just going to patch the video feeds trough that we are interested in, and leave out the stuff we don’t care about, for example all those horrid shopping channels. Back on the main window, we will click on the bottom tab for Server Configuration which will present us with some new panes to work within. The first thing you should notice now, is the new tabs which have just appeared along to top of the window, Channel Selection, Channel Merge, Channel Settings and EPG Sources. Starting in the first tab, Channel selection we have a two pane view again, On the left pane we now see all the video feeds (Channel’s) which we configured earlier, and on the right pane we will create a list of the channels we will be patching trough for our rebroadcast. If you have multiple sources, and the feed (channel) you are interested in appears in more than one source, you should move the additional copies of the channel to the right pane also. This will permit the DVBLink server to tune to the feed, if for example the first source is busy, by selecting this feed from the second source. Now, lets move on to the second tab Channel merge where we can use the options on the right side to help group video feeds (channels) of the same service together If you prefer, you can always drag and drop the feeds to group them as you need. The purpose of this process is to teach the DVBlink server which feeds are the same if they appear from different sources. Once you have completed setting up the merge groups, we can then move on to the **Channel settings **tab. Good work, now we just need to add channel numbers for each of the feeds we are broadcasting. Again, as you are the broadcaster, you have the control on what numbers you decided to use. When you have completed this, you are all done! Fantastic, now you can Select **OK **and if prompted you should agree to store the changes. Testing At the beginning of the article, we mentioned that the server broadcasts on two different technologies, The first is the “Virtual DVB-S cards”, and the second been a “UPNP Server”. We are going to test that the server is working by using the UPNP Server, Windows 7 is a nice simple client for UPNP, so all we need to do is launch Windows Explorer, and open network connections and verify you have the DVBLink Server present: Connect Open up Windows Media Player and under** Other Libraries** you should see an entry for DVBLink TV Server underneath your server name Select Recorded TV, and you should see** **all of your feeds been broadcast. Yep, Damn Cool. Double click on any of the channel and then when the window pops up select the Play button: Conclusion Well, let me first congratulate you, you are now your own satellite provider, and we have now the first major task completed in sorting or HTPC. Until next time.",
    
  },
  {    
    "id" : "http-0-0-0-0-4000-evolving-the-cloud-1286404320",
    "site" : "damianflynn.com",
    "title": "Evolving the Cloud",      
    "url": "http://0.0.0.0:4000/evolving-the-cloud/",                    
    "categories" : ["Cloud Strategy"],
    "tags" : ["White Paper / Case Study","Lionbridge","Virtual Machines / IaaS","System Center","Virtual Machine Manager","Cloud","ITIL","Self Service"],
    "authors" : ["Damian Flynn"],      
    "publishedDate" : "2010-10-06 22:32:00 +0000",
    "content" : "During the past 2 years hosting our cloud, we have migrating form Windows 2008 and Virtual Machine Manager 2008, to the current releases of Windows 2008 R2, and Virtual Machine Manager 2008 R2; which combined with the features within the self service portal, have served us well, and thought us a lot. Learning’s, on how to implement features correctly, and painful experiences when we do not structure stuff well. Like all technologies, with experiences we learn to live with poor processes, and often consider alternative approaches. The time has finally come, to address some of these issues, and begin a process of deploying a more accessible environment, while simplifying the experience for both the Administrator and more importantly, our users. Over the coming weeks, I will post learning’s, tricks, challenges, and even some guides as we begin this digression and prepare to release a new interface to our private cloud.",
    
  },
  {    
    "id" : "http-0-0-0-0-4000-sagetv-with-dvblogic-2-1286231460",
    "site" : "damianflynn.com",
    "title": "SageTV with DVBLogic",      
    "url": "http://0.0.0.0:4000/sagetv-with-dvblogic-2/",                    
    "categories" : ["Developer","Smart Home/Buildings"],
    "tags" : ["Live TV / PVR / IPTV","Audio Video"],
    "authors" : ["Damian Flynn"],      
    "publishedDate" : "2010-10-04 22:31:00 +0000",
    "content" : "Over the past 12 months, much has improved in the world of HTPC. Microsoft has released Windows 7 on us, Frey Technologies have presented their new Version 7 software HTPC, and DVB Logic have completed no less than 2 major released of their DVBLink software. &gt; &gt; Over the course of the next few posts, I am going to cover a complete rebuild of my HTPC solution, utilizing SageTV, DVBLink, DigiGuide, and some additional utilities to create a stable and scalable IPTV solution. Before we start on the actual installation and configuration of this solution, we are going to need to get a number of items organised, including the DVB tuner cards which we will utilize for Live TV Reception. With this in mind, you can prepare for the steps we are going to cover Start by Installing the DVB Tuner Cards, along with their current drivers Once the hardware is all setup and ready, we can proceed to download the software components which will construct this solution DVB Logic, TV Source 3.1 VLC SageTV Server DigiGuide (or other suitable XMLTV source for your geographic location) Stephanes XMLTV importer If your satisfied you have gathered all the parts needed to get this project running they we are already on the way. DVB Tuner Drivers The hardware I have elected to use as the IPTV server for my deployment, is a shallow 1u rack mountable system from Dell which has 2 PCI-E ports, a nice low power footprint, and a cool 8Gb of RAM, with 2 1Gb NICs to get good streaming, topped off with a 2Tb SATA disk for storing TV recodings. The TV Tuner cards I am using are slim dual head TBS-6980 DVB-S2 cards which are listed on the Harware compatability list of DVBLogics TVSource application. Installation of these cards in the Dell server is very simple, and withing a few minutes two cards are nice and snug within the chasis of the server. After restarting Windows 7, the OS quickly detects that the new cards have been installed and simply identifies the cards as a Multimedia Video Controller. A quick check on the TBS website http://www.tbsdtv.com/english/product/6980.html, and I can see that the current driver available is version 2.0.0.8. Wasting no time, let’s download this and extract its content ready for installation. Once the driverpackage is downloaded and extracted, we can continue with the installation. The Driver for the TBS 6980 is very easy to setup, we simply need to run the installer and it does the work for us, we just need to accept any changes and modules it would like to install, and a few seconds later we should be seeing a completed screen, which we can click Finish on to complete the setup. Prepare for DVBlogic Nice work, at this stage we have our hardware ready, we can hook up any antenna cables required, and relax as the physical labor is now complete. In the next stage we will install the DVBLogic software, and test that it is indeed working correctly with our new hardware and that we can indeed tune TV Channels.",
    
  },
  {    
    "id" : "http-0-0-0-0-4000-another-case-study-on-our-evolving-cloud-1279578480",
    "site" : "damianflynn.com",
    "title": "Another Case Study, on our evolving Cloud!",      
    "url": "http://0.0.0.0:4000/another-case-study-on-our-evolving-cloud/",                    
    "categories" : ["Announcements","Cloud Strategy"],
    "tags" : ["Achivements","White Paper / Case Study","Lionbridge","Community","Virtual Machines / IaaS","System Center","Virtual Machine Manager","Cloud","ITIL","Self Service"],
    "authors" : ["Damian Flynn"],      
    "publishedDate" : "2010-07-19 22:28:00 +0000",
    "content" : "Its difficult to believe that 2 years have passed since the first case study on our private cloud was published. In that time we have had the pleasure of working with some amazing talent within Microsoft, and continue to work on some very cool solutions accelerators, which will ultimately become part of the “System Centre vNext” product over the coming years. We have again teamed up and published a new case study on our current focus and plans for how this technology will be utilized along with the benefits we expect to gain. You can click here to read more. Things are getting interesting",
    
  },
  {    
    "id" : "http-0-0-0-0-4000-sagetv-and-hi-def-satellite-tv-1249770300",
    "site" : "damianflynn.com",
    "title": "SageTV and Hi-Def Satellite TV.",      
    "url": "http://0.0.0.0:4000/sagetv-and-hi-def-satellite-tv/",                    
    "categories" : ["Developer","Smart Home/Buildings"],
    "tags" : ["Live TV / PVR / IPTV","Audio Video"],
    "authors" : ["Damian Flynn"],      
    "publishedDate" : "2009-08-08 22:25:00 +0000",
    "content" : "Before we start, lets get the objectives listed. 1) Stable Server to host Sage 7 2) Multiple Satellite feeds for the Live TV Service 3) Fully working EPG As I am based in Ireland, the primary satellite I will be focusing on is Astra 28.2; otherwise know as Sky UK or Freesat. The solution will be able to deliver Standard and High Def TV to all my Extenders, and with a little more work which for legal reasons we will not delve into, have the ability to decode with legit cards encrypted Channels. I am fortunate to have 4 Satellite dishes out back, all of which I will be connecting to the Sage box, covering from Thor at 0.8 w, Hotbird at 13w, Astra at 19w and the focus for this document Eurobird/Astra2 at 28.2w Overview I am going to build this project from the ground up, so as you read trough, you are welcome to pick and mix the different modules to suite you own needs. The theory is sound for all Satellite’s following the Ku band technologies. The cards we use must be have a driver format know as “BDA”, otherwise you are really better to throw in the bin and go shopping. I have used Digital Everywhere FireDTV, and Technotrend cards with no real issues, however my latest hardware is dual head, increasing the number of feeds i can fit in the same small server. My Environment For this project I am using an older Server class piece of kit, the Dell 860 1U rack machine. It will be loaded with Windows 7 Ultimate, has 8gb of RAM, and using 2 new TBS 6980 Dual Head DVB-S2 satellite cards. The Feeds will be direct from a Quad LNB. Ingredients If you like to be organised up front, then now is as good a time as any to go gather in the tools we are going to use in this project. Please note that there are many choices available for each of the key components we will be utilizing. I will try to explain why i have made my personal choices to help you understand the logic I am employing, but please do not consider this as gospel. You are welcome and encourage to use the solutions which best fit your personal needs. OS DVB-S Bridge Software SageTV Extenders/Place shifters / Clients XMLTV TV Guide Data OS The choose of the Operating system is really up to you. The requirements are simple – Must support the BDA framework. I am going to focus on Windows OS; however with a little magic this could be done on Linux or MAC. you just need a version of the tools which are compatible with those operating systems. The problem software will generally be the DVB-S Bridge options as these are mainly windows based. If you do decide to use and x64 based OS, remember to update the paths in the guide to represent the x86 version [normally on x64 OS versions use c:program files (x86) as the base path]. For my deployment I am selecting Windows 7 Ultimate x64. I know that this OS has a full support layer for the Media components we need especially BDA. I have hacked a Windows 2008 server into getting BDA support added, but its to much pain to worry about, and if your going to make this the main TV solution for your home, you are not going to want the Wife Acceptance Factor (WAF) to sink to null if you can not quickly rule out where the issues are (it is just a painful experience). I am not going to hand hold you trough the installation of Windows 7, so just grab that USB or DVD and get the OS deployed. Expect that you will need to spend a little time installing the additional drivers required for the DVB-S2 cards. If you are joining a “Domain” go for it, or in windows 7 you could also create or join a “Homegroup” for sharing your media. ### DVB-S Bridge Before I even bother to waste further time working on this project, I am going to validate that the DVB-S system is working correctly. therefore I am going to jump directly into installing the bridge software. There are a number of options again available to select from for this part of the installation 1) DVBViewer for SageTV 2) TheatherTech for SageTV 3) DVB Enhanced for SageTV 4) DVBLink2 (Not validated support for SageTV in the current version) DVB Enhanced for SageTV (DVBE4SAGE) DVBE4SAGE is provided as a ZIP archive and is pretty simple to install. If you have not downloaded the software, point your browser to http://code.google.com/p/dvbe4sage/downloads/list and grab the latest build. This was 236 at the time of writing. Create a folder on your chosen computer, for example c:program filesdvb4sage and proceed to unzip the downloaded software. Once complete you will see a few new files and a new folder as in the shot: [![image](/assets/posts/2010/10/image_thumb.png)](/assets/posts/2010/10/image.png) &gt; &gt; What these files actually do is pretty much self explainatory dvbe4sagescv.exe We will ignore for now, but once we have fully debugged, this will permit us run the software as a windows Service. encoder.dll Support file for DVB stream handling dvb4sage.exe The main application we will be executing dvb4sage.ini The configuration file for the DVB4SAGE application Plugins Folder Holder folder for any SINGLE MDAPI Plug-in that you may need to utilize Ok, that is all there is to installing this software, its time to configure the settings, and then we can test. Configuration Lets start with the configuration file I am using for SkyUK/Freesat on 2 DVB-S Feeds, then we can break it into sections to understand what this really means. [General] LogLevel=2 NumberOfVirtualTuners=2 &gt; &gt; [Plugins] DCWTimeout=5 IsVGCam=0 MaxNumberOfResets=1 ServedCAIDs= ServedPROVIds= &gt; &gt; [Tuning] LNBSW=11700000 LNBLOF1=9750000 LNBLOF2=10600000 InitialFrequency=10714000 InitialSymbolRate=22000 InitialPolarization=H InitialModulation=QPSK (DVB-S) InitialFEC=5/6 DVBS2Tuners=2 TSPacketsPerBuffer=1024 NumberOfBuffers=400 TuningTimeout=20 TuningLockTimeout=5 InitialRunningTime=40 UseSidForTuning=0 UseNewTuningMethod=0 ScanAllTransponders=0 ExcludeTuners= ExcludeTunersMAC= IncludeTuners= IncludeTunersMAC= &gt; &gt; [Output] TSPacketsPerOutputBuffer=160000 TSPacketsOutputThreshold=135 DisableWriteBuffering=0 &gt; &gt; [Encoder] ListeningPort=6969 &gt; &gt; [Recording] PreferredAudioLanguage=eng PreferredAudioFormat=ac3 PreferredSubtitlesLanguage=eng &gt; &gt; [Advanced] PMTDilutionFactor=1 PATDilutionFactor=1 PMTThreshold=100 PSIMaturityTime=40 MaxECMCacheSize=3000 EnableECMCache=1 ECMCacheAutodeleteChunkSize=300 BouquetName=BSkyB Bouquet 4 - DTH Other PreferSDOverHD=0 &gt; &gt; Ok, If you just glanced over this, some of the options are pretty straight forward on what they are about. The configuration file sections and parameters are described in the following table: [General] LogLevel=2 Determine the amount of Logging we want - 2 is good for normal use, 0 will stop logging when you are happy all is good, and 5 will hide no secrets! NumberOfVirtualTuners=2 This is the number of Satellite Tuners which the DVB-S2 bridge tells Sage it can have, this should be as many as physical tuners you are using, and can be more depending on the channel layout. [Plugins] DCWTimeout=5 We will ignore this section as its focus is for CAM software when we need to decode encrypted TV IsVGCam=0 MaxNumberOfResets=1 ServedCAIDs= ServedPROVIds= [Tuning] LNBSW=11700000 One of the cool functions of this software is to go and find all the channels on the satellite for you, here we provide the frequency of any one transponder on the satellite and the software can then do the rest. LNBLOF1=9750000 LNBLOF2=10600000 InitialFrequency=10714000 InitialSymbolRate=22000 InitialPolarization=H InitialModulation=QPSK (DVB-S) InitialFEC=5/6 DVBS2Tuners=2 The number of Physically connected DVBS connections TSPacketsPerBuffer=1024 NumberOfBuffers=400 TuningTimeout=20 TuningLockTimeout=5 InitialRunningTime=40 UseSidForTuning=0 Do we want to use the Service ID or Channel Number to tune with? This is a cool new feature in the software which I will take advantage of UseNewTuningMethod=0 ScanAllTransponders=0 ExcludeTuners= ExcludeTunersMAC= IncludeTuners= IncludeTunersMAC= [Output] TSPacketsPerOutputBuffer=160000 TSPacketsOutputThreshold=135 DisableWriteBuffering=0 [Encoder] ListeningPort=6969 The TCP port we will connect to the Bridge on from SageTV, This will increment +1 for each Virtual Tuner we have as listed at the top of this configuration [Recording] PreferredAudioLanguage=eng If the Channel broadcasts multiple audio streams, which would we prefer to use? PreferredAudioFormat=ac3 PreferredSubtitlesLanguage=eng [Advanced] PMTDilutionFactor=1 PATDilutionFactor=1 PMTThreshold=100 PSIMaturityTime=40 MaxECMCacheSize=3000 EnableECMCache=1 ECMCacheAutodeleteChunkSize=300 BouquetName=BSkyB Bouquet 4 - DTH Other This is used in conjunction with the UseSidForTuning option, channel numbering may change by region on the satellites, so here we name the region we want PreferSDOverHD=0 If the channel is in SD and HD, would you prefer to use the SD version? Initial Run DVB4SAGE Ok, we are almost done with the setup for DVB4SAGE! Seriously. Before we move on to getting SageTV configured we will use the built in function in DVBE4SAGE to test that Channels can be recorded correctly. Start DVBE4SAGE, if your OS has UAC enabled, select to start with Administrator level privileges. I generally right click and select Run as administrator to launch the program. This helps ensure there is no resource issues we need to deal with. Start up will take about 60 seconds or so to complete, during this time it will Scan of the Hardware, bring online the DVB environment, and listen for the NIT/PMT table from the Transponder we defined in the configuration file, and grab the Channel number information for us. Once complete we should see a final message Graph Successfully stopper It is a good idea to scan over this log, to check for errors, but if you see messages such as in the below sequence at the top of the log then you can rest easy that stuff has gone to plan so far. _2010-05-28 20:08:33.218 #1: Loading filter \"TBS 6980 BDA DVBS/S2 A Tuner/Demod\" - succeeded! 2010-05-28 20:08:33.218 #1: Tuner Filter Info = \"TBS 6980 BDA DVBS/S2 A Tuner/Demod\" 2010-05-28 20:08:33.233 #1: Loading filter \"TBS 6980 DVBS/S2 A AVStream TS Capture\" - succeeded! 2010-05-28 20:08:33.236 #1: Loaded our transport stream filter 2010-05-28 20:08:33.237 #1: Added demux filter to the graph 2010-05-28 20:08:33.238 #1: Connected demux to our filter output pin 2010-05-28 20:08:33.242 #1: Loading filter \"BDA MPEG2 Transport Information Filter\" - succeeded! 2010-05-28 20:08:33.243 #1: Using tuning request-based tuning method... 2010-05-28 20:08:33.933 #1: Starting initial run for autodiscovery, transponder data: Frequency=10714000, Symbol Rate=22000, Polarization=H, Modulation=Not defined, FEC=5/6_ &gt; &gt; As you scroll back to the bottom of the log, we should see messages which you can recognise as channel names from the satellite, looking closer you will also notice that there is a channel number detected for each entry: Skip back to the end of the log and we should see the **Graph successfully stopped **message, which tells us we are ready for business. 2010-05-28 20:08:54.122 #1: Mapped SID=55355, ONID=2, TSID=2614, Channel=452, Name=\"Rush HD\", Type=25, Running Status=4 2010-05-28 20:09:13.935 #1: The initial run for autodiscovery finished 2010-05-28 20:09:13.938 #1: Signal locked, quality=98, strength=86 2010-05-28 20:09:13.955 #1:** Graph successfully stopped** &gt; &gt; Before we move on, just check the results of the Signal Locked entry, this will help us ensure that the Satellite feed is strong enough to start the tuning. Testing Now, we can proceed and test a recording. I recommend that you select an non-encrypted service or Free To Air channel as they are referred to. We will select the Operations menu and then Start Recording… In the previous stage we observed all the channel names and their respective channel numbers which had been detected. On this satellite one of the better know clear channels is Sky News, which is on Channel Number 501. There are many others, and looking back the log we can pick to test with, an example of a HD option is BBC HD which is on Channel 143. I’m going to start high since i am over confident and select the HD option. I will enter 143 in the Channel field, and then click on OK. This will start off the recorder, which should now create a new file in the program folder for DVBE4SAGE which is the recording. As the default record time is 60 seconds, we will then see the following lines appear once the time has expired 2010-05-28 20:53:13.017 #1: **Time passed, stopping recording of channel 143 (\"BBC HD\")** on source=\"TBS 6980 BDA DVBS/S2 A Tuner/Demod\", Ordinal=1 2010-05-28 20:53:13.017 #1: Graph successfully stopped &gt; &gt; Looking in the folder we should see the new recording file, which was just created. Don’t bother double clicking the file to play it, as Windows Media Player will likely start, and it is not able to play back the content without some more tweaking. I do however suggest you grab a copy of VLC and install it. then open the recording with VLC and you should actually see your recorded TV station. The result should look similar to this. Right, if everything has gone to plan, the DVBE4SAGE part of the configuration is done, and we can move on to the next part of the puzzle. If you have had some issues then its a good idea to get more technical, and investigate more before progressing to the next segment. Manually Tuning If you are reading this, then either your scan had no channel numbers, you are having more issues, or you just want to be a armed with good techie pub banter! Right, to make sense of the rest of the dialogue we will open a look at LyngSat SkyUK Page for some technical details on our target satellite. Of course all the sat details should be updated to represent your provider and location. Now, Armed with some knowledge, lets fill in the Recorder Interface. We are going to start with the simple option, because if this works, we can just skip straight on for SageTV preparation, otherwise we need to debug more. This time, we just enter the SID for the channel we are going to test, in this case Movies4Men which has a SID **of **53109. We type this into the Channel/SID field and also Click on the Use SID option. Now, just Click OK Now, in the same folder as we have the **DVB4SAGE **software we will see the new file created, and as long as its not 0 Bytes we should be in business. You can use the SageTV file browser to play this file back to see what was just recorded. _2009-08-04 15:15:23.905 Time passed, stopping recording of service 53109 (\"movies4men\") on tuner=\"FireDTV BDA Tuner DVBS2\", Ordinal=2 2009-08-04 15:15:23.913 Graph successfully stopped_ &gt; &gt; You will see if everything worked correctly, that the log shows we Auto Discovered the correct transponder details for the channel. When we get to the Setup step of mapping the EPG to the Channel, since Channel numbers did not work for us, we will have no option but to use this SID number instead, So that Lyngsat page will be very important in gathering this information. If you did not have success at this stage, go back to the recorded, but this time Untick **the option **Automatically discover the transponder settings and add them as you see them from the Lyngsat details. Refer to the image above for some additional assistance, as an example, we will untick the option Automatic discover transponder settings, **and enter the **Frequency **of **1224000, Symbol Rate of 27500, Polarity of Vertical, and FEC of 2/3 SageTV Go ahead, download if you have not already done so, and install the SageTV Application, current version is 7.0.9 at the time of writing. Once you have SageTV installed, connect up your extender if you are using one, and configure the environment with your regional details. If you are using the SageTV interface on the computer, ensure you have installed any necessary Codec packs also, otherwise we will have issues later when we try to view Live TV. Once you are happy that the Basic SageTV environment is working, and you can view an AVI or MKV File, listen to your MP3, and see some photos. Sage.Properties Stop SageTV and ensure the SageTV service is not running. Then add the following entries to the sage.properties file for each Virtual Tuner we will be adding to the environment. If you have configured DVB Enhancer to support more than one Virtual tuner, which is what we are connecting SageTV to, then we need to make a few minor changes to this detail for each additional tuner mmc/encoders/1111 The 1111 number should be incremented for each tuner, and should be unique to the sage.properties file. so tuner two should read for example mmc/encoders/1112 mmc/encoders/1111/1/0/device_name=Tuner 1 mmc/encoders/1111/encoding_host=127.0.0.1:6969 mmc/encoders/1111/video_capture_device_name=SageTV DVB-S2 Enhancer 1 mmc/encoders/1111/1/0/available_channels= mmc/encoders/1111/1/0/brightness=0 mmc/encoders/1111/1/0/contrast=0 mmc/encoders/1111/1/0/device_name=Tuner 1 mmc/encoders/1111/1/0/encode_digital_tv_as_program_stream=false mmc/encoders/1111/1/0/hue=0 mmc/encoders/1111/1/0/last_channel=2939 mmc/encoders/1111/1/0/provider_id=-8505416142496727040 mmc/encoders/1111/1/0/saturation=0 mmc/encoders/1111/1/0/sharpness=0 mmc/encoders/1111/1/0/tuning_mode=Air mmc/encoders/1111/1/0/tuning_plugin= mmc/encoders/1111/1/0/tuning_plugin_port=0 mmc/encoders/1111/1/0/video_crossbar_index=0 mmc/encoders/1111/1/0/video_crossbar_type=100 mmc/encoders/1111/audio_capture_device_index=-1 mmc/encoders/1111/audio_capture_device_name= mmc/encoders/1111/audio_capture_device_num=0 mmc/encoders/1111/audio_processor= mmc/encoders/1111/broadcast_standard=DVBT mmc/encoders/1111/capture_config=67600 mmc/encoders/1111/default_device_quality=Fair mmc/encoders/1111/device_class= mmc/encoders/1111/dshow_tv_type= mmc/encoders/1111/encoder_merit=7 mmc/encoders/1111/encoding_host=127.0.0.1:6969 mmc/encoders/1111/forced_video_storage_path_prefix= mmc/encoders/1111/last_cross_index=0 mmc/encoders/1111/last_cross_type=100 mmc/encoders/1111/live_audio_input= mmc/encoders/1111/multicast_host= mmc/encoders/1111/never_stop_encoding=false mmc/encoders/1111/video_capture_device_name=SageTV DVB-S2 Enhancer 1 mmc/encoders/1111/video_capture_device_num=0 mmc/encoders/1111/video_encoding_params=Fair mmc/encoders/1111/video_processor= SageTV Testing After we add the virtual tuner to SageTV, we can progress the configuration and test to see if the two systems are getting a successful communications channel open. Once SageTV is started, we can go to the tuner setup, where we should now see the new Virtual tuner we just added as part of the installed and available tuners list. After selecting this new tuner, we get presented the Source Details where we will use the Channel Setup option to proceed and configure a channel to validate that both SageTV and DVB Enhancer for SageTV can communicate with each other. Now, In Channel Setup we can select Edit Channel Line-up and then Add Channel to Line-up In the next Screen We can provide the detail for each Channel For example we Add a channel from SkyUK package, for example Sky News Ireland The Details for this will look as follows Station Name **Sky News **[The Name we will see in the guide for the channel] Station ID Logic (Guide) Channel Number **501 **[For your TV Channel Number] Physical (Tuning) Channel Number 4711 [This is the SID of the Channel and passed to DVBE4EPG to tune in the channel for us] Once you have provided this detail, we can click on the Add – Done option which will be us back to the **Channel Setup Page. **At this point you can of course go ahead and preview the channel to make sure that SageTV and DVB Enhancer are working as expected After clicking on preview, SageTV will send its message to DVBE4SAGE and start the tuning procedure, requesting the Physical (Tuning) Channel Number we defined a few moments earlier, when we added the channel. You should now see the picture from this channel and, if you check the logs in DVBE4SAGE you should see the tuning request correctly transfer, and the stream file begin to grow _2009-08-04 15:44:00.100 Received command: \"START SageTV DVB-S2 Enhancer 1 Digital TV Tuner|1539987586|4711|2498794080168|C:Program Files (x86)SageTVSageTVVideoAfternoonLiveWithKayBurley-4715-1.ts|Fair\" 2009-08-04 15:44:00.101 Received START command to start recording on source \"SageTV DVB-S2 Enhancer 1 Digital TV Tuner\", channel=4711, duration=2498794080, file=\"C:Program Files (x86)SageTVSageTVVideoAfternoonLiveWithKayBurley-4715-1.ts\" 2009-08-04 15:44:00.102 Service SID=4711 has Name=\"Sky News Eire (Cable)\" 2009-08-04 15:44:00.102 Autodiscovery results for SID=4711: TID=2026, Frequency=12207000, Symbol Rate=27500, Polarization=V, Modulation=QPSK (DVB-S), FEC=2/3 2009-08-04 15:44:00.102 Starting recording on tuner=\"FireDTV BDA Tuner DVBS2\", Ordinal=2, SID=4711 (\"Sky News Eire (Cable)\"), Autodiscovery=TRUE, Duration=2498794080, Frequency=12207000, Symbol Rate=27500, Polarization=V, Modulation=QPSK (DVB-S), FEC=2/3 2009-08-04 15:44:00.116 Loading filter \"FireDTV BDA Tuner DVBS2\" - succeeded! 2009-08-04 15:44:00.121 Tuner Filter Info = \"FireDTV BDA Tuner DVBS2\" 2009-08-04 15:44:00.145 Loaded our transport stream filter 2009-08-04 15:44:00.146 Added demux filter to the graph 2009-08-04 15:44:00.147 Connected demux to our filter output pin 2009-08-04 15:44:00.148 Loading filter \"BDA MPEG2 Transport Information Filter\" - succeeded! 2009-08-04 15:44:00.151 Using tuning request-based tuning method... 2009-08-04 15:44:00.428 Found CA descriptor EMM PID=0xC0(192), CAID=0x960(2400), PROVID=0x0(0), this CAID is served and will be passed to plugins 2009-08-04 15:44:00.669 Received command: \"GET_FILE_SIZE C:Program Files (x86)SageTVSageTVVideoAfternoonLiveWithKayBurley-4715-1.ts\" 2009-08-04 15:44:00.669 Replied 0 2009-08-04 15:44:00.716 Received command: \"NOOP\" 2009-08-04 15:44:00.949 Received command: \"GET_FILE_SIZE C:Program Files (x86)SageTVSageTVVideoAfternoonLiveWithKayBurley-4715-1.ts\" 2009-08-04 15:44:00.949 Replied 27636 2009-08-04 15:44:01.154 Received command: \"GET_FILE_SIZE C:Program Files (x86)SageTVSageTVVideoAfternoonLiveWithKayBurley-4715-1.ts\" 2009-08-04 15:44:01.154 Replied 134984 _ &gt; &gt; Congratulations – Phase 1 is done, and we have a working SageTV to DVB4Sage configuration DigiGuide First off, we download and install DigiGuide from www.digiguide.com. ### Install DigiGuide Start off the installation and we will be instantly requested for the location to install the tool to, we can accept the default, and click next. Before you know it, the tool is installed and ready for configuration The main screen is automatically presented as soon as installation is complete, and we can proceed with the Configuration Wizard. The first question is as simple as selecting the Country for which you will be subscribing to TV Channels. After this, we are asked to identify the provider / package for TV we are receiving. this will help the Guide get all the necessary channels we have in our area. Then we can select the Categories which we normally watch, or subscribe to. The next option, lets us put some preferences on the different channels categories we watch. This is not going to be very important for our installation as we will be using SageTV to present the guide and not relying on DigiGuide as a User Interface That’s should be it, We can finish off the wizard, and give DigiGuide a little time to work, as it builds up our channel list from the details we just provided, and then fetches the EPG details Once everything is updated we will see the guide information, and we can progress with the XML support for DigiGuide ## DigiGuide to XML Next we need to download the DG2XML support pack, this is provided on the Sage Site. Download and unpack the archive. Inside the archive you will find DG2XML.EXE and a folder DG2XML.dgscheme.web. We will move DG2XML.dgscheme.web folder into the DigiGuide Server Skins folder located normally in C:Program FilesDigiGuide TV Guideschemesserver skins Now, lets start up DigiGuide, and configure the Web Server, to use this DG2XML support pack as the Skin for the web service. To do this, In DigiGuide we select the Menu Tools -&gt; Customize -&gt; Web Service -&gt; Enable Web Service We don’t really need to configure support from password authentication, but if you wish, that’s fine Select the DG2XML skin from in the “Web Skin” section. Now, we can place the **DG2XML.exe **utility in the SageTV folder, as we will require this a little later as we configure the XML TV support in SageTV. Copy the DG2XML.exe file to a new folder, for example C:Program FilesDG2XML Now, we should be able to test that the XML Conversion tools are working, so lets pop open a command prompt and navigate to the DG2XML installation folder. Once there we will run the DG2XML utility and if it works correctly we should be presented with a new epgdata.xml file, generated from the **DigiGuide **information cd cd “program filesdg2xml” dg2xml.exe ## SageTV XMLTV Importer The next Stage of the process is to use the XMLTV tool from LMGestion. Stop the SageTV services and run the installer for the XMLTV Importer. After installation, we will be presented with the main interface Configure the XMLTV Grabber Now, we click on the top option XMLTV Files and Grabber to see what XMLTV sources we have configured. Since this is our first run we will just see one source called Default. We are going to edit this Default setting to work with our DigiGuide EPG Data. Click on Edit and we get to define more detail for the DigiGuide Feed. First we change the Source Name to read as DigiGuide. Next, Using the Browse button we locate the egpdata.xml file which was created when we tested that the DG2XML tool was working correctly. Once set the XMLTV File will be set to a setting similar to C:program filesdg2xmlepgdata.xml Finally we can tell the system to call the XML Grabber each time it runs, so to get the most recent information available in DigiGuide. to do this we set the Grabber to the folder we placed the DG2XML.exe file, this should resemble c:program filesdg2xmldg2xml.exe Now that we have provided the main configuration data, we can click next, where the tool will then go and read the XMLTV file to check that it is formatted correctly, and allow us make some fine tuning adjustments. We can now click Next to continue We are presented with One random program found in the XMLTV File, where we have the ability to verify and check that everything is correct. If your time zone is different from the DigiGuide source, we can use the Time Offset setting to apply any modifications needed. Next we can move on to the the last section of the configuration. Here we are offered a process to help determine when programs are part of a series. Using the DigiGuide service, we will set the tool to use the option Based on episode title or program Description. We can now click OK to complete the edit process. returning back to the XMLTV Files and Grabber. Channel Line Ups This is the next most important part of the process. Now that we have our XMLTV guide in place, we need to teach the system how to match programs in the XMLTV file to the actually channels we are receiving on the DVB Enhancer. Launching into this section of the Wizard lets us customise the details for our TV Viewing. First we will start with assigning a Name **for the feed, in this case **DigiGuide XMLTV Lineup As we are participating in an open community, I am going to allow my settings to be shared, so i will not be clicking on the options Do not upload this lineup to the online database Next, we will set the Country, Region, Type and Source information. As I am based in the West Of Ireland my specifics will be Country – Ireland Region – West Type – Digital Source – Satellite Provider – Sky Digital Finally, before we move on, We will Click on the option Use and external set top box or recorder plug-in as we are taking advantage of the fantastic DVBE4SAGE Plug-in in this build On the next page, we are going to be starting from a clean sheet, so plenty of time will help a lot here. We have now go to go ahead and add each channel that we are interested in receiving and viewing, along with its Guide data into the form. The Data which we will be providing will be ID Unique name – I am using the EPG number of the channel to help me sort the list Name A unique name for the Channel, I am using the same as the Call Sign Call Sign This is the Name which we will see in the EPG for the channel Number This is the most important entry in the file, When we load this detail into SageTV this number will be used for the** Logic (Guide) Channel Number** [For your TV Channel Number] and also Physical (Tuning) Channel Number [This is the SID of the Channel and passed to DVBE4EPG to tune in the channel for us]. for this reason we MUST use the SID number in this field Time Offset Some Channels may be focused for different time zones in Europe, so we can make adjustments here to bring these channels into the same time zone as the guide to get the EPG data to correspond correctly. HD Provides us an opportunity to define that this channel broadcasts in Hi-Def mainly, or Only. XMLTV Data This is a dropdown list of all the channels we have detected in the XMLTV file, with the channels name, and its guide Channel Number should we decided to use the channel same numbers. Look at LyngSat SkyUK Page for a list of the Channels you plan to add, and get busy! Using Movies4Men as an example - Name – Movies4Men Call Sign – Movies4Men Number – 53109 (Refer to the Shot above from Lyngsat) XMLTV Data – Select Movies4Men from the drop down list ID – Set it to the EPG Number of Movies4Men 323 (Note this has no effect in SageTV!) After adding a few Channels we will see the following presentation. Keep up the good work until you have all the required channels added Categories and Ratings The next Configuration we focus on will be the Categories. Categories This is used for improving the readability of the Program Guide when it is displayed. We click on Edit categories for reliable recordings and easily readable program guide Before we start the configuration, we have two different Categories lists which we will be matching up. On the Left we have the Categories which Sage utilizes for its guide, and on the right is the categories which we get in the DigiGuide XMLTV Feed. **SageTV EPG** **DigiGuide XMLTV** Serie Movie News Sport Recreation Music Educational Magazine Children Other Technology, Political, Gardening, Business and Finance, Education, Scientific Documentary, Motoring, Adult Entertainment, Reality Show, Quiz Show, Special Interest, Fashion, Nature, Kids Drama, Arts, Animation, Comedy, Chat Show, Travel, Entertainment, DIY, Cookery, Game Show, Religious, Health, Sitcom, Consumer, History Documentary, Drama, Talk Show, Soap, Documentary, Series, Science Fiction Series, Film, News, Sport, Music, Magazine Programme, and Childrens Our objective is to Match the Categories on the Left to the Categories on the right. This process is of course subjective to your viewing preferences, but as a guide to assist, this is the breakdown I am currently utilizing SageTV Guide Keyword Matcher Keyword Excluder DigiGuide Category Serie Serie Series Science Fiction Series Movie movie film drama kids Film Drama News news scientific business political News Scientific Documentary Business and Finance Political Sport Sport Sport Recreation Sitcom soap entertainment comedy adult Soap Sitcom Entertainment Comedy Music Music Music Educational education technology gardening nature Cookery DIY Technology DIY Cookery Gardening Education Nature Magazine magazine documentary motoring health consumer scientific Magazine Programme Health Motoring Consumer History Documentary Documentary Children Children Kids Animation Childrens Kids Drama Animation General Talk Show Religious Game Show Travel Chat Show Arts Fashion Special Interest Quiz Show Adult Entertainment Talk Show Religious Game Show Travel Chat Show Arts Fashion Special Interest Quiz Show Adult Entertainment The result of this exercise will look a little similar to the results below. Ratings Again, each guide will support different rating grades deepening on the broadcaster or the countries standards. In order for SageTV to process these with some degree of meaning, we are offered the opportunity in the tool to view all the Ratings detected in the XMLTV data file, and match these to the pre-defined ratings which SageTV Understands Advanced Options We are offered the ability to tag programs in the XMLTV File with Star Ratings and HD flags if the data is available. Using this page you can customise some of the settings to match your personal requirements Update Guide Data Finally, we should have all the correct details in place, so we can proceed and Update the Guide Data into SageTV Click on Start Import, and the tool will connect to the DigiGuide Source, Update the XMLTV File, Map the Channels, add any custom groups, and settings to the programs on the channel, including categories, ratings etc, and finally update the SageTV system with the new information. SageTV XMLTV Support We can now start-up SageTV again, and use the Setup Wizard to configure the XMLTV guide. From SageTV main menu, press Setup, then Setup Video Sources. This will bring us to the Source Wizard Summary screen, where we have the option to add additional sources or modify existing ones. We will select the existing connection we added previously for the network stream on the DVB Enhancer for SageTV After selecting the tuner we are configuring, we can select the option for EPG Line-up In the Wizard, we now are offered the option of determining which source to use for the EPG. As we are utilizing XMLTV as the source we will take the option Use US, Canada, or XMLTV Guide Data with Source. Next, we will be asked to select to source, this is not important in the configuration. Select either and we can move onto the next step We will enter 00000 as your zip code (press enter). If everything has went correctly, we should now be presented with the Name of the XMLTV Guide we configured in the XMLTV Import tool a little earlier. Select this entry to move forward. Now, we are offered the option of Basic Service or Extended Basic Service. I don’t think it matters but since we are customising the XMLTV Feed, lets go with Extended Basic Service Next, we are dropped back into the Channel Setup, and instantly prompted to Scan For Channels, however we will be selecting No – I will scan for channels later as the DVBE4SAGE feed does not work like a normal DVB Card. Great, we now should see the channels we included in the XMLTV Channel List a little earlier in the Channel Setup screen. We can now select the option Done with Channel Setup This will return is back to the Source Wizard Summary where can now see that the New DigiGuide XMLTV Lineup is now assgined That should do it. Now, lets just check the the program guide is presenting our new Channels which we added to the Channel Line-up in the XMLTV Import Tool And, of course now is as good a time as any to see that you can select one of these channels and view them. For example, we can highlight The Sky Riders on Movies4Men, and SageTV will ask what we would like to do The obvious choice is to Watch Now which if everything is good will show your show on screen, as we did earlier, we can check the logs on DVB4Sage and see that it is indeed getting the correct infomration and behaving as expected. 2009-08-04 14:10:14.440 Received command: “START SageTV DVB-S2 Enhancer 1 Digital TV Tuner|395403344|53109|2498782829002|C:Program Files (x86)SageTVSageTVVideoTheSkyRiders-8202-0.ts|Fair” 2009-08-04 14:10:14.441 Received START command to start recording on source “SageTV DVB-S2 Enhancer 1 Digital TV Tuner”, channel=53109, duration=2498782829, file=”C:Program Files (x86)SageTVSageTVVideoTheSkyRiders-8202-0.ts” 2009-08-04 14:10:14.442 Service SID=53109 has Name=”movies4men” 2009-08-04 14:10:14.442 Autodiscovery results for SID=53109: TID=2312, Frequency=11223670, Symbol Rate=27500, Polarization=V, Modulation=QPSK (DVB-S), FEC=2/3 2009-08-04 14:10:14.442 Starting recording on tuner=”FireDTV BDA Tuner DVBS2”, Ordinal=2, SID=53109 (“movies4men”), Autodiscovery=TRUE, Duration=2498782829, Frequency=11223670, Symbol Rate=27500, Polarization=V, Modulation=QPSK (DVB-S), FEC=2/3 2009-08-04 14:10:14.456 Loading filter “FireDTV BDA Tuner DVBS2” - succeeded! 2009-08-04 14:10:14.464 Tuner Filter Info = “FireDTV BDA Tuner DVBS2” 2009-08-04 14:10:14.481 Loading filter “FireDTV BDA Receiver DVBS2” - succeeded! 2009-08-04 14:10:14.489 Loaded our transport stream filter 2009-08-04 14:10:14.490 Added demux filter to the graph 2009-08-04 14:10:14.491 Connected demux to our filter output pin 2009-08-04 14:10:14.493 Loading filter “BDA MPEG2 Transport Information Filter” - succeeded! 2009-08-04 14:10:14.494 Using tuning request-based tuning method… 2009-08-04 14:10:14.670 TS packet messed up, fixing… 2009-08-04 14:10:14.812 Received command: “GET_FILE_SIZE C:Program Files (x86)SageTVSageTVVideoTheSkyRiders-8202-0.ts” 2009-08-04 14:10:14.923 Replied 0 2009-08-04 14:10:14.931 Signal locked, quality=81, strength=81 2009-08-04 14:10:14.984 Received command: “GET_FILE_SIZE C:Program Files (x86)SageTVSageTVVideoTheSkyRiders-8202-0.ts” 2009-08-04 14:10:15.286 Replied 40420 2009-08-04 14:10:15.287 Received command: “GET_FILE_SIZE C:Program Files (x86)SageTVSageTVVideoTheSkyRiders-8202-0.ts” 2009-08-04 14:10:15.287 Replied 40420 2009-08-04 14:10:15.342 Received command: “GET_FILE_SIZE C:Program Files (x86)SageTVSageTVVideoTheSkyRiders-8202-0.ts” 2009-08-04 14:10:15.343 Replied 66740 2009-08-04 14:10:15.372 Received command: “GET_FILE_SIZE C:Program Files (x86)SageTVSageTVVideoTheSkyRiders-8202-0.ts” 2009-08-04 14:10:15.372 Replied 79900 2009-08-04 14:10:15.495 Received command: “GET_FILE_SIZE C:Program Files (x86)SageTVSageTVVideoTheSkyRiders-8202-0.ts” Congratulations, you now have a working SageTV installation with the excellent XMLTV Import Tool and the amazing DVB4Sage Enhancer SageTV Force Update As you start working on this, you will keep cycling back to the DigiGuide for new Channels, If you do add a channel which is not in your normal coverage, you must assign it a channel number, or the XMLTV Import tool will not offer you this new channel in the drop down list. As you add new Channels in the XMLTV Import Tool, you will continue to provide as much detail as required, and then move to the Update Guide Data stage. Which will instruct SageTV of the new information. However it is quite likely that Sage will not go ahead and offer you the newly added channels in the Program Guide, as it believes that it has already added all the channels needed. Forcing XMLTV Reload We need to stop the SageTV system fully if this occurs, and open the Sage.properties File, and set the following entries as below, and restart the SageTV System, allowing it a few moments to reload the XMLTV data and rebuild the Channel List for you. (Note the /XXXXXXXX/ will be a random string in your file.) _epg_data_sources/XXXXXXXX/chan_download_complete=false_ &gt; &gt; _epg_data_sources/XXXXXXXX/last_run=0 epg_data_sources/XXXXXXXX/expanded_until=0_ &gt; &gt; SageTV Channel Numbers Once you are 100% happy that everything in the EPG feed is correct, we can go back to SageTV and have the Physical and Logic Channel numbers changed to what we really want to use. Conclusion",
    
  },
  {    
    "id" : "http-0-0-0-0-4000-cloud-case-study-published-1228083720",
    "site" : "damianflynn.com",
    "title": "Cloud Case Study Published!",      
    "url": "http://0.0.0.0:4000/cloud-case-study-published/",                    
    "categories" : ["Announcements","Cloud Strategy"],
    "tags" : ["Achivements","White Paper / Case Study","Lionbridge","Community","Virtual Machines / IaaS","System Center","Virtual Machine Manager","Cloud","ITIL","Self Service"],
    "authors" : ["Damian Flynn"],      
    "publishedDate" : "2008-11-30 22:22:00 +0000",
    "content" : "I am delighted to see a Case Study based on the work my team has completed building and deploying a Cloud environment using Microsoft virtualisation technologies has just been released, you can grab it here. Lionbridge, (My Employer) has created a Virtual Cloud Lab using Hyper-V and System Centre and give secure, global access to it using Windows Server 2008 Terminal Services, reducing our hardware costs by 80 percent as as result of the effort, while also standardized the testing environments, accelerated machine provisioning, and enhanced security and business continuity. The lab itself is in Boston and is currently 9 servers running around 300 virtual machines – this will reduce down to 6 physical servers in the not too distant future. We are using the Self Service Portal feature of System Centre Virtual Machine Manager to dynamically deploy new environments without any input from the local IT staff. Its only the beginning!",
    
  }]